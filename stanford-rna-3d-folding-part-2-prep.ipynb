{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":118765,"databundleVersionId":15231210,"sourceType":"competition"}],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load & Standardize Inputs","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# NOTEBOOK-1 / STAGE 1 — Load & Standardize Inputs (ONE CELL)\n# Stanford RNA 3D Folding Part 2\n#\n# Goals:\n# - Define canonical paths (MSA / PDB_RNA / extra / CSVs)\n# - Load train/val/test sequences + train/val labels + sample_submission\n# - Standardize types + add basic derived columns (L, temporal_cutoff_dt)\n# - Perform strict sanity checks (columns, missing files, sequence chars)\n# - Print a compact dataset summary for debugging\n#\n# Outputs (globals):\n# - PATHS, COMP_ROOT, MSA_DIR, PDB_RNA_DIR, EXTRA_DIR, OUT_DIR\n# - df_train_seq, df_val_seq, df_test_seq\n# - df_train_lbl, df_val_lbl\n# - df_sample_sub\n# - LABEL_COORD_COLS_TRAIN, LABEL_COORD_COLS_VAL\n# - SEQ_ALLOWED\n# ============================================================\n\nimport os, re, json, math, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\npd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.width\", 160)\n\n# ----------------------------\n# 0) Canonical paths (as you provided)\n# ----------------------------\nCOMP_ROOT    = Path(\"/kaggle/input/stanford-rna-3d-folding-2\")\nMSA_DIR      = COMP_ROOT / \"MSA\"\nPDB_RNA_DIR  = COMP_ROOT / \"PDB_RNA\"\nEXTRA_DIR    = COMP_ROOT / \"extra\"\n\nPATHS = {\n    \"COMP_ROOT\": str(COMP_ROOT),\n    \"MSA_DIR\": str(MSA_DIR),\n    \"PDB_RNA_DIR\": str(PDB_RNA_DIR),\n    \"EXTRA_DIR\": str(EXTRA_DIR),\n    \"SAMPLE_SUB\": str(COMP_ROOT / \"sample_submission.csv\"),\n    \"TEST_SEQ\": str(COMP_ROOT / \"test_sequences.csv\"),\n    \"TRAIN_LBL\": str(COMP_ROOT / \"train_labels.csv\"),\n    \"TRAIN_SEQ\": str(COMP_ROOT / \"train_sequences.csv\"),\n    \"VAL_LBL\": str(COMP_ROOT / \"validation_labels.csv\"),\n    \"VAL_SEQ\": str(COMP_ROOT / \"validation_sequences.csv\"),\n    \"PARSE_FASTA_PY\": str(EXTRA_DIR / \"parse_fasta_py.py\"),\n    \"RNA_METADATA\": str(EXTRA_DIR / \"rna_metadata.csv\"),\n    \"EXTRA_README\": str(EXTRA_DIR / \"README.md\"),\n}\n\n# Write outputs here (versioned folder recommended)\nOUT_DIR = Path(\"/kaggle/working/rna3d_artifacts_v1\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\ndef _require(p: str | Path, what: str = \"\"):\n    p = Path(p)\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing {what or 'file/dir'}: {p}\")\n    return p\n\n# Validate required files/dirs exist\n_require(MSA_DIR, \"MSA_DIR\")\n_require(PDB_RNA_DIR, \"PDB_RNA_DIR\")\n_require(EXTRA_DIR, \"EXTRA_DIR\")\nfor k in [\"SAMPLE_SUB\",\"TEST_SEQ\",\"TRAIN_LBL\",\"TRAIN_SEQ\",\"VAL_LBL\",\"VAL_SEQ\"]:\n    _require(PATHS[k], k)\n\n# ----------------------------\n# 1) Helpers\n# ----------------------------\nSEQ_ALLOWED = set(\"ACGU\")\n\ndef _clean_str_series(s: pd.Series) -> pd.Series:\n    return s.astype(\"string\").fillna(\"\").str.strip()\n\ndef _add_seq_len_and_time(df: pd.DataFrame, name: str) -> pd.DataFrame:\n    df = df.copy()\n    # Standardize key cols\n    for c in [\"target_id\",\"sequence\",\"temporal_cutoff\",\"description\",\"stoichiometry\",\"all_sequences\",\"ligand_ids\",\"ligand_SMILES\"]:\n        if c in df.columns:\n            df[c] = _clean_str_series(df[c])\n    # Derived columns\n    if \"sequence\" in df.columns:\n        df[\"L\"] = df[\"sequence\"].str.len().astype(\"int32\")\n    if \"temporal_cutoff\" in df.columns:\n        df[\"temporal_cutoff_dt\"] = pd.to_datetime(df[\"temporal_cutoff\"], errors=\"coerce\", utc=False)\n    # Basic validity checks\n    if \"target_id\" in df.columns:\n        if df[\"target_id\"].isna().any():\n            raise ValueError(f\"[{name}] Found NaN target_id.\")\n        if df[\"target_id\"].duplicated().any():\n            # sequences csv should be unique per target_id within split\n            dups = df.loc[df[\"target_id\"].duplicated(), \"target_id\"].head(10).tolist()\n            raise ValueError(f\"[{name}] Duplicate target_id detected (showing up to 10): {dups}\")\n    if \"sequence\" in df.columns:\n        bad_mask = ~df[\"sequence\"].str.fullmatch(r\"[ACGU]*\")\n        if bad_mask.any():\n            bad = df.loc[bad_mask, [\"target_id\",\"sequence\"]].head(10)\n            raise ValueError(\n                f\"[{name}] Found non-ACGU characters in `sequence` (showing up to 10 rows):\\n{bad}\"\n            )\n    return df\n\ndef _read_labels_fast(csv_path: str | Path, split_name: str):\n    \"\"\"\n    Read labels with dtypes tuned:\n    - ID: string\n    - resname: string\n    - resid: int32\n    - chain: string\n    - copy: int16\n    - coordinate columns: float32\n    Also returns list of coordinate columns found.\n    \"\"\"\n    csv_path = Path(csv_path)\n    # First read header only\n    cols = pd.read_csv(csv_path, nrows=0).columns.tolist()\n\n    # Detect coordinate columns: x_k, y_k, z_k (k can be 1..N)\n    coord_pat = re.compile(r\"^[xyz]_\\d+$\")\n    coord_cols = [c for c in cols if coord_pat.match(c)]\n\n    # Minimal required\n    required = {\"ID\",\"resname\",\"resid\"}\n    missing_req = sorted(list(required - set(cols)))\n    if missing_req:\n        raise ValueError(f\"[{split_name}] labels missing required columns: {missing_req}\")\n\n    # dtype map\n    dtype = {}\n    for c in cols:\n        if c in coord_cols:\n            dtype[c] = \"float32\"\n        elif c == \"resid\":\n            dtype[c] = \"int32\"\n        elif c == \"copy\":\n            dtype[c] = \"int16\"\n        else:\n            # ID, resname, chain, etc.\n            dtype[c] = \"string\"\n\n    df = pd.read_csv(csv_path, dtype=dtype)\n    # Standardize strings\n    for c in [\"ID\",\"resname\",\"chain\"]:\n        if c in df.columns:\n            df[c] = _clean_str_series(df[c])\n    # Quick parse target_id + resid from ID (safe for later)\n    # ID format: {target_id}_{resid} where resid is 1-based integer\n    if \"ID\" in df.columns:\n        sp = df[\"ID\"].str.rsplit(\"_\", n=1, expand=True)\n        if sp.shape[1] == 2:\n            df[\"target_id\"] = sp[0].astype(\"string\")\n            # keep numeric resid_from_id for consistency check later\n            df[\"resid_from_id\"] = pd.to_numeric(sp[1], errors=\"coerce\").astype(\"Int32\")\n        else:\n            df[\"target_id\"] = pd.NA\n            df[\"resid_from_id\"] = pd.NA\n\n    # Basic checks\n    if df[\"resid\"].isna().any():\n        raise ValueError(f\"[{split_name}] labels: NaN resid found.\")\n    if (df[\"resid\"] <= 0).any():\n        bad = df.loc[df[\"resid\"] <= 0, [\"ID\",\"resid\"]].head(10)\n        raise ValueError(f\"[{split_name}] labels: resid must be 1-based (>0). Bad rows:\\n{bad}\")\n\n    # coord columns count should be multiple of 3 (x/y/z for each reference/pred)\n    if len(coord_cols) % 3 != 0:\n        raise ValueError(f\"[{split_name}] labels: number of coord cols is not multiple of 3: {len(coord_cols)}\")\n\n    return df, coord_cols\n\ndef _print_seq_summary(df: pd.DataFrame, name: str):\n    L = df[\"L\"].to_numpy()\n    print(f\"\\n[{name}] n_targets={len(df):,} | L: min={L.min():,}  p50={int(np.median(L)):,}  p90={int(np.quantile(L,0.9)):,}  max={L.max():,}\")\n    if \"temporal_cutoff_dt\" in df.columns:\n        tmin = df[\"temporal_cutoff_dt\"].min()\n        tmax = df[\"temporal_cutoff_dt\"].max()\n        nbad = df[\"temporal_cutoff_dt\"].isna().sum()\n        print(f\"[{name}] temporal_cutoff_dt: min={tmin}  max={tmax}  invalid_dates={nbad:,}\")\n\n# ----------------------------\n# 2) Load sequences\n# ----------------------------\ndf_train_seq = pd.read_csv(PATHS[\"TRAIN_SEQ\"])\ndf_val_seq   = pd.read_csv(PATHS[\"VAL_SEQ\"])\ndf_test_seq  = pd.read_csv(PATHS[\"TEST_SEQ\"])\n\ndf_train_seq = _add_seq_len_and_time(df_train_seq, \"train_sequences\")\ndf_val_seq   = _add_seq_len_and_time(df_val_seq, \"validation_sequences\")\ndf_test_seq  = _add_seq_len_and_time(df_test_seq, \"test_sequences\")\n\n# Verify required columns exist in sequences\nSEQ_REQUIRED = [\"target_id\",\"sequence\",\"temporal_cutoff\",\"stoichiometry\",\"all_sequences\"]\nfor name, df in [(\"train_sequences\", df_train_seq), (\"validation_sequences\", df_val_seq), (\"test_sequences\", df_test_seq)]:\n    miss = [c for c in SEQ_REQUIRED if c not in df.columns]\n    if miss:\n        raise ValueError(f\"[{name}] missing required columns: {miss}\")\n\n# ----------------------------\n# 3) Load labels (fast dtype)\n# ----------------------------\ndf_train_lbl, LABEL_COORD_COLS_TRAIN = _read_labels_fast(PATHS[\"TRAIN_LBL\"], \"train_labels\")\ndf_val_lbl,   LABEL_COORD_COLS_VAL   = _read_labels_fast(PATHS[\"VAL_LBL\"], \"validation_labels\")\n\n# ----------------------------\n# 4) Load sample_submission (and basic checks)\n# ----------------------------\ndf_sample_sub = pd.read_csv(PATHS[\"SAMPLE_SUB\"])\n\n# Must contain ID + coordinates for 5 predictions (x_1..z_5)\nif \"ID\" not in df_sample_sub.columns:\n    raise ValueError(\"sample_submission.csv must contain column `ID`.\")\n\nsub_cols = df_sample_sub.columns.tolist()\nsub_coord_pat = re.compile(r\"^[xyz]_[1-5]$\")\nsub_coord_cols = [c for c in sub_cols if sub_coord_pat.match(c)]\nif len(sub_coord_cols) != 15:\n    # x_1..x_5 (5) + y_1..y_5 (5) + z_1..z_5 (5) = 15\n    raise ValueError(f\"sample_submission must contain 15 coord cols (x_1..z_5). Found {len(sub_coord_cols)}\")\n\n# ----------------------------\n# 5) Optional: detect aux files for later stages (no heavy parsing here)\n# ----------------------------\nhas_parse_fasta = Path(PATHS[\"PARSE_FASTA_PY\"]).exists()\nhas_rna_metadata = Path(PATHS[\"RNA_METADATA\"]).exists()\n\nmsa_files = sorted(MSA_DIR.glob(\"*.MSA.fasta\"))\ncif_files = sorted(PDB_RNA_DIR.glob(\"*.cif\"))\n\n# ----------------------------\n# 6) Summary prints (compact)\n# ----------------------------\nprint(\"=== PATHS ===\")\nprint(json.dumps(PATHS, indent=2))\n\n_print_seq_summary(df_train_seq, \"train_sequences\")\n_print_seq_summary(df_val_seq,   \"validation_sequences\")\n_print_seq_summary(df_test_seq,  \"test_sequences\")\n\nprint(f\"\\n[train_labels] rows={len(df_train_lbl):,} | coord_cols={len(LABEL_COORD_COLS_TRAIN)} (refs={len(LABEL_COORD_COLS_TRAIN)//3})\")\nprint(f\"[val_labels]   rows={len(df_val_lbl):,} | coord_cols={len(LABEL_COORD_COLS_VAL)} (refs={len(LABEL_COORD_COLS_VAL)//3})\")\n\nprint(f\"\\n[sample_submission] rows={len(df_sample_sub):,} | coord_cols={len(sub_coord_cols)} (must be 15)\")\n\nprint(\"\\n=== AUX FILES (for next stages) ===\")\nprint(f\"MSA_DIR exists: {MSA_DIR.exists()} | n_msa_files(train+val provided)={len(msa_files):,} | example={msa_files[0].name if msa_files else None}\")\nprint(f\"PDB_RNA_DIR exists: {PDB_RNA_DIR.exists()} | n_cif_files={len(cif_files):,} | example={cif_files[0].name if cif_files else None}\")\nprint(f\"extra/parse_fasta_py.py exists: {has_parse_fasta}\")\nprint(f\"extra/rna_metadata.csv exists: {has_rna_metadata}\")\nprint(f\"OUT_DIR: {OUT_DIR}\")\n\n# Quick alignment sanity: labels target_id should be subset of sequences target_id for the same split\ntrain_targets = set(df_train_seq[\"target_id\"].tolist())\nval_targets   = set(df_val_seq[\"target_id\"].tolist())\ntrain_lbl_targets = set(df_train_lbl[\"target_id\"].dropna().unique().tolist())\nval_lbl_targets   = set(df_val_lbl[\"target_id\"].dropna().unique().tolist())\n\nmissing_train = sorted(list(train_lbl_targets - train_targets))[:10]\nmissing_val   = sorted(list(val_lbl_targets - val_targets))[:10]\n\nif missing_train:\n    raise ValueError(f\"train_labels contain target_id not found in train_sequences (show up to 10): {missing_train}\")\nif missing_val:\n    raise ValueError(f\"validation_labels contain target_id not found in validation_sequences (show up to 10): {missing_val}\")\n\nprint(\"\\n[OK] Stage 1 complete: inputs loaded & standardized.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:31:48.444746Z","iopub.execute_input":"2026-01-08T14:31:48.445078Z","iopub.status.idle":"2026-01-08T14:32:29.423688Z","shell.execute_reply.started":"2026-01-08T14:31:48.445054Z","shell.execute_reply":"2026-01-08T14:32:29.422728Z"}},"outputs":[{"name":"stdout","text":"=== PATHS ===\n{\n  \"COMP_ROOT\": \"/kaggle/input/stanford-rna-3d-folding-2\",\n  \"MSA_DIR\": \"/kaggle/input/stanford-rna-3d-folding-2/MSA\",\n  \"PDB_RNA_DIR\": \"/kaggle/input/stanford-rna-3d-folding-2/PDB_RNA\",\n  \"EXTRA_DIR\": \"/kaggle/input/stanford-rna-3d-folding-2/extra\",\n  \"SAMPLE_SUB\": \"/kaggle/input/stanford-rna-3d-folding-2/sample_submission.csv\",\n  \"TEST_SEQ\": \"/kaggle/input/stanford-rna-3d-folding-2/test_sequences.csv\",\n  \"TRAIN_LBL\": \"/kaggle/input/stanford-rna-3d-folding-2/train_labels.csv\",\n  \"TRAIN_SEQ\": \"/kaggle/input/stanford-rna-3d-folding-2/train_sequences.csv\",\n  \"VAL_LBL\": \"/kaggle/input/stanford-rna-3d-folding-2/validation_labels.csv\",\n  \"VAL_SEQ\": \"/kaggle/input/stanford-rna-3d-folding-2/validation_sequences.csv\",\n  \"PARSE_FASTA_PY\": \"/kaggle/input/stanford-rna-3d-folding-2/extra/parse_fasta_py.py\",\n  \"RNA_METADATA\": \"/kaggle/input/stanford-rna-3d-folding-2/extra/rna_metadata.csv\",\n  \"EXTRA_README\": \"/kaggle/input/stanford-rna-3d-folding-2/extra/README.md\"\n}\n\n[train_sequences] n_targets=5,716 | L: min=10  p50=94  p90=4,751  max=125,580\n[train_sequences] temporal_cutoff_dt: min=1978-04-12 00:00:00  max=2025-12-17 00:00:00  invalid_dates=0\n\n[validation_sequences] n_targets=28 | L: min=19  p50=91  p90=407  max=4,640\n[validation_sequences] temporal_cutoff_dt: min=2025-06-04 00:00:00  max=2025-12-03 00:00:00  invalid_dates=0\n\n[test_sequences] n_targets=28 | L: min=19  p50=91  p90=407  max=4,640\n[test_sequences] temporal_cutoff_dt: min=2025-06-04 00:00:00  max=2025-12-03 00:00:00  invalid_dates=0\n\n[train_labels] rows=7,794,971 | coord_cols=3 (refs=1)\n[val_labels]   rows=9,762 | coord_cols=120 (refs=40)\n\n[sample_submission] rows=9,762 | coord_cols=15 (must be 15)\n\n=== AUX FILES (for next stages) ===\nMSA_DIR exists: True | n_msa_files(train+val provided)=5,744 | example=157D.MSA.fasta\nPDB_RNA_DIR exists: True | n_cif_files=9,564 | example=100d.cif\nextra/parse_fasta_py.py exists: True\nextra/rna_metadata.csv exists: True\nOUT_DIR: /kaggle/working/rna3d_artifacts_v1\n\n[OK] Stage 1 complete: inputs loaded & standardized.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Parse all_sequences + Resolve stoichiometry + Build Chain Boundaries","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# NOTEBOOK-1 / STAGE 2 — Parse all_sequences + Resolve stoichiometry + Build Chain Boundaries (ONE CELL)\n# REVISI FULL (fixes the 3 failing train targets)\n#\n# Key fixes vs previous:\n# 1) More robust FASTA header parsing: collects BOTH chain_id and auth_chain_id as aliases\n#    e.g. \"Chain A[auth B]\" -> aliases include \"A\" and \"B\"\n# 2) Normalize chain sequences to match canonical target sequence:\n#    - uppercase\n#    - convert T -> U\n# 3) Do NOT partially write segments for a target that fails validation (atomic per-target segment commit)\n# 4) Hard-fail policy is adjustable; default keeps going but prints failing targets clearly\n#\n# Requires globals from STAGE 1:\n# - PATHS, OUT_DIR\n# - df_train_seq, df_val_seq, df_test_seq\n#\n# Writes:\n# - OUT_DIR/tables/targets_*_stage2.parquet\n# - OUT_DIR/tables/segments_*.parquet\n# - OUT_DIR/meta/qa_stage2_*.csv\n#\n# Outputs (globals):\n# - df_train_seq2, df_val_seq2, df_test_seq2\n# - seg_train, seg_val, seg_test\n# - qa_train2, qa_val2, qa_test2\n# ============================================================\n\nimport re, json, hashlib, warnings\nfrom pathlib import Path\nfrom typing import Dict, Tuple, List, Optional\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Require STAGE 1 globals\n# ----------------------------\nfor need in [\"PATHS\", \"OUT_DIR\", \"df_train_seq\", \"df_val_seq\", \"df_test_seq\"]:\n    if need not in globals():\n        raise RuntimeError(f\"Missing `{need}`. Run STAGE 1 first.\")\n\nOUT_DIR = Path(OUT_DIR)\nTABLE_DIR = OUT_DIR / \"tables\"\nMETA_DIR  = OUT_DIR / \"meta\"\nTABLE_DIR.mkdir(parents=True, exist_ok=True)\nMETA_DIR.mkdir(parents=True, exist_ok=True)\n\n# ----------------------------\n# 1) Robust FASTA parser (extends provided idea):\n#    - extracts chain_id and auth_chain_id aliases for each chain entry\n# ----------------------------\nRE_CHAINS_PREFIX = re.compile(r\"^Chains?\\s+\", re.IGNORECASE)\nRE_AUTH = re.compile(r\"\\[auth\\s+([^\\]]+)\\]\", re.IGNORECASE)\n\ndef _norm_seq(s: str) -> str:\n    # canonicalize: uppercase + DNA->RNA\n    return str(s).strip().upper().replace(\"T\", \"U\")\n\ndef _parse_chain_token(token: str) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"\n    token examples:\n      \"A[auth B]\"\n      \"A [auth B]\"\n      \"A\"\n      \"B[auth 1]\" (multi-char auth allowed)\n    returns:\n      (chain_id, auth_chain_id)\n    \"\"\"\n    t = token.strip()\n    if not t:\n        return None, None\n    # chain_id is the leading chunk before '[' (or whole string if no '[')\n    chain_id = t.split(\"[\", 1)[0].strip()\n    chain_id = chain_id.split()[0].strip() if chain_id else None\n\n    m = RE_AUTH.search(t)\n    auth_id = m.group(1).strip() if m else None\n    return chain_id, auth_id\n\ndef parse_fasta_allseq(fasta_content: str) -> Dict[str, Tuple[str, List[str]]]:\n    \"\"\"\n    Returns dict: {primary_key: (sequence, aliases)}\n    - primary_key: prefer first auth_chain_id if present else first chain_id\n    - aliases: include BOTH chain_id and auth_chain_id for all chains listed in header\n    \"\"\"\n    result: Dict[str, Tuple[str, List[str]]] = {}\n    if fasta_content is None:\n        return result\n\n    lines = str(fasta_content).strip().splitlines()\n    i = 0\n    while i < len(lines):\n        line = lines[i].strip()\n        if line.startswith(\">\"):\n            parts = line.split(\"|\")\n            chains_part = parts[1].strip() if len(parts) >= 2 else \"\"\n            chains_part = RE_CHAINS_PREFIX.sub(\"\", chains_part).strip()\n\n            aliases: List[str] = []\n            # split by comma for \"Chains A[auth A], B[auth B]\"\n            chain_tokens = [c.strip() for c in chains_part.split(\",\") if c.strip()] if chains_part else []\n\n            # collect both chain_id and auth_id as aliases\n            primary_auth: Optional[str] = None\n            primary_chain: Optional[str] = None\n\n            for tok in chain_tokens:\n                chain_id, auth_id = _parse_chain_token(tok)\n                if primary_chain is None and chain_id:\n                    primary_chain = chain_id\n                if primary_auth is None and auth_id:\n                    primary_auth = auth_id\n                if chain_id:\n                    aliases.append(chain_id)\n                if auth_id:\n                    aliases.append(auth_id)\n\n            # if header malformed, aliases empty -> fallback primary None\n            primary = primary_auth or primary_chain\n\n            # read sequence lines until next header\n            seq = \"\"\n            while (i + 1) < len(lines) and (not lines[i + 1].startswith(\">\")):\n                seq += lines[i + 1].strip()\n                i += 1\n            seq = _norm_seq(seq)\n\n            if primary:\n                # de-dup aliases, keep order\n                seen = set()\n                aliases_u = []\n                for a in aliases:\n                    a = str(a).strip()\n                    if a and a not in seen:\n                        aliases_u.append(a)\n                        seen.add(a)\n                # ensure primary included\n                if primary not in seen:\n                    aliases_u = [primary] + aliases_u\n                result[str(primary).strip()] = (seq, aliases_u)\n\n        i += 1\n\n    return result\n\n# ----------------------------\n# 2) Stoichiometry parsing + hashing utils\n# ----------------------------\nSTOICH_ITEM_RE = re.compile(r\"^\\s*([A-Za-z0-9]+)\\s*:\\s*([0-9]+)\\s*$\")\n\ndef parse_stoichiometry(stoich: str):\n    if stoich is None:\n        return []\n    s = str(stoich).strip()\n    if not s:\n        return []\n    s = s.replace(\"{\", \"\").replace(\"}\", \"\")\n    items = []\n    for part in s.split(\";\"):\n        part = part.strip()\n        if not part:\n            continue\n        m = STOICH_ITEM_RE.match(part)\n        if not m:\n            raise ValueError(f\"Bad stoichiometry token: `{part}` (from `{stoich}`)\")\n        chain_token = m.group(1).strip()\n        copies = int(m.group(2))\n        if copies <= 0:\n            raise ValueError(f\"Stoichiometry copies must be >0: `{part}` (from `{stoich}`)\")\n        items.append((chain_token, copies))\n    return items\n\ndef md5_str(s: str) -> str:\n    h = hashlib.md5()\n    h.update(s.encode(\"utf-8\"))\n    return h.hexdigest()\n\ndef md5_pieces(pieces: List[str]) -> str:\n    h = hashlib.md5()\n    for p in pieces:\n        h.update(p.encode(\"utf-8\"))\n    return h.hexdigest()\n\ndef build_alias_to_primary(chain_dict: Dict[str, Tuple[str, List[str]]]):\n    \"\"\"\n    chain_dict: {primary: (seq, aliases)}\n    returns:\n      primary_to_seq, alias_to_primary\n    \"\"\"\n    primary_to_seq = {}\n    alias_to_primary = {}\n    for primary, (seq, aliases) in (chain_dict or {}).items():\n        primary = str(primary).strip()\n        if not primary:\n            continue\n        seq = _norm_seq(seq)\n        primary_to_seq[primary] = seq\n        aliases = aliases or [primary]\n        for a in aliases:\n            a = str(a).strip()\n            if a and a not in alias_to_primary:\n                alias_to_primary[a] = primary\n        # include primary\n        if primary not in alias_to_primary:\n            alias_to_primary[primary] = primary\n    return primary_to_seq, alias_to_primary\n\n# ----------------------------\n# 3) Core processing per split (atomic per-target segments)\n# ----------------------------\ndef process_split(df_seq: pd.DataFrame, split_name: str, join_threshold: int = 20000, hard_fail: bool = False):\n    required = [\"target_id\",\"sequence\",\"L\",\"stoichiometry\",\"all_sequences\"]\n    miss = [c for c in required if c not in df_seq.columns]\n    if miss:\n        raise ValueError(f\"[{split_name}] missing required columns: {miss}\")\n\n    out = df_seq.copy()\n    # normalize target sequence too (should already be ACGU)\n    out[\"sequence\"] = out[\"sequence\"].astype(\"string\").fillna(\"\").str.strip().str.upper().str.replace(\"T\", \"U\", regex=False)\n\n    # new cols\n    out[\"stoich_items_json\"] = \"\"\n    out[\"n_segments\"] = 0\n    out[\"n_primary_chains\"] = 0\n    out[\"L_calc\"] = pd.NA\n    out[\"boundary_starts_json\"] = \"\"\n    out[\"parse_ok\"] = False\n    out[\"stoich_ok\"] = False\n    out[\"rebuild_ok\"] = False\n    out[\"err_stage2\"] = \"\"\n\n    seg_rows_all = []\n    qa_rows = []\n\n    for row in out.itertuples(index=True):\n        idx = row.Index\n        tid = row.target_id\n        seq_given = row.sequence\n        L_given = int(row.L)\n\n        parse_ok = False\n        stoich_ok = False\n        rebuild_ok = False\n        err = \"\"\n        L_calc = None\n        boundary_starts = []\n        stoich_items = None\n        n_segments = 0\n        n_primary = 0\n\n        # per-target temp holders to avoid partial commit\n        seg_rows_tmp = []\n        pieces = []\n        primaries_used = set()\n\n        try:\n            chain_dict = parse_fasta_allseq(row.all_sequences)\n            primary_to_seq, alias_to_primary = build_alias_to_primary(chain_dict)\n            if not primary_to_seq:\n                raise ValueError(\"parse_fasta_allseq returned empty chain dictionary\")\n            parse_ok = True\n\n            stoich_items = parse_stoichiometry(row.stoichiometry)\n            if not stoich_items:\n                raise ValueError(\"empty stoichiometry after parsing\")\n\n            start = 1\n            seg_idx = 0\n\n            for chain_token, copies in stoich_items:\n                # try direct, then uppercase (defensive)\n                ct = str(chain_token).strip()\n                if ct not in alias_to_primary and ct.upper() in alias_to_primary:\n                    ct = ct.upper()\n\n                if ct in alias_to_primary:\n                    primary = alias_to_primary[ct]\n                else:\n                    # last resort: sometimes tokens include whitespace; try strip already done\n                    raise KeyError(f\"stoichiometry chain `{chain_token}` not found among aliases (n_aliases={len(alias_to_primary)})\")\n\n                chain_seq = primary_to_seq.get(primary, \"\")\n                chain_seq = _norm_seq(chain_seq)\n                if not chain_seq:\n                    raise ValueError(f\"resolved primary chain `{primary}` has empty sequence\")\n\n                primaries_used.add(primary)\n\n                for copy_idx in range(1, copies + 1):\n                    seg_len = len(chain_seq)\n                    end = start + seg_len - 1\n                    seg_idx += 1\n                    boundary_starts.append(start)\n                    pieces.append(chain_seq)\n\n                    seg_rows_tmp.append({\n                        \"split\": split_name,\n                        \"target_id\": tid,\n                        \"seg_idx\": seg_idx,\n                        \"chain_token\": chain_token,\n                        \"chain_primary\": primary,\n                        \"copy_idx\": copy_idx,\n                        \"start_1based\": start,\n                        \"end_1based\": end,\n                        \"seg_len\": seg_len,\n                    })\n                    start = end + 1\n\n            L_calc = start - 1\n            n_segments = seg_idx\n            n_primary = len(primaries_used)\n            stoich_ok = True\n\n            # length check\n            if L_calc != L_given:\n                raise ValueError(f\"L mismatch: L_calc={L_calc} vs L_given={L_given}\")\n\n            # sequence check (direct for moderate, md5 for huge)\n            if L_calc <= join_threshold:\n                seq_rebuilt = \"\".join(pieces)\n                if seq_rebuilt != seq_given:\n                    # common edge case: sequence_given already uppercase; still mismatch means mapping/order issue\n                    raise ValueError(\"sequence mismatch (direct compare)\")\n            else:\n                if md5_pieces(pieces) != md5_str(seq_given):\n                    raise ValueError(\"sequence mismatch (md5 compare)\")\n\n            rebuild_ok = True\n\n            # commit segments only if OK\n            seg_rows_all.extend(seg_rows_tmp)\n\n        except Exception as e:\n            err = str(e)\n\n        out.at[idx, \"stoich_items_json\"] = json.dumps(stoich_items) if stoich_items is not None else \"\"\n        out.at[idx, \"n_segments\"] = int(n_segments)\n        out.at[idx, \"n_primary_chains\"] = int(n_primary)\n        out.at[idx, \"L_calc\"] = int(L_calc) if L_calc is not None else pd.NA\n        out.at[idx, \"boundary_starts_json\"] = json.dumps(boundary_starts) if boundary_starts else \"\"\n        out.at[idx, \"parse_ok\"] = bool(parse_ok)\n        out.at[idx, \"stoich_ok\"] = bool(stoich_ok)\n        out.at[idx, \"rebuild_ok\"] = bool(rebuild_ok)\n        out.at[idx, \"err_stage2\"] = err\n\n        qa_rows.append({\n            \"split\": split_name,\n            \"target_id\": tid,\n            \"L\": L_given,\n            \"L_calc\": (int(L_calc) if L_calc is not None else np.nan),\n            \"n_segments\": int(n_segments),\n            \"n_primary_chains\": int(n_primary),\n            \"parse_ok\": bool(parse_ok),\n            \"stoich_ok\": bool(stoich_ok),\n            \"rebuild_ok\": bool(rebuild_ok),\n            \"err\": err,\n        })\n\n    qa_df = pd.DataFrame(qa_rows)\n    seg_df = pd.DataFrame(seg_rows_all)\n\n    n_total = len(out)\n    n_ok = int(out[\"rebuild_ok\"].sum())\n    n_bad = n_total - n_ok\n    print(f\"[{split_name}] STAGE2 rebuild_ok: {n_ok:,}/{n_total:,} (bad={n_bad:,})\")\n\n    if n_bad > 0:\n        bad_preview = qa_df.loc[~qa_df[\"rebuild_ok\"]].head(20)[\n            [\"target_id\",\"L\",\"L_calc\",\"parse_ok\",\"stoich_ok\",\"err\"]\n        ]\n        print(f\"\\n[{split_name}] Failures (up to 20):\")\n        print(bad_preview.to_string(index=False))\n        if hard_fail:\n            raise RuntimeError(f\"[{split_name}] STAGE2 found {n_bad} failing targets. Fix mapping before proceeding.\")\n\n    return out, seg_df, qa_df\n\n# ----------------------------\n# 4) Run STAGE2 (default hard_fail=False to allow progress; set True if you want strict)\n# ----------------------------\ndf_train_seq2, seg_train, qa_train2 = process_split(df_train_seq, \"train\", join_threshold=20000, hard_fail=False)\ndf_val_seq2,   seg_val,   qa_val2   = process_split(df_val_seq,   \"val\",   join_threshold=20000, hard_fail=True)   # val/test should be clean\ndf_test_seq2,  seg_test,  qa_test2  = process_split(df_test_seq,  \"test\",  join_threshold=20000, hard_fail=True)\n\n# ----------------------------\n# 5) Save artifacts\n# ----------------------------\ndf_train_seq2.to_parquet(TABLE_DIR / \"targets_train_stage2.parquet\", index=False)\ndf_val_seq2.to_parquet(  TABLE_DIR / \"targets_val_stage2.parquet\",   index=False)\ndf_test_seq2.to_parquet( TABLE_DIR / \"targets_test_stage2.parquet\",  index=False)\n\nseg_train.to_parquet(TABLE_DIR / \"segments_train.parquet\", index=False)\nseg_val.to_parquet(  TABLE_DIR / \"segments_val.parquet\",   index=False)\nseg_test.to_parquet( TABLE_DIR / \"segments_test.parquet\",  index=False)\n\nqa_train2.to_csv(META_DIR / \"qa_stage2_train.csv\", index=False)\nqa_val2.to_csv(  META_DIR / \"qa_stage2_val.csv\",   index=False)\nqa_test2.to_csv( META_DIR / \"qa_stage2_test.csv\",  index=False)\n\n# Helpful summary: how many targets were skipped in train (if any)\nn_train_bad = int((~df_train_seq2[\"rebuild_ok\"]).sum())\nif n_train_bad > 0:\n    bad_ids = df_train_seq2.loc[~df_train_seq2[\"rebuild_ok\"], \"target_id\"].head(20).tolist()\n    print(f\"\\n[train] WARNING: {n_train_bad} targets could not be segmented/rebuilt. Example ids (up to 20): {bad_ids}\")\n    print(\"They remain in targets_train_stage2.parquet with rebuild_ok=False, but will NOT appear in segments_train.parquet.\")\n\nprint(\"\\n[OK] STAGE 2 complete.\")\nprint(f\"Saved targets:  {TABLE_DIR}/targets_*_stage2.parquet\")\nprint(f\"Saved segments: {TABLE_DIR}/segments_*.parquet\")\nprint(f\"Saved QA:       {META_DIR}/qa_stage2_*.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:48:25.473818Z","iopub.execute_input":"2026-01-08T14:48:25.474196Z","iopub.status.idle":"2026-01-08T14:48:27.681413Z","shell.execute_reply.started":"2026-01-08T14:48:25.474171Z","shell.execute_reply":"2026-01-08T14:48:27.680633Z"}},"outputs":[{"name":"stdout","text":"[train] STAGE2 rebuild_ok: 5,550/5,716 (bad=166)\n\n[train] Failures (up to 20):\ntarget_id    L  L_calc  parse_ok  stoich_ok                                     err\n     1FEU   40    42.0      True       True     L mismatch: L_calc=42 vs L_given=40\n     1M5K  113   184.0      True       True   L mismatch: L_calc=184 vs L_given=113\n     1N35   15    20.0      True       True     L mismatch: L_calc=20 vs L_given=15\n     1TFY   46    44.0      True       True     L mismatch: L_calc=44 vs L_given=46\n     1YSH  163    96.0      True       True    L mismatch: L_calc=96 vs L_given=163\n     2E9T   15    14.0      True       True     L mismatch: L_calc=14 vs L_given=15\n     3BO2  222    41.0      True       True    L mismatch: L_calc=41 vs L_given=222\n     3BO3  222    41.0      True       True    L mismatch: L_calc=41 vs L_given=222\n     3DEG  313   367.0      True       True   L mismatch: L_calc=367 vs L_given=313\n     3HAX   77    28.0      True       True     L mismatch: L_calc=28 vs L_given=77\n     3J0L  788   787.0      True       True   L mismatch: L_calc=787 vs L_given=788\n     3J0O  738   569.0      True       True   L mismatch: L_calc=569 vs L_given=738\n     3J0P  579   677.0      True       True   L mismatch: L_calc=677 vs L_given=579\n     3J0Q  600   698.0      True       True   L mismatch: L_calc=698 vs L_given=600\n     4AQY 1543  1552.0      True       True L mismatch: L_calc=1552 vs L_given=1543\n     4B3M 1543  1553.0      True       True L mismatch: L_calc=1553 vs L_given=1543\n     4B3R 1543  1553.0      True       True L mismatch: L_calc=1553 vs L_given=1543\n     4B3S 1543  1553.0      True       True L mismatch: L_calc=1553 vs L_given=1543\n     4B3T 1543  1553.0      True       True L mismatch: L_calc=1553 vs L_given=1543\n     4V42 4793  3394.0      True       True L mismatch: L_calc=3394 vs L_given=4793\n[val] STAGE2 rebuild_ok: 28/28 (bad=0)\n[test] STAGE2 rebuild_ok: 28/28 (bad=0)\n\n[train] WARNING: 166 targets could not be segmented/rebuilt. Example ids (up to 20): ['1FEU', '1M5K', '1N35', '1TFY', '1YSH', '2E9T', '3BO2', '3BO3', '3DEG', '3HAX', '3J0L', '3J0O', '3J0P', '3J0Q', '4AQY', '4B3M', '4B3R', '4B3S', '4B3T', '4V42']\nThey remain in targets_train_stage2.parquet with rebuild_ok=False, but will NOT appear in segments_train.parquet.\n\n[OK] STAGE 2 complete.\nSaved targets:  /kaggle/working/rna3d_artifacts_v1/tables/targets_*_stage2.parquet\nSaved segments: /kaggle/working/rna3d_artifacts_v1/tables/segments_*.parquet\nSaved QA:       /kaggle/working/rna3d_artifacts_v1/meta/qa_stage2_*.csv\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Rebuild & Validate Target Sequence + Residue Index Table","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# NOTEBOOK-1 / STAGE 3 — Rebuild-Validate (coverage) + Residue Index Table (ONE CELL)\n#\n# Requires STAGE 2 artifacts already saved in:\n#   OUT_DIR=/kaggle/working/rna3d_artifacts_v1\n#   - tables/targets_*_stage2.parquet\n#   - tables/segments_*.parquet\n#\n# What this stage does:\n# 1) Load targets_stage2 + segments tables\n# 2) For each split:\n#    - keep only targets with rebuild_ok=True (train has some False; val/test all True)\n#    - validate segment coverage: start at 1, contiguous, last end == L\n#    - build a per-residue index table:\n#        columns: target_id, resid, resname, chain_primary, copy_idx, seg_idx, is_chain_start, is_chain_end\n#    - write as partitioned parquet parts (safe memory):\n#        OUT_DIR/residue_index/{split}/part-xxxxx.parquet\n#    - write QA summary:\n#        OUT_DIR/meta/qa_stage3_{split}.csv\n#\n# Outputs (globals):\n# - targets2_train, targets2_val, targets2_test\n# - seg_train, seg_val, seg_test\n# - qa3_train, qa3_val, qa3_test\n# - RESIDX_DIR\n# ============================================================\n\nimport gc, math, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Paths + Load STAGE2 artifacts\n# ----------------------------\nif \"OUT_DIR\" not in globals():\n    OUT_DIR = Path(\"/kaggle/working/rna3d_artifacts_v1\")\nelse:\n    OUT_DIR = Path(OUT_DIR)\n\nTABLE_DIR = OUT_DIR / \"tables\"\nMETA_DIR  = OUT_DIR / \"meta\"\nRESIDX_DIR = OUT_DIR / \"residue_index\"\n\nfor p in [TABLE_DIR, META_DIR]:\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing STAGE2 output directory: {p}. Run STAGE 2 first.\")\n\ndef _req(p: Path):\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing file: {p}\")\n    return p\n\ntargets2_train = pd.read_parquet(_req(TABLE_DIR / \"targets_train_stage2.parquet\"))\ntargets2_val   = pd.read_parquet(_req(TABLE_DIR / \"targets_val_stage2.parquet\"))\ntargets2_test  = pd.read_parquet(_req(TABLE_DIR / \"targets_test_stage2.parquet\"))\n\nseg_train = pd.read_parquet(_req(TABLE_DIR / \"segments_train.parquet\"))\nseg_val   = pd.read_parquet(_req(TABLE_DIR / \"segments_val.parquet\"))\nseg_test  = pd.read_parquet(_req(TABLE_DIR / \"segments_test.parquet\"))\n\n# Ensure types\nfor df in [targets2_train, targets2_val, targets2_test]:\n    df[\"target_id\"] = df[\"target_id\"].astype(\"string\")\n    df[\"sequence\"]  = df[\"sequence\"].astype(\"string\")\n    df[\"L\"]         = df[\"L\"].astype(\"int32\")\n    if \"rebuild_ok\" in df.columns:\n        df[\"rebuild_ok\"] = df[\"rebuild_ok\"].astype(\"bool\")\n\nfor df in [seg_train, seg_val, seg_test]:\n    df[\"target_id\"] = df[\"target_id\"].astype(\"string\")\n    for c in [\"seg_idx\",\"copy_idx\",\"start_1based\",\"end_1based\",\"seg_len\"]:\n        if c in df.columns:\n            df[c] = df[c].astype(\"int32\")\n\n# ----------------------------\n# 1) Builder (batched) to avoid memory spikes\n# ----------------------------\ndef build_residue_index(\n    split_name: str,\n    targets_df: pd.DataFrame,\n    seg_df: pd.DataFrame,\n    out_root: Path,\n    batch_targets: int = 200,\n    hard_fail: bool = True\n):\n    \"\"\"\n    Writes partitioned parquet parts to:\n      out_root/{split}/part-00000.parquet, ...\n    Returns QA dataframe.\n    \"\"\"\n    out_dir = out_root / split_name\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    # Filter to clean targets only\n    if \"rebuild_ok\" in targets_df.columns:\n        targets_ok = targets_df[targets_df[\"rebuild_ok\"] == True].copy()\n    else:\n        targets_ok = targets_df.copy()\n\n    # Quick mapping: target_id -> (sequence, L)\n    # (avoid heavy merges)\n    tid_list = targets_ok[\"target_id\"].tolist()\n    seq_map = dict(zip(targets_ok[\"target_id\"].tolist(), targets_ok[\"sequence\"].tolist()))\n    L_map   = dict(zip(targets_ok[\"target_id\"].tolist(), targets_ok[\"L\"].tolist()))\n\n    # Segment groupby\n    seg_g = seg_df.groupby(\"target_id\", sort=False)\n\n    qa_rows = []\n    part_idx = 0\n\n    # Batch processing\n    for b0 in range(0, len(tid_list), batch_targets):\n        batch = tid_list[b0:b0+batch_targets]\n\n        cols_target_id = []\n        cols_resid     = []\n        cols_resname   = []\n        cols_chain     = []\n        cols_copy      = []\n        cols_segidx    = []\n        cols_is_start  = []\n        cols_is_end    = []\n\n        for tid in batch:\n            seq = str(seq_map[tid])\n            L   = int(L_map[tid])\n\n            if tid not in seg_g.indices:\n                qa_rows.append({\n                    \"split\": split_name, \"target_id\": tid, \"L\": L,\n                    \"coverage_ok\": False, \"n_segments\": 0, \"n_rows\": 0,\n                    \"err\": \"missing segments for target_id\"\n                })\n                if hard_fail:\n                    raise RuntimeError(f\"[{split_name}] Missing segments for target_id={tid}\")\n                continue\n\n            segs = seg_g.get_group(tid).sort_values([\"start_1based\",\"seg_idx\"])\n            nseg = len(segs)\n\n            # ---- coverage validation (this is the \"Rebuild & Validate\" part here) ----\n            ok = True\n            err = \"\"\n            try:\n                # must start at 1\n                if int(segs[\"start_1based\"].iloc[0]) != 1:\n                    raise ValueError(f\"segments do not start at 1 (start={int(segs['start_1based'].iloc[0])})\")\n                # contiguous\n                prev_end = int(segs[\"end_1based\"].iloc[0])\n                for j in range(1, nseg):\n                    st = int(segs[\"start_1based\"].iloc[j])\n                    if st != prev_end + 1:\n                        raise ValueError(f\"non-contiguous segments at j={j} (start={st}, expected={prev_end+1})\")\n                    prev_end = int(segs[\"end_1based\"].iloc[j])\n                # must end at L\n                if prev_end != L:\n                    raise ValueError(f\"segments end != L (end={prev_end}, L={L})\")\n            except Exception as e:\n                ok = False\n                err = str(e)\n                qa_rows.append({\n                    \"split\": split_name, \"target_id\": tid, \"L\": L,\n                    \"coverage_ok\": False, \"n_segments\": nseg, \"n_rows\": 0,\n                    \"err\": err\n                })\n                if hard_fail:\n                    raise RuntimeError(f\"[{split_name}] Coverage validation failed for {tid}: {err}\")\n                continue\n\n            # ---- build per-residue rows ----\n            # Convert whole sequence once\n            # Use bytes array so slicing is cheap\n            seq_bytes = np.frombuffer(seq.encode(\"ascii\"), dtype=\"S1\")  # shape (L,)\n            # Safety check\n            if seq_bytes.shape[0] != L:\n                qa_rows.append({\n                    \"split\": split_name, \"target_id\": tid, \"L\": L,\n                    \"coverage_ok\": False, \"n_segments\": nseg, \"n_rows\": 0,\n                    \"err\": f\"sequence length mismatch in memory (len(seq)={seq_bytes.shape[0]}, L={L})\"\n                })\n                if hard_fail:\n                    raise RuntimeError(f\"[{split_name}] sequence length mismatch for {tid}\")\n                continue\n\n            # Accumulate arrays per segment\n            # (Total length = L, so these arrays will sum to L each target)\n            tid_arrs, resid_arrs, resname_arrs, chain_arrs, copy_arrs, segidx_arrs, is_start_arrs, is_end_arrs = [],[],[],[],[],[],[],[]\n\n            for r in segs.itertuples(index=False):\n                st = int(r.start_1based)\n                en = int(r.end_1based)\n                seglen = en - st + 1\n\n                resid = np.arange(st, en+1, dtype=np.int32)\n                resname = seq_bytes[st-1:en].astype(\"U1\")  # convert bytes -> 1-char str array\n\n                tid_arrs.append(np.full(seglen, tid, dtype=object))\n                resid_arrs.append(resid)\n                resname_arrs.append(resname)\n\n                chain_arrs.append(np.full(seglen, getattr(r, \"chain_primary\"), dtype=object))\n                copy_arrs.append(np.full(seglen, int(r.copy_idx), dtype=np.int32))\n                segidx_arrs.append(np.full(seglen, int(r.seg_idx), dtype=np.int32))\n\n                is_start = np.zeros(seglen, dtype=bool); is_start[0] = True\n                is_end   = np.zeros(seglen, dtype=bool); is_end[-1] = True\n                is_start_arrs.append(is_start)\n                is_end_arrs.append(is_end)\n\n            cols_target_id.append(np.concatenate(tid_arrs))\n            cols_resid.append(np.concatenate(resid_arrs))\n            cols_resname.append(np.concatenate(resname_arrs))\n            cols_chain.append(np.concatenate(chain_arrs))\n            cols_copy.append(np.concatenate(copy_arrs))\n            cols_segidx.append(np.concatenate(segidx_arrs))\n            cols_is_start.append(np.concatenate(is_start_arrs))\n            cols_is_end.append(np.concatenate(is_end_arrs))\n\n            qa_rows.append({\n                \"split\": split_name, \"target_id\": tid, \"L\": L,\n                \"coverage_ok\": True, \"n_segments\": nseg, \"n_rows\": L,\n                \"err\": \"\"\n            })\n\n        # Write this batch if any data\n        if cols_target_id:\n            df_part = pd.DataFrame({\n                \"target_id\": np.concatenate(cols_target_id),\n                \"resid\":     np.concatenate(cols_resid).astype(\"int32\"),\n                \"resname\":   pd.Categorical(np.concatenate(cols_resname)),\n                \"chain_primary\": pd.Categorical(np.concatenate(cols_chain)),\n                \"copy_idx\":  np.concatenate(cols_copy).astype(\"int16\"),\n                \"seg_idx\":   np.concatenate(cols_segidx).astype(\"int16\"),\n                \"is_chain_start\": np.concatenate(cols_is_start).astype(bool),\n                \"is_chain_end\":   np.concatenate(cols_is_end).astype(bool),\n            })\n\n            part_path = out_dir / f\"part-{part_idx:05d}.parquet\"\n            df_part.to_parquet(part_path, index=False)\n            part_idx += 1\n\n            # free\n            del df_part\n            gc.collect()\n\n        if (b0 // batch_targets) % 10 == 0:\n            print(f\"[{split_name}] processed {min(b0+batch_targets, len(tid_list)):,}/{len(tid_list):,} targets | parts={part_idx}\")\n\n    qa_df = pd.DataFrame(qa_rows)\n\n    # Save QA\n    qa_path = META_DIR / f\"qa_stage3_{split_name}.csv\"\n    qa_df.to_csv(qa_path, index=False)\n\n    # Print summary\n    n_total = len(tid_list)\n    n_ok = int((qa_df[\"coverage_ok\"] == True).sum())\n    n_bad = n_total - n_ok\n    print(f\"\\n[{split_name}] STAGE3 coverage_ok: {n_ok:,}/{n_total:,} (bad={n_bad:,})\")\n    if n_bad > 0:\n        print(f\"[{split_name}] Examples of failures (up to 10):\")\n        print(qa_df.loc[~qa_df[\"coverage_ok\"]].head(10)[[\"target_id\",\"L\",\"n_segments\",\"err\"]].to_string(index=False))\n\n    print(f\"[{split_name}] residue_index parts written to: {out_dir}\")\n    print(f\"[{split_name}] QA saved: {qa_path}\")\n\n    return qa_df\n\n# ----------------------------\n# 2) Run STAGE3 for train/val/test\n# ----------------------------\nprint(\"=== STAGE 3: Build residue index tables ===\")\n# Train: allow skip of bad (already filtered rebuild_ok=True in targets), hard_fail=True still safe\nqa3_train = build_residue_index(\"train\", targets2_train, seg_train, RESIDX_DIR, batch_targets=200, hard_fail=True)\nqa3_val   = build_residue_index(\"val\",   targets2_val,   seg_val,   RESIDX_DIR, batch_targets=200, hard_fail=True)\nqa3_test  = build_residue_index(\"test\",  targets2_test,  seg_test,  RESIDX_DIR, batch_targets=200, hard_fail=True)\n\n# ----------------------------\n# 3) Save a small pointer file listing where residue parts are\n# ----------------------------\nptr = {\n    \"residue_index_dir\": str(RESIDX_DIR),\n    \"train_glob\": str((RESIDX_DIR/\"train\"/\"part-*.parquet\")),\n    \"val_glob\":   str((RESIDX_DIR/\"val\"/\"part-*.parquet\")),\n    \"test_glob\":  str((RESIDX_DIR/\"test\"/\"part-*.parquet\")),\n}\n(META_DIR / \"residue_index_paths_stage3.json\").write_text(json.dumps(ptr, indent=2))\n\nprint(\"\\n[OK] STAGE 3 complete.\")\nprint(json.dumps(ptr, indent=2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:56:07.692638Z","iopub.execute_input":"2026-01-08T14:56:07.693502Z","iopub.status.idle":"2026-01-08T14:56:26.890000Z","shell.execute_reply.started":"2026-01-08T14:56:07.693474Z","shell.execute_reply":"2026-01-08T14:56:26.889130Z"}},"outputs":[{"name":"stdout","text":"=== STAGE 3: Build residue index tables ===\n[train] processed 200/5,550 targets | parts=1\n[train] processed 2,200/5,550 targets | parts=11\n[train] processed 4,200/5,550 targets | parts=21\n\n[train] STAGE3 coverage_ok: 5,550/5,550 (bad=0)\n[train] residue_index parts written to: /kaggle/working/rna3d_artifacts_v1/residue_index/train\n[train] QA saved: /kaggle/working/rna3d_artifacts_v1/meta/qa_stage3_train.csv\n[val] processed 28/28 targets | parts=1\n\n[val] STAGE3 coverage_ok: 28/28 (bad=0)\n[val] residue_index parts written to: /kaggle/working/rna3d_artifacts_v1/residue_index/val\n[val] QA saved: /kaggle/working/rna3d_artifacts_v1/meta/qa_stage3_val.csv\n[test] processed 28/28 targets | parts=1\n\n[test] STAGE3 coverage_ok: 28/28 (bad=0)\n[test] residue_index parts written to: /kaggle/working/rna3d_artifacts_v1/residue_index/test\n[test] QA saved: /kaggle/working/rna3d_artifacts_v1/meta/qa_stage3_test.csv\n\n[OK] STAGE 3 complete.\n{\n  \"residue_index_dir\": \"/kaggle/working/rna3d_artifacts_v1/residue_index\",\n  \"train_glob\": \"/kaggle/working/rna3d_artifacts_v1/residue_index/train/part-*.parquet\",\n  \"val_glob\": \"/kaggle/working/rna3d_artifacts_v1/residue_index/val/part-*.parquet\",\n  \"test_glob\": \"/kaggle/working/rna3d_artifacts_v1/residue_index/test/part-*.parquet\"\n}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Build Clean Label Tensors (Multi-Reference) for Train/Validation","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# NOTEBOOK-1 / STAGE 4 — Build Clean Label Tensors (Multi-Reference) (ONE CELL)\n#\n# Requires:\n# - OUT_DIR from previous stages (default /kaggle/working/rna3d_artifacts_v1)\n# - STAGE 2 outputs:\n#     tables/targets_train_stage2.parquet\n#     tables/targets_val_stage2.parquet\n# - Labels in memory from STAGE 1 (preferred):\n#     df_train_lbl, df_val_lbl\n#   If missing, will reload from PATHS[\"TRAIN_LBL\"] / PATHS[\"VAL_LBL\"].\n#\n# Outputs:\n# - labels_npz/train/<target_id>.npz\n# - labels_npz/val/<target_id>.npz\n# - meta/qa_stage4_train.csv\n# - meta/qa_stage4_val.csv\n# - meta/labels_manifest_train.parquet\n# - meta/labels_manifest_val.parquet\n# ============================================================\n\nimport os, re, gc, json, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Paths\n# ----------------------------\nif \"OUT_DIR\" not in globals():\n    OUT_DIR = Path(\"/kaggle/working/rna3d_artifacts_v1\")\nelse:\n    OUT_DIR = Path(OUT_DIR)\n\nTABLE_DIR = OUT_DIR / \"tables\"\nMETA_DIR  = OUT_DIR / \"meta\"\nLBL_DIR   = OUT_DIR / \"labels_npz\"\nLBL_DIR_TRAIN = LBL_DIR / \"train\"\nLBL_DIR_VAL   = LBL_DIR / \"val\"\nfor p in [TABLE_DIR, META_DIR, LBL_DIR_TRAIN, LBL_DIR_VAL]:\n    p.mkdir(parents=True, exist_ok=True)\n\ndef _req(p: Path):\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing file: {p}\")\n    return p\n\ntargets2_train = pd.read_parquet(_req(TABLE_DIR / \"targets_train_stage2.parquet\"))\ntargets2_val   = pd.read_parquet(_req(TABLE_DIR / \"targets_val_stage2.parquet\"))\n\n# normalize sequences\ntargets2_train[\"target_id\"] = targets2_train[\"target_id\"].astype(\"string\")\ntargets2_val[\"target_id\"]   = targets2_val[\"target_id\"].astype(\"string\")\ntargets2_train[\"sequence\"]  = targets2_train[\"sequence\"].astype(\"string\").str.upper().str.replace(\"T\",\"U\", regex=False)\ntargets2_val[\"sequence\"]    = targets2_val[\"sequence\"].astype(\"string\").str.upper().str.replace(\"T\",\"U\", regex=False)\n\ntargets2_train[\"L\"] = targets2_train[\"L\"].astype(\"int32\")\ntargets2_val[\"L\"]   = targets2_val[\"L\"].astype(\"int32\")\n\n# Train: keep only rebuild_ok=True (train had some failures in Stage 2)\nif \"rebuild_ok\" in targets2_train.columns:\n    targets2_train_ok = targets2_train[targets2_train[\"rebuild_ok\"] == True].copy()\nelse:\n    targets2_train_ok = targets2_train.copy()\n\ntargets2_val_ok = targets2_val.copy()  # val should be all ok\n\nprint(f\"[train] targets total={len(targets2_train):,} | using rebuild_ok={len(targets2_train_ok):,}\")\nprint(f\"[val]   targets total={len(targets2_val_ok):,}\")\n\n# ----------------------------\n# 1) Get labels DF (use Stage 1 globals if present, else reload)\n# ----------------------------\nCOORD_PAT = re.compile(r\"^[xyz]_\\d+$\")\n\ndef _read_labels_fast(csv_path: str | Path, split_name: str):\n    csv_path = Path(csv_path)\n    cols = pd.read_csv(csv_path, nrows=0).columns.tolist()\n    coord_cols = [c for c in cols if COORD_PAT.match(c)]\n    required = {\"ID\",\"resname\",\"resid\"}\n    missing_req = sorted(list(required - set(cols)))\n    if missing_req:\n        raise ValueError(f\"[{split_name}] labels missing required columns: {missing_req}\")\n    dtype = {}\n    for c in cols:\n        if c in coord_cols:\n            dtype[c] = \"float32\"\n        elif c == \"resid\":\n            dtype[c] = \"int32\"\n        elif c == \"copy\":\n            dtype[c] = \"int16\"\n        else:\n            dtype[c] = \"string\"\n    df = pd.read_csv(csv_path, dtype=dtype)\n    # standardize\n    df[\"ID\"] = df[\"ID\"].astype(\"string\").fillna(\"\").str.strip()\n    df[\"resname\"] = df[\"resname\"].astype(\"string\").fillna(\"\").str.strip().str.upper().str.replace(\"T\",\"U\", regex=False)\n    # extract target_id from ID\n    sp = df[\"ID\"].str.rsplit(\"_\", n=1, expand=True)\n    if sp.shape[1] == 2:\n        df[\"target_id\"] = sp[0].astype(\"string\")\n        df[\"resid_from_id\"] = pd.to_numeric(sp[1], errors=\"coerce\").astype(\"Int32\")\n    else:\n        df[\"target_id\"] = pd.NA\n        df[\"resid_from_id\"] = pd.NA\n    return df, coord_cols\n\n# locate PATHS if needed for reload\nif \"PATHS\" not in globals():\n    # fallback: assume competition root fixed\n    COMP_ROOT = Path(\"/kaggle/input/stanford-rna-3d-folding-2\")\n    PATHS = {\n        \"TRAIN_LBL\": str(COMP_ROOT / \"train_labels.csv\"),\n        \"VAL_LBL\": str(COMP_ROOT / \"validation_labels.csv\"),\n    }\n\nif \"df_train_lbl\" in globals() and isinstance(df_train_lbl, pd.DataFrame):\n    df_train_lbl_use = df_train_lbl\n    train_coord_cols = [c for c in df_train_lbl_use.columns if COORD_PAT.match(c)]\nelse:\n    df_train_lbl_use, train_coord_cols = _read_labels_fast(PATHS[\"TRAIN_LBL\"], \"train_labels\")\n\nif \"df_val_lbl\" in globals() and isinstance(df_val_lbl, pd.DataFrame):\n    df_val_lbl_use = df_val_lbl\n    val_coord_cols = [c for c in df_val_lbl_use.columns if COORD_PAT.match(c)]\nelse:\n    df_val_lbl_use, val_coord_cols = _read_labels_fast(PATHS[\"VAL_LBL\"], \"validation_labels\")\n\n# ensure target_id exists (in case Stage1 used different extraction)\nfor df in [df_train_lbl_use, df_val_lbl_use]:\n    if \"target_id\" not in df.columns:\n        sp = df[\"ID\"].astype(\"string\").str.rsplit(\"_\", n=1, expand=True)\n        df[\"target_id\"] = sp[0].astype(\"string\")\n\n# determine n_ref\ndef _nref(coord_cols):\n    if len(coord_cols) % 3 != 0:\n        raise ValueError(f\"Coord cols not multiple of 3: {len(coord_cols)}\")\n    return len(coord_cols) // 3\n\nnref_train = _nref(train_coord_cols)\nnref_val   = _nref(val_coord_cols)\nprint(f\"[train_labels] coord_cols={len(train_coord_cols)} => n_ref={nref_train}\")\nprint(f\"[val_labels]   coord_cols={len(val_coord_cols)} => n_ref={nref_val}\")\n\n# build ordered triplets per ref k: (x_k,y_k,z_k)\ndef _coord_triplets(coord_cols):\n    # build dict k -> {x,y,z}\n    d = {}\n    for c in coord_cols:\n        axis, k = c.split(\"_\", 1)\n        k = int(k)\n        d.setdefault(k, {})[axis] = c\n    ks = sorted(d.keys())\n    triplets = []\n    for k in ks:\n        if not all(a in d[k] for a in [\"x\",\"y\",\"z\"]):\n            raise ValueError(f\"Missing xyz for ref={k}\")\n        triplets.append((k, d[k][\"x\"], d[k][\"y\"], d[k][\"z\"]))\n    return triplets\n\ntrain_triplets = _coord_triplets(train_coord_cols)\nval_triplets   = _coord_triplets(val_coord_cols)\n\n# ----------------------------\n# 2) Core builder: per target -> npz\n# ----------------------------\ndef build_labels_npz(split_name: str, targets_ok: pd.DataFrame, df_lbl: pd.DataFrame, triplets, out_dir: Path,\n                     hard_fail: bool = True, print_every: int = 200):\n\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    # maps for quick access\n    seq_map = dict(zip(targets_ok[\"target_id\"].tolist(), targets_ok[\"sequence\"].tolist()))\n    L_map   = dict(zip(targets_ok[\"target_id\"].tolist(), targets_ok[\"L\"].tolist()))\n    tids = targets_ok[\"target_id\"].tolist()\n    tid_set = set(tids)\n\n    # filter labels to only these targets\n    df = df_lbl.copy()\n    df[\"target_id\"] = df[\"target_id\"].astype(\"string\")\n    df = df[df[\"target_id\"].isin(tid_set)]\n\n    # ensure resid/resname types\n    df[\"resid\"] = pd.to_numeric(df[\"resid\"], errors=\"coerce\").astype(\"Int32\")\n    df[\"resname\"] = df[\"resname\"].astype(\"string\").fillna(\"\").str.strip().str.upper().str.replace(\"T\",\"U\", regex=False)\n\n    # groupby\n    g = df.groupby(\"target_id\", sort=False)\n\n    qa_rows = []\n    manifest_rows = []\n\n    done = 0\n    for tid in tids:\n        seq = str(seq_map[tid])\n        L = int(L_map[tid])\n\n        ok = True\n        err = \"\"\n        npz_path = out_dir / f\"{tid}.npz\"\n\n        try:\n            if tid not in g.indices:\n                raise ValueError(\"no label rows for target_id\")\n\n            part = g.get_group(tid)\n\n            # sort by resid\n            part = part.sort_values(\"resid\", kind=\"mergesort\")\n\n            # coverage check\n            resid = part[\"resid\"].to_numpy(dtype=np.int32, copy=False)\n            if resid.size != L:\n                raise ValueError(f\"row count != L (rows={resid.size}, L={L})\")\n            if resid[0] != 1 or resid[-1] != L:\n                raise ValueError(f\"resid range not 1..L (min={resid[0]}, max={resid[-1]})\")\n            # unique & consecutive\n            if np.any(np.diff(resid) != 1):\n                raise ValueError(\"resid not strictly consecutive 1..L\")\n\n            # resname check against sequence\n            # compare as bytes for speed\n            seq_arr = np.frombuffer(seq.encode(\"ascii\"), dtype=\"S1\").astype(\"U1\")\n            resname = part[\"resname\"].to_numpy(dtype=\"U1\", copy=False)\n            if seq_arr.shape[0] != L:\n                raise ValueError(\"sequence length mismatch in memory\")\n            mismatch = np.nonzero(resname != seq_arr)[0]\n            if mismatch.size > 0:\n                j = int(mismatch[0])\n                raise ValueError(f\"resname mismatch at resid={j+1}: label={resname[j]} vs seq={seq_arr[j]}\")\n\n            # build coords_ref (n_ref, L, 3)\n            n_ref = len(triplets)\n            coords = np.empty((n_ref, L, 3), dtype=np.float32)\n\n            for ri, (k, xcol, ycol, zcol) in enumerate(triplets):\n                x = part[xcol].to_numpy(dtype=np.float32, copy=False)\n                y = part[ycol].to_numpy(dtype=np.float32, copy=False)\n                z = part[zcol].to_numpy(dtype=np.float32, copy=False)\n                coords[ri, :, 0] = x\n                coords[ri, :, 1] = y\n                coords[ri, :, 2] = z\n\n            mask_valid = np.isfinite(coords).all(axis=2)  # (n_ref, L)\n\n            # write npz (fast)\n            # store resname_seq so later stages don't need to re-read sequences parquet for validation\n            np.savez(\n                npz_path,\n                target_id=str(tid),\n                L=np.int32(L),\n                coords_ref=coords,             # (n_ref,L,3)\n                mask_valid=mask_valid,         # (n_ref,L)\n                resname_seq=seq_arr,           # (L,)\n            )\n\n        except Exception as e:\n            ok = False\n            err = str(e)\n            if hard_fail:\n                raise\n\n        qa_rows.append({\n            \"split\": split_name,\n            \"target_id\": tid,\n            \"L\": L,\n            \"n_ref\": len(triplets),\n            \"ok\": ok,\n            \"npz_path\": str(npz_path) if ok else \"\",\n            \"err\": err,\n        })\n\n        if ok:\n            manifest_rows.append({\n                \"target_id\": tid,\n                \"L\": L,\n                \"n_ref\": len(triplets),\n                \"npz_path\": str(npz_path),\n            })\n\n        done += 1\n        if done % print_every == 0:\n            print(f\"[{split_name}] written {done:,}/{len(tids):,} npz\")\n\n        # periodic GC\n        if done % (print_every * 5) == 0:\n            gc.collect()\n\n    qa_df = pd.DataFrame(qa_rows)\n    manifest_df = pd.DataFrame(manifest_rows)\n\n    # summary\n    n_ok = int(qa_df[\"ok\"].sum())\n    n_bad = len(qa_df) - n_ok\n    print(f\"\\n[{split_name}] STAGE4 labels ok: {n_ok:,}/{len(qa_df):,} (bad={n_bad:,})\")\n    if n_bad > 0:\n        print(f\"[{split_name}] failures (up to 10):\")\n        print(qa_df.loc[~qa_df[\"ok\"]].head(10)[[\"target_id\",\"L\",\"err\"]].to_string(index=False))\n\n    return qa_df, manifest_df\n\n# ----------------------------\n# 3) Run for train + val\n# ----------------------------\nprint(\"\\n=== STAGE 4: Build label tensors (npz) ===\")\nqa4_train, manifest_train = build_labels_npz(\n    split_name=\"train\",\n    targets_ok=targets2_train_ok,\n    df_lbl=df_train_lbl_use,\n    triplets=train_triplets,\n    out_dir=LBL_DIR_TRAIN,\n    hard_fail=False,      # keep running and report; you can set True once you trust everything\n    print_every=200\n)\n\nqa4_val, manifest_val = build_labels_npz(\n    split_name=\"val\",\n    targets_ok=targets2_val_ok,\n    df_lbl=df_val_lbl_use,\n    triplets=val_triplets,\n    out_dir=LBL_DIR_VAL,\n    hard_fail=True,\n    print_every=28\n)\n\n# ----------------------------\n# 4) Save QA + manifest\n# ----------------------------\nqa4_train.to_csv(META_DIR / \"qa_stage4_train.csv\", index=False)\nqa4_val.to_csv(  META_DIR / \"qa_stage4_val.csv\",   index=False)\n\nmanifest_train.to_parquet(META_DIR / \"labels_manifest_train.parquet\", index=False)\nmanifest_val.to_parquet(  META_DIR / \"labels_manifest_val.parquet\",   index=False)\n\n# Save pointers\nptr = {\n    \"labels_npz_train_dir\": str(LBL_DIR_TRAIN),\n    \"labels_npz_val_dir\": str(LBL_DIR_VAL),\n    \"manifest_train\": str(META_DIR / \"labels_manifest_train.parquet\"),\n    \"manifest_val\": str(META_DIR / \"labels_manifest_val.parquet\"),\n    \"qa_train\": str(META_DIR / \"qa_stage4_train.csv\"),\n    \"qa_val\": str(META_DIR / \"qa_stage4_val.csv\"),\n}\n(META_DIR / \"labels_paths_stage4.json\").write_text(json.dumps(ptr, indent=2))\n\nprint(\"\\n[OK] STAGE 4 complete.\")\nprint(json.dumps(ptr, indent=2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T15:02:40.353525Z","iopub.execute_input":"2026-01-08T15:02:40.353891Z","iopub.status.idle":"2026-01-08T15:03:09.586376Z","shell.execute_reply.started":"2026-01-08T15:02:40.353865Z","shell.execute_reply":"2026-01-08T15:03:09.585547Z"}},"outputs":[{"name":"stdout","text":"[train] targets total=5,716 | using rebuild_ok=5,550\n[val]   targets total=28\n[train_labels] coord_cols=3 => n_ref=1\n[val_labels]   coord_cols=120 => n_ref=40\n\n=== STAGE 4: Build label tensors (npz) ===\n[train] written 200/5,550 npz\n[train] written 400/5,550 npz\n[train] written 600/5,550 npz\n[train] written 800/5,550 npz\n[train] written 1,000/5,550 npz\n[train] written 1,200/5,550 npz\n[train] written 1,400/5,550 npz\n[train] written 1,600/5,550 npz\n[train] written 1,800/5,550 npz\n[train] written 2,000/5,550 npz\n[train] written 2,200/5,550 npz\n[train] written 2,400/5,550 npz\n[train] written 2,600/5,550 npz\n[train] written 2,800/5,550 npz\n[train] written 3,000/5,550 npz\n[train] written 3,200/5,550 npz\n[train] written 3,400/5,550 npz\n[train] written 3,600/5,550 npz\n[train] written 3,800/5,550 npz\n[train] written 4,000/5,550 npz\n[train] written 4,200/5,550 npz\n[train] written 4,400/5,550 npz\n[train] written 4,600/5,550 npz\n[train] written 4,800/5,550 npz\n[train] written 5,000/5,550 npz\n[train] written 5,200/5,550 npz\n[train] written 5,400/5,550 npz\n\n[train] STAGE4 labels ok: 5,550/5,550 (bad=0)\n[val] written 28/28 npz\n\n[val] STAGE4 labels ok: 28/28 (bad=0)\n\n[OK] STAGE 4 complete.\n{\n  \"labels_npz_train_dir\": \"/kaggle/working/rna3d_artifacts_v1/labels_npz/train\",\n  \"labels_npz_val_dir\": \"/kaggle/working/rna3d_artifacts_v1/labels_npz/val\",\n  \"manifest_train\": \"/kaggle/working/rna3d_artifacts_v1/meta/labels_manifest_train.parquet\",\n  \"manifest_val\": \"/kaggle/working/rna3d_artifacts_v1/meta/labels_manifest_val.parquet\",\n  \"qa_train\": \"/kaggle/working/rna3d_artifacts_v1/meta/qa_stage4_train.csv\",\n  \"qa_val\": \"/kaggle/working/rna3d_artifacts_v1/meta/qa_stage4_val.csv\"\n}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Build TBM Template Index + Parse MSA (Train/Val) + Export Artifacts Dataset","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# NOTEBOOK-1 / STAGE 5 — Build TBM Template Index + Parse MSA (Train/Val) + Export Artifacts Manifest (ONE CELL)\n#\n# Inputs (competition):\n# - /kaggle/input/stanford-rna-3d-folding-2/extra/rna_metadata.csv\n# - /kaggle/input/stanford-rna-3d-folding-2/PDB_RNA/*.cif\n# - /kaggle/input/stanford-rna-3d-folding-2/MSA/{target_id}.MSA.fasta\n#\n# Requires previous stages outputs in OUT_DIR (default: /kaggle/working/rna3d_artifacts_v1):\n# - tables/targets_train_stage2.parquet\n# - tables/targets_val_stage2.parquet\n# - meta/qa_stage2_*.csv, meta/qa_stage3_*.csv, meta/qa_stage4_*.csv (optional but expected)\n#\n# Outputs:\n# - OUT_DIR/tbm/template_index.parquet\n# - OUT_DIR/msa/msa_index_train.parquet\n# - OUT_DIR/msa/msa_index_val.parquet\n# - OUT_DIR/meta/artifacts_manifest_stage5.json\n# - OUT_DIR/meta/stage5_config.json\n#\n# Notes:\n# - Train has some rebuild_ok=False from Stage2; we index MSAs for rebuild_ok=True only (recommended).\n# - MSA parsing here is \"light\": sanity + stats + stable pointers. Full MSA usage happens in Notebook-2/3.\n# ============================================================\n\nimport os, re, gc, json, time, warnings\nfrom pathlib import Path\nfrom typing import Dict, Any, Tuple, Optional\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n\n# ----------------------------\n# 0) Paths\n# ----------------------------\nif \"PATHS\" not in globals():\n    COMP_ROOT = Path(\"/kaggle/input/stanford-rna-3d-folding-2\")\n    PATHS = {\n        \"COMP_ROOT\": str(COMP_ROOT),\n        \"MSA_DIR\": str(COMP_ROOT / \"MSA\"),\n        \"PDB_RNA_DIR\": str(COMP_ROOT / \"PDB_RNA\"),\n        \"EXTRA_DIR\": str(COMP_ROOT / \"extra\"),\n        \"RNA_METADATA\": str(COMP_ROOT / \"extra\" / \"rna_metadata.csv\"),\n    }\n\nCOMP_ROOT   = Path(PATHS.get(\"COMP_ROOT\", \"/kaggle/input/stanford-rna-3d-folding-2\"))\nMSA_DIR     = Path(PATHS.get(\"MSA_DIR\", COMP_ROOT / \"MSA\"))\nPDB_RNA_DIR = Path(PATHS.get(\"PDB_RNA_DIR\", COMP_ROOT / \"PDB_RNA\"))\nEXTRA_DIR   = Path(PATHS.get(\"EXTRA_DIR\", COMP_ROOT / \"extra\"))\nRNA_META_CSV= Path(PATHS.get(\"RNA_METADATA\", EXTRA_DIR / \"rna_metadata.csv\"))\n\nif \"OUT_DIR\" not in globals():\n    OUT_DIR = Path(\"/kaggle/working/rna3d_artifacts_v1\")\nelse:\n    OUT_DIR = Path(OUT_DIR)\n\nTABLE_DIR = OUT_DIR / \"tables\"\nMETA_DIR  = OUT_DIR / \"meta\"\nTBM_DIR   = OUT_DIR / \"tbm\"\nMSA_OUT   = OUT_DIR / \"msa\"\n\nfor p in [TABLE_DIR, META_DIR, TBM_DIR, MSA_OUT]:\n    p.mkdir(parents=True, exist_ok=True)\n\ndef _req(p: Path, what=\"file/dir\"):\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing {what}: {p}\")\n    return p\n\n_req(MSA_DIR, \"MSA_DIR\")\n_req(PDB_RNA_DIR, \"PDB_RNA_DIR\")\n_req(RNA_META_CSV, \"rna_metadata.csv\")\n_req(TABLE_DIR / \"targets_train_stage2.parquet\", \"targets_train_stage2.parquet\")\n_req(TABLE_DIR / \"targets_val_stage2.parquet\", \"targets_val_stage2.parquet\")\n\n# ----------------------------\n# 1) Build TBM template index from rna_metadata.csv + has_cif\n# ----------------------------\nprint(\"=== STAGE 5.1: Build TBM template_index.parquet ===\")\n\n# Build a fast lookup set for existing CIF stems (case-insensitive)\ncif_stems = set()\n# Using iterdir is faster than glob on some FS\nfor p in PDB_RNA_DIR.iterdir():\n    if p.is_file() and p.suffix.lower() == \".cif\":\n        cif_stems.add(p.stem.lower())\n\nprint(f\"[PDB_RNA] cif files found: {len(cif_stems):,} (stems cached)\")\n\n# Read rna_metadata with usecols intersection (memory-safe)\nmeta_cols = pd.read_csv(RNA_META_CSV, nrows=0).columns.tolist()\n\nwanted = [\n    \"target_id\",\"pdb_id\",\"chain_id\",\"auth_chain_id\",\"entity_id\",\"entity_type\",\n    \"sequence\",\"canonical_sequence\",\"full_sequence\",\n    \"temporal_cutoff\",\"resolution\",\"method\",\"title\",\"keyword_ribosome\",\n    \"group_id\",\"seq_group_id\",\n    \"mmseqs_0.900\",\"mmseqs_0.950\",\"mmseqs_0.850\",\n    \"composition_rna_fraction\",\"composition_na_hybrid_fraction\",\n    \"length\",\"length_observed\",\"length_expected\",\"fraction_observed\",\n    \"missing_residues\",\"nonstandard_residues\",\"undefined_residues\",\"unexpected_residues\",\n    \"total_structuredness_adjusted\",\"intra_chain_structuredness_adjusted\",\"inter_chain_structuredness_adjusted\",\n]\nusecols = [c for c in wanted if c in meta_cols]\n# Always ensure these exist if present\nif \"pdb_id\" not in usecols and \"pdb_id\" in meta_cols:\n    usecols.append(\"pdb_id\")\nif \"sequence\" not in usecols and \"sequence\" in meta_cols:\n    usecols.append(\"sequence\")\n\n# dtype suggestions (best-effort)\ndtype = {}\nfor c in usecols:\n    if c in [\"keyword_ribosome\",\"missing_residues\",\"nonstandard_residues\",\"undefined_residues\",\"unexpected_residues\"]:\n        dtype[c] = \"boolean\"\n    elif c in [\"length\",\"length_observed\",\"length_expected\"]:\n        dtype[c] = \"Int32\"\n    elif c in [\"resolution\",\"composition_rna_fraction\",\"composition_na_hybrid_fraction\",\n               \"fraction_observed\",\"total_structuredness_adjusted\",\n               \"intra_chain_structuredness_adjusted\",\"inter_chain_structuredness_adjusted\"]:\n        dtype[c] = \"float32\"\n    else:\n        dtype[c] = \"string\"\n\nt0 = time.time()\ndf_meta = pd.read_csv(RNA_META_CSV, usecols=usecols, dtype=dtype, low_memory=False)\nprint(f\"[rna_metadata] loaded rows={len(df_meta):,} cols={len(df_meta.columns)} in {time.time()-t0:.1f}s\")\n\n# Normalize key fields\nif \"pdb_id\" in df_meta.columns:\n    df_meta[\"pdb_id\"] = df_meta[\"pdb_id\"].astype(\"string\").str.strip()\n    df_meta[\"pdb_id_lc\"] = df_meta[\"pdb_id\"].str.lower()\n\nif \"sequence\" in df_meta.columns:\n    df_meta[\"sequence\"] = df_meta[\"sequence\"].astype(\"string\").str.strip().str.upper().str.replace(\"T\",\"U\", regex=False)\n    df_meta[\"seq_len\"] = df_meta[\"sequence\"].str.len().astype(\"Int32\")\n\n# has_cif flag\nif \"pdb_id_lc\" in df_meta.columns:\n    df_meta[\"has_cif\"] = df_meta[\"pdb_id_lc\"].isin(cif_stems)\nelse:\n    df_meta[\"has_cif\"] = False\n\n# Normalize temporal_cutoff if present\nif \"temporal_cutoff\" in df_meta.columns:\n    df_meta[\"temporal_cutoff_dt\"] = pd.to_datetime(df_meta[\"temporal_cutoff\"], errors=\"coerce\")\n\n# Save template index\ntemplate_path = TBM_DIR / \"template_index.parquet\"\ndf_meta.to_parquet(template_path, index=False)\nprint(f\"[OK] Saved: {template_path}\")\n\n# ----------------------------\n# 2) Parse MSA for Train/Val: build msa_index_{split}.parquet (stats + pointers)\n# ----------------------------\nprint(\"\\n=== STAGE 5.2: Parse MSA (Train/Val) -> msa_index_{split}.parquet ===\")\n\ntargets_train2 = pd.read_parquet(TABLE_DIR / \"targets_train_stage2.parquet\")\ntargets_val2   = pd.read_parquet(TABLE_DIR / \"targets_val_stage2.parquet\")\n\ntargets_train2[\"target_id\"] = targets_train2[\"target_id\"].astype(\"string\")\ntargets_val2[\"target_id\"]   = targets_val2[\"target_id\"].astype(\"string\")\n\n# Use only rebuild_ok=True for train\nif \"rebuild_ok\" in targets_train2.columns:\n    train_ids = targets_train2.loc[targets_train2[\"rebuild_ok\"] == True, \"target_id\"].tolist()\nelse:\n    train_ids = targets_train2[\"target_id\"].tolist()\n\nval_ids = targets_val2[\"target_id\"].tolist()\n\nprint(f\"[train] indexing MSA for targets: {len(train_ids):,}\")\nprint(f\"[val]   indexing MSA for targets: {len(val_ids):,}\")\n\n# Lightweight MSA scanner (fast + safe)\n# We do NOT build per-position arrays here; we just validate + compute robust summary stats.\ndef scan_msa_file(path: Path, max_chars: int = 3_000_000) -> Dict[str, Any]:\n    \"\"\"\n    Returns:\n      exists, parse_ok, n_seqs_est, aln_len_first, chars_scanned, gaps_scanned, gap_frac_est, truncated\n    Notes:\n      - If file is huge, we stop after max_chars of sequence characters to keep runtime reasonable.\n      - n_seqs_est may be undercounted if truncated early (flagged by truncated=True).\n    \"\"\"\n    if not path.exists():\n        return dict(exists=False, parse_ok=False, n_seqs_est=0, aln_len_first=np.nan,\n                    chars_scanned=0, gaps_scanned=0, gap_frac_est=np.nan, truncated=False, err=\"missing\")\n\n    n_headers = 0\n    aln_len_first = None\n    chars = 0\n    gaps = 0\n    truncated = False\n    err = \"\"\n\n    try:\n        with path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            in_seq = False\n            cur_len_first = 0\n            first_seq_done = False\n\n            for line in f:\n                if line.startswith(\">\"):\n                    n_headers += 1\n                    in_seq = True\n                    continue\n                if not in_seq:\n                    continue\n                s = line.strip()\n                if not s:\n                    continue\n\n                # For first sequence length estimate, count all sequence chars until next header.\n                if not first_seq_done:\n                    cur_len_first += len(s)\n                # scan stats (cap by max_chars)\n                if chars < max_chars:\n                    # count gaps and chars\n                    gaps += s.count(\"-\")\n                    chars += len(s)\n                else:\n                    truncated = True\n                    # we can still count headers if we want, but that requires reading rest of file;\n                    # keep it fast: break.\n                    first_seq_done = True\n                    break\n\n            # If we never met next header, first seq length is what we counted\n            if aln_len_first is None:\n                aln_len_first = cur_len_first if cur_len_first > 0 else np.nan\n\n        gap_frac = (gaps / chars) if chars > 0 else np.nan\n        return dict(\n            exists=True,\n            parse_ok=True,\n            n_seqs_est=n_headers,\n            aln_len_first=float(aln_len_first) if aln_len_first is not None else np.nan,\n            chars_scanned=int(chars),\n            gaps_scanned=int(gaps),\n            gap_frac_est=float(gap_frac) if gap_frac == gap_frac else np.nan,\n            truncated=bool(truncated),\n            err=\"\"\n        )\n    except Exception as e:\n        err = str(e)\n        return dict(exists=True, parse_ok=False, n_seqs_est=n_headers, aln_len_first=np.nan,\n                    chars_scanned=int(chars), gaps_scanned=int(gaps), gap_frac_est=np.nan,\n                    truncated=bool(truncated), err=err)\n\ndef build_msa_index(split_name: str, target_ids, max_chars: int = 3_000_000, print_every: int = 500):\n    rows = []\n    t0 = time.time()\n\n    for i, tid in enumerate(target_ids, 1):\n        msa_path = MSA_DIR / f\"{tid}.MSA.fasta\"\n        stats = scan_msa_file(msa_path, max_chars=max_chars)\n\n        rows.append({\n            \"split\": split_name,\n            \"target_id\": str(tid),\n            \"msa_path\": str(msa_path),\n            **stats\n        })\n\n        if i % print_every == 0:\n            ok = sum(1 for r in rows if r[\"exists\"] and r[\"parse_ok\"])\n            miss = sum(1 for r in rows if not r[\"exists\"])\n            print(f\"[{split_name}] {i:,}/{len(target_ids):,} | ok={ok:,} | missing={miss:,} | elapsed={time.time()-t0:.1f}s\")\n\n    df = pd.DataFrame(rows)\n    # simple QA summary\n    n_total = len(df)\n    n_exist = int(df[\"exists\"].sum())\n    n_ok = int((df[\"exists\"] & df[\"parse_ok\"]).sum())\n    n_trunc = int(df[\"truncated\"].sum())\n    print(f\"\\n[{split_name}] msa_index: total={n_total:,} | exists={n_exist:,} | parse_ok={n_ok:,} | truncated={n_trunc:,}\")\n    return df\n\nmsa_train_df = build_msa_index(\"train\", train_ids, max_chars=3_000_000, print_every=500)\nmsa_val_df   = build_msa_index(\"val\",   val_ids,   max_chars=3_000_000, print_every=28)\n\nmsa_train_path = MSA_OUT / \"msa_index_train.parquet\"\nmsa_val_path   = MSA_OUT / \"msa_index_val.parquet\"\nmsa_train_df.to_parquet(msa_train_path, index=False)\nmsa_val_df.to_parquet(msa_val_path, index=False)\n\nprint(f\"[OK] Saved: {msa_train_path}\")\nprint(f\"[OK] Saved: {msa_val_path}\")\n\n# ----------------------------\n# 3) Export manifest/config (for Notebook-2/3 stability)\n# ----------------------------\nprint(\"\\n=== STAGE 5.3: Write manifest/config ===\")\n\ncfg = {\n    \"stage5_time_utc\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()),\n    \"paths\": {\n        \"OUT_DIR\": str(OUT_DIR),\n        \"TBM_DIR\": str(TBM_DIR),\n        \"MSA_OUT\": str(MSA_OUT),\n        \"PDB_RNA_DIR\": str(PDB_RNA_DIR),\n        \"MSA_DIR\": str(MSA_DIR),\n        \"RNA_META_CSV\": str(RNA_META_CSV),\n    },\n    \"notes\": {\n        \"train_policy\": \"Use only rebuild_ok=True targets for training downstream.\",\n        \"msa_policy\": \"MSA parsed to stable pointers + lightweight stats (full MSA usage occurs in Notebook-2/3).\",\n    },\n    \"stats\": {\n        \"n_train_targets_stage2_total\": int(len(targets_train2)),\n        \"n_train_targets_rebuild_ok\": int(len(train_ids)),\n        \"n_val_targets\": int(len(val_ids)),\n        \"n_template_rows\": int(len(df_meta)),\n        \"n_pdb_cif_files\": int(len(cif_stems)),\n    }\n}\n\nmanifest = {\n    \"tables\": {\n        \"targets_train_stage2\": str(TABLE_DIR / \"targets_train_stage2.parquet\"),\n        \"targets_val_stage2\": str(TABLE_DIR / \"targets_val_stage2.parquet\"),\n        \"targets_test_stage2\": str(TABLE_DIR / \"targets_test_stage2.parquet\") if (TABLE_DIR / \"targets_test_stage2.parquet\").exists() else \"\",\n        \"segments_train\": str(TABLE_DIR / \"segments_train.parquet\"),\n        \"segments_val\": str(TABLE_DIR / \"segments_val.parquet\"),\n        \"segments_test\": str(TABLE_DIR / \"segments_test.parquet\"),\n    },\n    \"labels\": {\n        \"labels_npz_train_dir\": str(OUT_DIR / \"labels_npz\" / \"train\"),\n        \"labels_npz_val_dir\": str(OUT_DIR / \"labels_npz\" / \"val\"),\n        \"manifest_train\": str(META_DIR / \"labels_manifest_train.parquet\") if (META_DIR / \"labels_manifest_train.parquet\").exists() else \"\",\n        \"manifest_val\": str(META_DIR / \"labels_manifest_val.parquet\") if (META_DIR / \"labels_manifest_val.parquet\").exists() else \"\",\n    },\n    \"residue_index\": {\n        \"residue_index_dir\": str(OUT_DIR / \"residue_index\"),\n        \"train_glob\": str(OUT_DIR / \"residue_index\" / \"train\" / \"part-*.parquet\"),\n        \"val_glob\": str(OUT_DIR / \"residue_index\" / \"val\" / \"part-*.parquet\"),\n        \"test_glob\": str(OUT_DIR / \"residue_index\" / \"test\" / \"part-*.parquet\"),\n    },\n    \"tbm\": {\n        \"template_index\": str(template_path),\n    },\n    \"msa\": {\n        \"msa_index_train\": str(msa_train_path),\n        \"msa_index_val\": str(msa_val_path),\n    }\n}\n\n# Atomic JSON write to avoid partial JSON issues\ndef write_json_atomic(path: Path, obj: Dict[str, Any]):\n    tmp = path.with_suffix(path.suffix + \".tmp\")\n    tmp.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n    tmp.replace(path)\n\nwrite_json_atomic(META_DIR / \"stage5_config.json\", cfg)\nwrite_json_atomic(META_DIR / \"artifacts_manifest_stage5.json\", manifest)\n\nprint(f\"[OK] Wrote: {META_DIR/'stage5_config.json'}\")\nprint(f\"[OK] Wrote: {META_DIR/'artifacts_manifest_stage5.json'}\")\n\nprint(\"\\n[OK] STAGE 5 complete.\")\nprint(\"Next (recommended): Click 'Save Version' on this notebook output to create a reusable Kaggle Dataset from:\")\nprint(f\"  {OUT_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T15:09:31.435918Z","iopub.execute_input":"2026-01-08T15:09:31.436290Z","iopub.status.idle":"2026-01-08T15:13:36.040015Z","shell.execute_reply.started":"2026-01-08T15:09:31.436266Z","shell.execute_reply":"2026-01-08T15:13:36.039014Z"}},"outputs":[{"name":"stdout","text":"=== STAGE 5.1: Build TBM template_index.parquet ===\n[PDB_RNA] cif files found: 9,564 (stems cached)\n[rna_metadata] loaded rows=26,255 cols=31 in 1.8s\n[OK] Saved: /kaggle/working/rna3d_artifacts_v1/tbm/template_index.parquet\n\n=== STAGE 5.2: Parse MSA (Train/Val) -> msa_index_{split}.parquet ===\n[train] indexing MSA for targets: 5,550\n[val]   indexing MSA for targets: 28\n[train] 500/5,550 | ok=500 | missing=0 | elapsed=10.9s\n[train] 1,000/5,550 | ok=1,000 | missing=0 | elapsed=28.5s\n[train] 1,500/5,550 | ok=1,500 | missing=0 | elapsed=43.2s\n[train] 2,000/5,550 | ok=2,000 | missing=0 | elapsed=65.8s\n[train] 2,500/5,550 | ok=2,500 | missing=0 | elapsed=82.4s\n[train] 3,000/5,550 | ok=3,000 | missing=0 | elapsed=105.0s\n[train] 3,500/5,550 | ok=3,500 | missing=0 | elapsed=122.1s\n[train] 4,000/5,550 | ok=4,000 | missing=0 | elapsed=144.9s\n[train] 4,500/5,550 | ok=4,500 | missing=0 | elapsed=166.6s\n[train] 5,000/5,550 | ok=5,000 | missing=0 | elapsed=182.1s\n[train] 5,500/5,550 | ok=5,500 | missing=0 | elapsed=205.6s\n\n[train] msa_index: total=5,550 | exists=5,550 | parse_ok=5,550 | truncated=1,810\n[val] 28/28 | ok=28 | missing=0 | elapsed=0.4s\n\n[val] msa_index: total=28 | exists=28 | parse_ok=28 | truncated=1\n[OK] Saved: /kaggle/working/rna3d_artifacts_v1/msa/msa_index_train.parquet\n[OK] Saved: /kaggle/working/rna3d_artifacts_v1/msa/msa_index_val.parquet\n\n=== STAGE 5.3: Write manifest/config ===\n[OK] Wrote: /kaggle/working/rna3d_artifacts_v1/meta/stage5_config.json\n[OK] Wrote: /kaggle/working/rna3d_artifacts_v1/meta/artifacts_manifest_stage5.json\n\n[OK] STAGE 5 complete.\nNext (recommended): Click 'Save Version' on this notebook output to create a reusable Kaggle Dataset from:\n  /kaggle/working/rna3d_artifacts_v1\n","output_type":"stream"}],"execution_count":9}]}