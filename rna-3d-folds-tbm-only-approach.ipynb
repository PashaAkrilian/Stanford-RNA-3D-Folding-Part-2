{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed68a5ed",
   "metadata": {
    "_cell_guid": "0ec49051-b8ac-4f27-b152-0c2fe59d4b5c",
    "_uuid": "7e2d63ef-4429-4722-95ee-6681af62a6d5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-09T18:10:50.297421Z",
     "iopub.status.busy": "2026-01-09T18:10:50.297005Z",
     "iopub.status.idle": "2026-01-09T18:11:04.305706Z",
     "shell.execute_reply": "2026-01-09T18:11:04.304300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 14.0184,
     "end_time": "2026-01-09T18:11:04.307659",
     "exception": false,
     "start_time": "2026-01-09T18:10:50.289259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data files...\n",
      "Loaded 5716 training sequences, 28 validation sequences, and 28 test sequences\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial import distance_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nLoading data files...\")\n",
    "train_seqs = pd.read_csv('/kaggle/input/stanford-rna-3d-folding-2/train_sequences.csv')\n",
    "valid_seqs = pd.read_csv('/kaggle/input/stanford-rna-3d-folding-2/validation_sequences.csv')\n",
    "test_seqs = pd.read_csv('/kaggle/input/stanford-rna-3d-folding-2/test_sequences.csv')\n",
    "train_labels = pd.read_csv('/kaggle/input/stanford-rna-3d-folding-2/train_labels.csv')\n",
    "valid_labels = pd.read_csv('/kaggle/input/stanford-rna-3d-folding-2/validation_labels.csv')\n",
    "\n",
    "print(f\"Loaded {len(train_seqs)} training sequences, {len(valid_seqs)} validation sequences, and {len(test_seqs)} test sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55dc3f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:11:04.321055Z",
     "iopub.status.busy": "2026-01-09T18:11:04.320701Z",
     "iopub.status.idle": "2026-01-09T18:11:04.324667Z",
     "shell.execute_reply": "2026-01-09T18:11:04.323680Z"
    },
    "papermill": {
     "duration": 0.012604,
     "end_time": "2026-01-09T18:11:04.326499",
     "exception": false,
     "start_time": "2026-01-09T18:11:04.313895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_seqs_v2 = pd.read_csv('/kaggle/input/extended-rna/train_sequences_v2.csv')\n",
    "# train_labels_v2 = pd.read_csv('/kaggle/input/extended-rna/train_labels_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cee4e30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:11:04.339223Z",
     "iopub.status.busy": "2026-01-09T18:11:04.338848Z",
     "iopub.status.idle": "2026-01-09T18:11:16.035509Z",
     "shell.execute_reply": "2026-01-09T18:11:16.034229Z"
    },
    "papermill": {
     "duration": 11.705208,
     "end_time": "2026-01-09T18:11:16.037579",
     "exception": false,
     "start_time": "2026-01-09T18:11:04.332371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_seqs_v2 = pd.read_csv('/kaggle/input/rna-cif-to-csv/rna_sequences.csv')\n",
    "train_labels_v2 = pd.read_csv('/kaggle/input/rna-cif-to-csv/rna_coordinates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e7e3d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:11:16.052897Z",
     "iopub.status.busy": "2026-01-09T18:11:16.052539Z",
     "iopub.status.idle": "2026-01-09T18:11:16.079864Z",
     "shell.execute_reply": "2026-01-09T18:11:16.078719Z"
    },
    "papermill": {
     "duration": 0.037305,
     "end_time": "2026-01-09T18:11:16.081686",
     "exception": false,
     "start_time": "2026-01-09T18:11:16.044381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2D19_A</td>\n",
       "      <td>GCUGAAGUGCACACGGC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6OXI_QA</td>\n",
       "      <td>GUUGGAGAGUUUGAUCCUGGCUCAGGGUGAACGCUGGCGGCGUGCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6OXI_QV</td>\n",
       "      <td>CGCGGGGUGGAGCAGCCUGGUAGCUCGUCGGGCUCAUAACCCGAAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6OXI_QX</td>\n",
       "      <td>CAAGGAGGUAAAAAUGU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6OXI_RA</td>\n",
       "      <td>AGAUGGUAAGGGCCCACGGUGGAUGCCUCGGCACCCGAGCCGAUGA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target_id                                           sequence\n",
       "0    2D19_A                                  GCUGAAGUGCACACGGC\n",
       "1   6OXI_QA  GUUGGAGAGUUUGAUCCUGGCUCAGGGUGAACGCUGGCGGCGUGCC...\n",
       "2   6OXI_QV  CGCGGGGUGGAGCAGCCUGGUAGCUCGUCGGGCUCAUAACCCGAAG...\n",
       "3   6OXI_QX                                  CAAGGAGGUAAAAAUGU\n",
       "4   6OXI_RA  AGAUGGUAAGGGCCCACGGUGGAUGCCUCGGCACCCGAGCCGAUGA..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seqs_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b59eae20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:11:16.095758Z",
     "iopub.status.busy": "2026-01-09T18:11:16.095363Z",
     "iopub.status.idle": "2026-01-09T18:11:16.120897Z",
     "shell.execute_reply": "2026-01-09T18:11:16.119531Z"
    },
    "papermill": {
     "duration": 0.034726,
     "end_time": "2026-01-09T18:11:16.122890",
     "exception": false,
     "start_time": "2026-01-09T18:11:16.088164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10135546 entries, 0 to 10135545\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   ID       object \n",
      " 1   resname  object \n",
      " 2   resid    int64  \n",
      " 3   x_1      float64\n",
      " 4   y_1      float64\n",
      " 5   z_1      float64\n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 464.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_labels_v2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19bbbf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:11:16.137213Z",
     "iopub.status.busy": "2026-01-09T18:11:16.136835Z",
     "iopub.status.idle": "2026-01-09T18:11:16.143145Z",
     "shell.execute_reply": "2026-01-09T18:11:16.141968Z"
    },
    "papermill": {
     "duration": 0.015818,
     "end_time": "2026-01-09T18:11:16.145017",
     "exception": false,
     "start_time": "2026-01-09T18:11:16.129199",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # 1. Check if v2 is larger than original\n",
    "# print(\"Dataset Size Comparison:\")\n",
    "# print(f\"train_seqs: {len(train_seqs)} rows → train_seqs_v2: {len(train_seqs_v2)} rows\")\n",
    "# print(f\"train_labels: {len(train_labels)} rows → train_labels_v2: {len(train_labels_v2)} rows\")\n",
    "# print()\n",
    "\n",
    "# # 2. Verify all train_seqs records exist in train_seqs_v2\n",
    "# print(\"Checking if original sequences exist in v2...\")\n",
    "# original_target_ids = set(train_seqs['target_id'])\n",
    "# extended_target_ids = set(train_seqs_v2['target_id'])\n",
    "# all_targets_included = original_target_ids.issubset(extended_target_ids)\n",
    "\n",
    "# print(f\"Original train_seqs has {len(original_target_ids)} unique target_ids\")\n",
    "# print(f\"All original target_ids found in train_seqs_v2: {all_targets_included}\")\n",
    "\n",
    "# if not all_targets_included:\n",
    "#     missing_ids = original_target_ids - extended_target_ids\n",
    "#     print(f\"Missing {len(missing_ids)} target_ids\")\n",
    "#     if len(missing_ids) <= 5:\n",
    "#         print(f\"Missing IDs: {list(missing_ids)}\")\n",
    "#     else:\n",
    "#         print(f\"First 5 missing IDs: {list(missing_ids)[:5]}\")\n",
    "# print()\n",
    "\n",
    "# # 3. Check for consistency in a sample of target_ids that exist in both datasets\n",
    "# if all_targets_included:\n",
    "#     print(\"Checking consistency of data between original and v2 sequences...\")\n",
    "#     # Sample a few target_ids that exist in both datasets\n",
    "#     sample_size = min(5, len(original_target_ids))\n",
    "#     sample_ids = np.random.choice(list(original_target_ids), sample_size, replace=False)\n",
    "    \n",
    "#     for target_id in sample_ids:\n",
    "#         original_row = train_seqs[train_seqs['target_id'] == target_id].iloc[0]\n",
    "#         extended_row = train_seqs_v2[train_seqs_v2['target_id'] == target_id].iloc[0]\n",
    "        \n",
    "#         # Compare important columns\n",
    "#         sequence_match = original_row['sequence'] == extended_row['sequence']\n",
    "#         print(f\"Target ID {target_id}: Sequences match: {sequence_match}\")\n",
    "# print()\n",
    "\n",
    "# # 4. Check if all train_labels IDs exist in train_labels_v2\n",
    "# print(\"Checking labels dataset...\")\n",
    "# # Since labels dataset is large, we'll check a sample of IDs\n",
    "# sample_size = min(1000, len(train_labels))\n",
    "# sample_indices = np.random.choice(len(train_labels), sample_size, replace=False)\n",
    "# sample_rows = train_labels.iloc[sample_indices]\n",
    "\n",
    "# # Create a composite key for comparison (ID + resid)\n",
    "# sample_rows['composite_key'] = sample_rows['ID'] + '_' + sample_rows['resid'].astype(str)\n",
    "# train_labels_v2['composite_key'] = train_labels_v2['ID'] + '_' + train_labels_v2['resid'].astype(str)\n",
    "\n",
    "# sample_keys = set(sample_rows['composite_key'])\n",
    "# extended_keys = set(train_labels_v2['composite_key'])\n",
    "\n",
    "# keys_found = sample_keys.issubset(extended_keys)\n",
    "# if keys_found:\n",
    "#     found_percentage = 100\n",
    "# else:\n",
    "#     intersection = sample_keys.intersection(extended_keys)\n",
    "#     found_percentage = (len(intersection) / len(sample_keys)) * 100\n",
    "\n",
    "# print(f\"Sampled {len(sample_keys)} keys from train_labels\")\n",
    "# print(f\"All sampled keys found in train_labels_v2: {keys_found} ({found_percentage:.2f}%)\")\n",
    "\n",
    "# if not keys_found:\n",
    "#     missing_keys = sample_keys - extended_keys\n",
    "#     print(f\"Missing {len(missing_keys)} keys out of {len(sample_keys)} sampled\")\n",
    "#     if len(missing_keys) <= 5:\n",
    "#         print(f\"Missing keys: {list(missing_keys)}\")\n",
    "#     else:\n",
    "#         print(f\"First 5 missing keys: {list(missing_keys)[:5]}\")\n",
    "# print()\n",
    "\n",
    "# # 5. Data type consistency check\n",
    "# print(\"Data type consistency check:\")\n",
    "# print(\"train_seqs vs train_seqs_v2:\")\n",
    "# for col in train_seqs.columns:\n",
    "#     print(f\"  Column '{col}': {train_seqs[col].dtype} → {train_seqs_v2[col].dtype} - Match: {train_seqs[col].dtype == train_seqs_v2[col].dtype}\")\n",
    "\n",
    "# print(\"\\ntrain_labels vs train_labels_v2:\")\n",
    "# for col in train_labels.columns:\n",
    "#     if col != 'composite_key':  # Skip the key we created\n",
    "#         print(f\"  Column '{col}': {train_labels[col].dtype} → {train_labels_v2[col].dtype} - Match: {train_labels[col].dtype == train_labels_v2[col].dtype}\")\n",
    "# print()\n",
    "\n",
    "# # 6. Check for missing values pattern\n",
    "# print(\"Missing values comparison:\")\n",
    "# print(\"train_seqs vs train_seqs_v2:\")\n",
    "# for col in train_seqs.columns:\n",
    "#     original_missing = train_seqs[col].isnull().sum() / len(train_seqs) * 100\n",
    "#     extended_missing = train_seqs_v2[col].isnull().sum() / len(train_seqs_v2) * 100\n",
    "#     print(f\"  Column '{col}': {original_missing:.2f}% → {extended_missing:.2f}%\")\n",
    "\n",
    "# print(\"\\ntrain_labels vs train_labels_v2:\")\n",
    "# for col in train_labels.columns:\n",
    "#     if col != 'composite_key':  # Skip the key we created\n",
    "#         original_missing = train_labels[col].isnull().sum() / len(train_labels) * 100\n",
    "#         extended_missing = train_labels_v2[col].isnull().sum() / len(train_labels_v2) * 100\n",
    "#         print(f\"  Column '{col}': {original_missing:.2f}% → {extended_missing:.2f}%\")\n",
    "# print()\n",
    "\n",
    "# # Clean up the temporary column we added\n",
    "# if 'composite_key' in train_labels_v2.columns:\n",
    "#     train_labels_v2.drop('composite_key', axis=1, inplace=True)\n",
    "\n",
    "# # Final assessment\n",
    "# print(\"FINAL ASSESSMENT:\")\n",
    "# print(\"-\" * 50)\n",
    "# seqs_extended_properly = all_targets_included and len(train_seqs_v2) > len(train_seqs)\n",
    "# labels_extended_properly = found_percentage > 99 and len(train_labels_v2) > len(train_labels)\n",
    "\n",
    "# if seqs_extended_properly and labels_extended_properly:\n",
    "#     print(\"✓ PASS: Both train_seqs_v2 and train_labels_v2 appear to be proper extensions of the original datasets.\")\n",
    "#     print(\"✓ It should be safe to swap them.\")\n",
    "# else:\n",
    "#     print(\"✗ ISSUES DETECTED:\")\n",
    "#     if not seqs_extended_properly:\n",
    "#         print(\"  - train_seqs_v2 may not fully contain train_seqs data\")\n",
    "#     if not labels_extended_properly:\n",
    "#         print(\"  - train_labels_v2 may not fully contain train_labels data\")\n",
    "#     print(\"✗ Recommend investigating the issues above before swapping datasets.\")\n",
    "# print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e5a3267",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:11:16.160161Z",
     "iopub.status.busy": "2026-01-09T18:11:16.159776Z",
     "iopub.status.idle": "2026-01-09T18:14:10.473573Z",
     "shell.execute_reply": "2026-01-09T18:14:10.471772Z"
    },
    "papermill": {
     "duration": 174.324055,
     "end_time": "2026-01-09T18:14:10.475667",
     "exception": false,
     "start_time": "2026-01-09T18:11:16.151612",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXTENDING SEQUENCE DATASETS\n",
      "==================================================\n",
      "Extending train_seqs...\n",
      "  Original size: 5716 rows\n",
      "  v2 size: 18881 rows\n",
      "  Keys only in original: 5716\n",
      "  Keys only in v2: 18881\n",
      "  Common keys: 0\n",
      "  New records added: 18881\n",
      "  Extended dataset size: 24597 rows\n",
      "  Verification - All original keys in extended dataset: True\n",
      "  Column 'temporal_cutoff': Missing values - Original: 0, Extended: 18881\n",
      "  Column 'description': Missing values - Original: 0, Extended: 18881\n",
      "  Column 'stoichiometry': Missing values - Original: 0, Extended: 18881\n",
      "  Column 'all_sequences': Missing values - Original: 0, Extended: 18881\n",
      "  Column 'ligand_ids': Missing values - Original: 1868, Extended: 20749\n",
      "  Column 'ligand_SMILES': Missing values - Original: 1855, Extended: 20736\n",
      "\n",
      "==================================================\n",
      "EXTENDING LABELS DATASETS\n",
      "==================================================\n",
      "Extending train_labels...\n",
      "  Original size: 7794971 rows\n",
      "  v2 size: 10135546 rows\n",
      "  Keys only in original: 7794971\n",
      "  Keys only in v2: 10135546\n",
      "  Common keys: 0\n",
      "  New records added: 10135546\n",
      "  Extended dataset size: 17930517 rows\n",
      "  Verification - All original keys in extended dataset: True\n",
      "  Column 'x_1': Missing values - Original: 486412, Extended: 486412\n",
      "  Column 'y_1': Missing values - Original: 486412, Extended: 486412\n",
      "  Column 'z_1': Missing values - Original: 486412, Extended: 486412\n",
      "  Column 'chain': Missing values - Original: 0, Extended: 10135546\n",
      "  Column 'copy': Missing values - Original: 0, Extended: 10135546\n",
      "\n",
      "==================================================\n",
      "VERIFYING RELATIONSHIPS\n",
      "==================================================\n",
      "Total unique sequence IDs: 24597\n",
      "Sequence IDs with corresponding labels: 1870 (7.60%)\n",
      "Sequence IDs without corresponding labels: 22727 (92.40%)\n",
      "Sample of sequence IDs without labels (up to 5):\n",
      "['8SWO_A', '2VPL_B', '7TDA', '2C51', '7RDX_T']\n",
      "\n",
      "==================================================\n",
      "SUMMARY OF EXTENDED DATASETS\n",
      "==================================================\n",
      "Original train_seqs: 5716 rows\n",
      "Original train_labels: 7794971 rows\n",
      "Extended train_seqs: 24597 rows (+18881)\n",
      "Extended train_labels: 17930517 rows (+10135546)\n",
      "\n",
      "==================================================\n",
      "DONE! Extended datasets created.\n",
      "To save the datasets, uncomment the last two lines.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to extend the original dataset with new records from v2\n",
    "def extend_dataset(original_df, v2_df, key_columns, dataset_name):\n",
    "    print(f\"Extending {dataset_name}...\")\n",
    "    print(f\"  Original size: {len(original_df)} rows\")\n",
    "    print(f\"  v2 size: {len(v2_df)} rows\")\n",
    "    \n",
    "    # Create a composite key for identification if multiple key columns\n",
    "    if isinstance(key_columns, list) and len(key_columns) > 1:\n",
    "        original_df['temp_key'] = original_df[key_columns].astype(str).agg('_'.join, axis=1)\n",
    "        v2_df['temp_key'] = v2_df[key_columns].astype(str).agg('_'.join, axis=1)\n",
    "        key_for_identification = 'temp_key'\n",
    "    else:\n",
    "        key_for_identification = key_columns[0] if isinstance(key_columns, list) else key_columns\n",
    "    \n",
    "    # Identify unique records in each dataset\n",
    "    original_keys = set(original_df[key_for_identification])\n",
    "    v2_keys = set(v2_df[key_for_identification])\n",
    "    \n",
    "    # Calculate stats\n",
    "    keys_only_in_original = original_keys - v2_keys\n",
    "    keys_only_in_v2 = v2_keys - original_keys \n",
    "    common_keys = original_keys.intersection(v2_keys)\n",
    "    \n",
    "    print(f\"  Keys only in original: {len(keys_only_in_original)}\")\n",
    "    print(f\"  Keys only in v2: {len(keys_only_in_v2)}\")\n",
    "    print(f\"  Common keys: {len(common_keys)}\")\n",
    "    \n",
    "    # Create a mask to filter v2 records that don't exist in original\n",
    "    new_records_mask = ~v2_df[key_for_identification].isin(original_keys)\n",
    "    new_records = v2_df[new_records_mask].copy()\n",
    "    \n",
    "    # Drop temporary key if it was created\n",
    "    if key_for_identification == 'temp_key':\n",
    "        new_records.drop('temp_key', axis=1, inplace=True)\n",
    "        original_df.drop('temp_key', axis=1, inplace=True)\n",
    "    \n",
    "    # Combine original with new records from v2\n",
    "    extended_df = pd.concat([original_df, new_records], ignore_index=True)\n",
    "    \n",
    "    # Report final sizes\n",
    "    print(f\"  New records added: {len(new_records)}\")\n",
    "    print(f\"  Extended dataset size: {len(extended_df)} rows\")\n",
    "    print(f\"  Verification - All original keys in extended dataset: {set(original_df[key_columns[0] if isinstance(key_columns, list) else key_columns]).issubset(set(extended_df[key_columns[0] if isinstance(key_columns, list) else key_columns]))}\")\n",
    "    \n",
    "    # Check for missing values in key columns\n",
    "    for col in extended_df.columns:\n",
    "        original_missing = original_df[col].isnull().sum()\n",
    "        extended_missing = extended_df[col].isnull().sum()\n",
    "        if original_missing > 0 or extended_missing > 0:\n",
    "            print(f\"  Column '{col}': Missing values - Original: {original_missing}, Extended: {extended_missing}\")\n",
    "    \n",
    "    # Clean up\n",
    "    if key_for_identification == 'temp_key' and 'temp_key' in v2_df.columns:\n",
    "        v2_df.drop('temp_key', axis=1, inplace=True)\n",
    "        \n",
    "    return extended_df\n",
    "\n",
    "# 1. Extend train_seqs with train_seqs_v2\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXTENDING SEQUENCE DATASETS\")\n",
    "print(\"=\"*50)\n",
    "train_seqs_extended = extend_dataset(\n",
    "    train_seqs, \n",
    "    train_seqs_v2,\n",
    "    ['target_id'],  # Using target_id as the unique identifier\n",
    "    \"train_seqs\"\n",
    ")\n",
    "\n",
    "# 2. Extend train_labels with train_labels_v2\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXTENDING LABELS DATASETS\")\n",
    "print(\"=\"*50)\n",
    "# For labels, we need a composite key of ID and resid\n",
    "train_labels_extended = extend_dataset(\n",
    "    train_labels,\n",
    "    train_labels_v2,\n",
    "    ['ID', 'resid'],  # Using composite key\n",
    "    \"train_labels\"\n",
    ")\n",
    "\n",
    "# Verify relationships between extended datasets\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VERIFYING RELATIONSHIPS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check if all sequence IDs have corresponding labels\n",
    "seq_ids = set(train_seqs_extended['target_id'].unique())\n",
    "label_ids = set(train_labels_extended['ID'].unique())\n",
    "\n",
    "seq_ids_with_labels = seq_ids.intersection(label_ids)\n",
    "seq_ids_without_labels = seq_ids - label_ids\n",
    "\n",
    "print(f\"Total unique sequence IDs: {len(seq_ids)}\")\n",
    "print(f\"Sequence IDs with corresponding labels: {len(seq_ids_with_labels)} ({len(seq_ids_with_labels)/len(seq_ids)*100:.2f}%)\")\n",
    "print(f\"Sequence IDs without corresponding labels: {len(seq_ids_without_labels)} ({len(seq_ids_without_labels)/len(seq_ids)*100:.2f}%)\")\n",
    "\n",
    "if len(seq_ids_without_labels) > 0:\n",
    "    print(\"Sample of sequence IDs without labels (up to 5):\")\n",
    "    print(list(seq_ids_without_labels)[:5])\n",
    "\n",
    "# Print summary of extended datasets\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY OF EXTENDED DATASETS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original train_seqs: {len(train_seqs)} rows\")\n",
    "print(f\"Original train_labels: {len(train_labels)} rows\")\n",
    "print(f\"Extended train_seqs: {len(train_seqs_extended)} rows (+{len(train_seqs_extended)-len(train_seqs)})\")\n",
    "print(f\"Extended train_labels: {len(train_labels_extended)} rows (+{len(train_labels_extended)-len(train_labels)})\")\n",
    "\n",
    "# Save the extended datasets (uncomment to save)\n",
    "# train_seqs_extended.to_csv('train_seqs_combined.csv', index=False)\n",
    "# train_labels_extended.to_csv('train_labels_combined.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DONE! Extended datasets created.\")\n",
    "print(\"To save the datasets, uncomment the last two lines.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aff7bddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:14:10.491336Z",
     "iopub.status.busy": "2026-01-09T18:14:10.490842Z",
     "iopub.status.idle": "2026-01-09T18:14:10.511638Z",
     "shell.execute_reply": "2026-01-09T18:14:10.510069Z"
    },
    "papermill": {
     "duration": 0.03064,
     "end_time": "2026-01-09T18:14:10.513530",
     "exception": false,
     "start_time": "2026-01-09T18:14:10.482890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24597 entries, 0 to 24596\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   target_id        24597 non-null  object\n",
      " 1   sequence         24597 non-null  object\n",
      " 2   temporal_cutoff  5716 non-null   object\n",
      " 3   description      5716 non-null   object\n",
      " 4   stoichiometry    5716 non-null   object\n",
      " 5   all_sequences    5716 non-null   object\n",
      " 6   ligand_ids       3848 non-null   object\n",
      " 7   ligand_SMILES    3861 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_seqs_extended.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0813634",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:14:10.528920Z",
     "iopub.status.busy": "2026-01-09T18:14:10.528543Z",
     "iopub.status.idle": "2026-01-09T18:14:10.538413Z",
     "shell.execute_reply": "2026-01-09T18:14:10.537080Z"
    },
    "papermill": {
     "duration": 0.019337,
     "end_time": "2026-01-09T18:14:10.540115",
     "exception": false,
     "start_time": "2026-01-09T18:14:10.520778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17930517 entries, 0 to 17930516\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   ID       object \n",
      " 1   resname  object \n",
      " 2   resid    int64  \n",
      " 3   x_1      float64\n",
      " 4   y_1      float64\n",
      " 5   z_1      float64\n",
      " 6   chain    object \n",
      " 7   copy     float64\n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 1.1+ GB\n"
     ]
    }
   ],
   "source": [
    "train_labels_extended.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a901320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:14:10.555863Z",
     "iopub.status.busy": "2026-01-09T18:14:10.555478Z",
     "iopub.status.idle": "2026-01-09T18:14:10.559857Z",
     "shell.execute_reply": "2026-01-09T18:14:10.558662Z"
    },
    "papermill": {
     "duration": 0.01442,
     "end_time": "2026-01-09T18:14:10.561806",
     "exception": false,
     "start_time": "2026-01-09T18:14:10.547386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get the first 500 sequences\n",
    "# train_seqs_small = train_seqs_extended.iloc[:500].copy()\n",
    "\n",
    "# # Extract base IDs from train_labels_extended once\n",
    "# base_ids = train_labels_extended['ID'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# # Filter labels where the base ID is in our sequence IDs\n",
    "# train_labels_small = train_labels_extended[base_ids.isin(train_seqs_small['target_id'])].copy()\n",
    "\n",
    "# # Verify\n",
    "# print(f\"Number of sequences in train_seqs_small: {len(train_seqs_small)}\")\n",
    "# print(f\"Total rows in train_labels_small: {len(train_labels_small)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e348693",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:14:10.577635Z",
     "iopub.status.busy": "2026-01-09T18:14:10.577279Z",
     "iopub.status.idle": "2026-01-09T18:37:00.659448Z",
     "shell.execute_reply": "2026-01-09T18:37:00.658318Z"
    },
    "papermill": {
     "duration": 1370.092032,
     "end_time": "2026-01-09T18:37:00.661302",
     "exception": false,
     "start_time": "2026-01-09T18:14:10.569270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing structures: 100%|██████████| 24422/24422 [20:35<00:00, 19.77it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_labels(labels_df):\n",
    "    coords_dict = {}\n",
    "    \n",
    "    # Group by target ID and wrap with tqdm for progress tracking\n",
    "    id_groups = labels_df.groupby(lambda x: labels_df['ID'][x].rsplit('_', 1)[0])\n",
    "    for id_prefix, group in tqdm(id_groups, desc=\"Processing structures\"):\n",
    "        # Extract just the coordinates columns for the first structure (x_1, y_1, z_1)\n",
    "        coords = []\n",
    "        for _, row in group.sort_values('resid').iterrows():\n",
    "            coords.append([row['x_1'], row['y_1'], row['z_1']])\n",
    "        \n",
    "        coords_dict[id_prefix] = np.array(coords)\n",
    "    \n",
    "    return coords_dict\n",
    "\n",
    "train_coords_dict = process_labels(train_labels_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde905a4",
   "metadata": {
    "papermill": {
     "duration": 0.289147,
     "end_time": "2026-01-09T18:37:01.234452",
     "exception": false,
     "start_time": "2026-01-09T18:37:00.945305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f25fa6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:37:01.862335Z",
     "iopub.status.busy": "2026-01-09T18:37:01.861929Z",
     "iopub.status.idle": "2026-01-09T18:37:02.439670Z",
     "shell.execute_reply": "2026-01-09T18:37:02.438342Z"
    },
    "papermill": {
     "duration": 0.868575,
     "end_time": "2026-01-09T18:37:02.441599",
     "exception": false,
     "start_time": "2026-01-09T18:37:01.573024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio import pairwise2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_sequences(query_seq, train_seqs_df, train_coords_dict, top_n=5):\n",
    "    \"\"\"\n",
    "    Find similar RNA sequences using enhanced scoring and clustering for diversity.\n",
    "    \n",
    "    Improvements:\n",
    "    - Multi-tier length filtering\n",
    "    - Enhanced alignment scoring with multiple algorithms\n",
    "    - RNA-specific structural features\n",
    "    - Adaptive clustering\n",
    "    \"\"\"\n",
    "    similar_seqs = []\n",
    "    query_seq_obj = Seq(query_seq)\n",
    "    query_features = _extract_enhanced_rna_features(query_seq)\n",
    "    \n",
    "    # Step 1: Enhanced candidate selection with multi-tier filtering\n",
    "    for _, row in train_seqs_df.iterrows():\n",
    "        target_id = row['target_id']\n",
    "        train_seq = row['sequence']\n",
    "        \n",
    "        # Skip if coordinates not available\n",
    "        if target_id not in train_coords_dict:\n",
    "            continue\n",
    "        \n",
    "        # Multi-tier length filtering (more permissive for very short/long sequences)\n",
    "        len_ratio = abs(len(train_seq) - len(query_seq)) / max(len(train_seq), len(query_seq))\n",
    "        if len(query_seq) < 50 or len(train_seq) < 50:  # Short sequences - more permissive\n",
    "            if len_ratio > 0.6:\n",
    "                continue\n",
    "        elif len(query_seq) > 1000 or len(train_seq) > 1000:  # Long sequences - stricter\n",
    "            if len_ratio > 0.2:\n",
    "                continue\n",
    "        else:  # Medium sequences - original threshold\n",
    "            if len_ratio > 0.4:\n",
    "                continue\n",
    "        \n",
    "        # Calculate composite similarity score\n",
    "        composite_score = _calculate_composite_similarity(query_seq, train_seq, query_features)\n",
    "        \n",
    "        if composite_score > 0:  # Only keep sequences with positive similarity\n",
    "            similar_seqs.append((target_id, train_seq, composite_score, train_coords_dict[target_id]))\n",
    "    \n",
    "    # Sort by composite score and take top candidates\n",
    "    similar_seqs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Adaptive candidate selection based on score distribution\n",
    "    candidate_count = min(50, len(similar_seqs))  # Increased initial pool\n",
    "    if len(similar_seqs) > 10:\n",
    "        # Filter out sequences with very low scores (bottom 20%)\n",
    "        score_threshold = np.percentile([x[2] for x in similar_seqs], 80)\n",
    "        filtered_candidates = [x for x in similar_seqs if x[2] >= score_threshold]\n",
    "        candidate_count = min(candidate_count, len(filtered_candidates))\n",
    "        top_candidates = filtered_candidates[:candidate_count]\n",
    "    else:\n",
    "        top_candidates = similar_seqs[:candidate_count]\n",
    "    \n",
    "    # If we have fewer sequences than requested clusters, return all\n",
    "    if len(top_candidates) <= top_n:\n",
    "        return top_candidates[:top_n]\n",
    "    \n",
    "    # Step 2: Enhanced feature matrix for better clustering\n",
    "    feature_matrix = []\n",
    "    for _, seq, _, _ in top_candidates:\n",
    "        features = _extract_enhanced_rna_features(seq)\n",
    "        feature_matrix.append(features)\n",
    "    \n",
    "    feature_matrix = np.array(feature_matrix)\n",
    "    \n",
    "    # Step 3: Adaptive clustering\n",
    "    n_clusters = min(top_n, len(top_candidates))\n",
    "    \n",
    "    # Use different clustering approach based on dataset size\n",
    "    if len(top_candidates) >= 15:\n",
    "        # K-means for larger datasets\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "    else:\n",
    "        # Simple diversity-based selection for smaller datasets\n",
    "        cluster_labels = _diversity_based_clustering(feature_matrix, n_clusters)\n",
    "    \n",
    "    # Step 4: Select best representative from each cluster\n",
    "    final_results = []\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_sequences = [top_candidates[i] for i in range(len(top_candidates)) \n",
    "                           if cluster_labels[i] == cluster_id]\n",
    "        \n",
    "        if cluster_sequences:\n",
    "            # Sort by composite score and take the best one\n",
    "            cluster_sequences.sort(key=lambda x: x[2], reverse=True)\n",
    "            final_results.append(cluster_sequences[0])\n",
    "    \n",
    "    # Sort final results by similarity score\n",
    "    final_results.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return final_results[:top_n]\n",
    "\n",
    "def _calculate_composite_similarity(query_seq, train_seq, query_features):\n",
    "    \"\"\"\n",
    "    Calculate composite similarity using multiple alignment methods and features.\n",
    "    \"\"\"\n",
    "    query_seq_obj = Seq(query_seq)\n",
    "    \n",
    "    # 1. Global alignment (original method)\n",
    "    global_alignments = pairwise2.align.globalms(query_seq_obj, train_seq, 2.9, -1, -10, -0.5, one_alignment_only=True)\n",
    "    global_score = 0\n",
    "    if global_alignments:\n",
    "        alignment = global_alignments[0]\n",
    "        global_score = alignment.score / (2 * min(len(query_seq), len(train_seq)))\n",
    "    \n",
    "    # 2. Local alignment for finding similar regions\n",
    "    local_alignments = pairwise2.align.localms(query_seq_obj, train_seq, 2.9, -1, -10, -0.5, one_alignment_only=True)\n",
    "    local_score = 0\n",
    "    if local_alignments:\n",
    "        alignment = local_alignments[0]\n",
    "        local_score = alignment.score / (2 * min(len(query_seq), len(train_seq)))\n",
    "    \n",
    "    # 3. Feature-based similarity\n",
    "    train_features = _extract_enhanced_rna_features(train_seq)\n",
    "    feature_similarity = cosine_similarity([query_features], [train_features])[0][0]\n",
    "    \n",
    "    # 4. K-mer similarity for sequence motifs\n",
    "    kmer_similarity = _calculate_kmer_similarity(query_seq, train_seq, k=3)\n",
    "    \n",
    "    # Weighted composite score\n",
    "    composite_score = (\n",
    "        0.4 * global_score + \n",
    "        0.3 * local_score + \n",
    "        0.2 * feature_similarity + \n",
    "        0.1 * kmer_similarity\n",
    "    )\n",
    "    \n",
    "    return composite_score\n",
    "\n",
    "def _calculate_kmer_similarity(seq1, seq2, k=3):\n",
    "    \"\"\"Calculate k-mer based similarity between sequences.\"\"\"\n",
    "    def get_kmers(seq, k):\n",
    "        return set(seq[i:i+k] for i in range(len(seq) - k + 1))\n",
    "    \n",
    "    kmers1 = get_kmers(seq1.upper(), k)\n",
    "    kmers2 = get_kmers(seq2.upper(), k)\n",
    "    \n",
    "    if not kmers1 or not kmers2:\n",
    "        return 0\n",
    "    \n",
    "    intersection = len(kmers1.intersection(kmers2))\n",
    "    union = len(kmers1.union(kmers2))\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def _diversity_based_clustering(feature_matrix, n_clusters):\n",
    "    \"\"\"Simple diversity-based clustering for small datasets.\"\"\"\n",
    "    n_samples = len(feature_matrix)\n",
    "    cluster_labels = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    if n_samples <= n_clusters:\n",
    "        return np.arange(n_samples)\n",
    "    \n",
    "    # Select diverse representatives\n",
    "    selected_indices = [0]  # Start with first sequence\n",
    "    \n",
    "    for cluster_id in range(1, n_clusters):\n",
    "        max_min_distance = -1\n",
    "        best_idx = -1\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            if i in selected_indices:\n",
    "                continue\n",
    "            \n",
    "            # Find minimum distance to already selected sequences\n",
    "            min_distance = min(\n",
    "                np.linalg.norm(feature_matrix[i] - feature_matrix[j]) \n",
    "                for j in selected_indices\n",
    "            )\n",
    "            \n",
    "            if min_distance > max_min_distance:\n",
    "                max_min_distance = min_distance\n",
    "                best_idx = i\n",
    "        \n",
    "        if best_idx != -1:\n",
    "            selected_indices.append(best_idx)\n",
    "    \n",
    "    # Assign remaining sequences to closest cluster centers\n",
    "    for i in range(n_samples):\n",
    "        if i not in selected_indices:\n",
    "            distances = [\n",
    "                np.linalg.norm(feature_matrix[i] - feature_matrix[j]) \n",
    "                for j in selected_indices\n",
    "            ]\n",
    "            cluster_labels[i] = np.argmin(distances)\n",
    "        else:\n",
    "            cluster_labels[i] = selected_indices.index(i)\n",
    "    \n",
    "    return cluster_labels\n",
    "\n",
    "def _extract_enhanced_rna_features(sequence):\n",
    "    \"\"\"\n",
    "    Extract comprehensive RNA-specific features for better clustering and similarity.\n",
    "    \"\"\"\n",
    "    seq = sequence.upper()\n",
    "    features = []\n",
    "    \n",
    "    # 1. Basic nucleotide frequencies\n",
    "    nucleotides = ['A', 'U', 'G', 'C']\n",
    "    for nuc in nucleotides:\n",
    "        freq = seq.count(nuc) / len(seq) if len(seq) > 0 else 0\n",
    "        features.append(freq)\n",
    "    \n",
    "    # 2. Dinucleotide frequencies (reduced set - most important for RNA)\n",
    "    important_dinucs = ['AU', 'UA', 'GC', 'CG', 'GU', 'UG', 'AA', 'UU', 'GG', 'CC']\n",
    "    for dinuc in important_dinucs:\n",
    "        count = 0\n",
    "        for i in range(len(seq) - 1):\n",
    "            if seq[i:i+2] == dinuc:\n",
    "                count += 1\n",
    "        freq = count / (len(seq) - 1) if len(seq) > 1 else 0\n",
    "        features.append(freq)\n",
    "    \n",
    "    # 3. RNA secondary structure indicators\n",
    "    gc_content = (seq.count('G') + seq.count('C')) / len(seq) if len(seq) > 0 else 0\n",
    "    au_content = (seq.count('A') + seq.count('U')) / len(seq) if len(seq) > 0 else 0\n",
    "    purine_content = (seq.count('A') + seq.count('G')) / len(seq) if len(seq) > 0 else 0\n",
    "    pyrimidine_content = (seq.count('U') + seq.count('C')) / len(seq) if len(seq) > 0 else 0\n",
    "    \n",
    "    features.extend([gc_content, au_content, purine_content, pyrimidine_content])\n",
    "    \n",
    "    # 4. Sequence complexity measures\n",
    "    length_normalized = min(len(seq) / 1000.0, 1.0)  # Capped normalization\n",
    "    \n",
    "    # Simple entropy calculation\n",
    "    entropy = 0\n",
    "    for nuc in nucleotides:\n",
    "        freq = seq.count(nuc) / len(seq) if len(seq) > 0 else 0\n",
    "        if freq > 0:\n",
    "            entropy -= freq * np.log2(freq)\n",
    "    entropy_normalized = entropy / 2.0  # Max entropy for 4 nucleotides is 2\n",
    "    \n",
    "    features.extend([length_normalized, entropy_normalized])\n",
    "    \n",
    "    # 5. Repetitive pattern detection\n",
    "    repeat_content = _calculate_repeat_content(seq)\n",
    "    features.append(repeat_content)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def _calculate_repeat_content(sequence):\n",
    "    \"\"\"Calculate the proportion of repetitive content in the sequence.\"\"\"\n",
    "    if len(sequence) < 6:\n",
    "        return 0\n",
    "    \n",
    "    repeat_count = 0\n",
    "    window_size = 3\n",
    "    \n",
    "    for i in range(len(sequence) - window_size + 1):\n",
    "        motif = sequence[i:i + window_size]\n",
    "        # Look for the same motif in the rest of the sequence\n",
    "        for j in range(i + window_size, len(sequence) - window_size + 1):\n",
    "            if sequence[j:j + window_size] == motif:\n",
    "                repeat_count += 1\n",
    "                break\n",
    "    \n",
    "    return repeat_count / (len(sequence) - window_size + 1) if len(sequence) > window_size else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b45afbb1",
   "metadata": {
    "_cell_guid": "aaf3f747-0607-4470-a425-dbe714562373",
    "_uuid": "0d532205-a881-4262-86ec-36588b58b8f8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-09T18:37:02.996226Z",
     "iopub.status.busy": "2026-01-09T18:37:02.995807Z",
     "iopub.status.idle": "2026-01-09T18:37:03.009476Z",
     "shell.execute_reply": "2026-01-09T18:37:03.008304Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.292773,
     "end_time": "2026-01-09T18:37:03.011376",
     "exception": false,
     "start_time": "2026-01-09T18:37:02.718603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adaptive_rna_constraints(coordinates, sequence, confidence=1.0):\n",
    "    # Make a copy of coordinates to refine\n",
    "    refined_coords = coordinates.copy()\n",
    "    n_residues = len(sequence)\n",
    "    \n",
    "    # Calculate constraint strength (inverse of confidence)\n",
    "    # High confidence templates receive gentler constraints\n",
    "    constraint_strength = 0.8 * (1.0 - min(confidence, 0.8))\n",
    "    \n",
    "    # 1. Sequential distance constraints (consecutive nucleotides)\n",
    "    # More flexible distance range (statistical distribution from PDB)\n",
    "    seq_min_dist = 5.5  # Minimum sequential distance\n",
    "    seq_max_dist = 6.5  # Maximum sequential distance\n",
    "    \n",
    "    for i in range(n_residues - 1):\n",
    "        current_pos = refined_coords[i]\n",
    "        next_pos = refined_coords[i+1]\n",
    "        \n",
    "        # Calculate current distance\n",
    "        current_dist = np.linalg.norm(next_pos - current_pos)\n",
    "        \n",
    "        # Only adjust if significantly outside expected range\n",
    "        if current_dist < seq_min_dist or current_dist > seq_max_dist:\n",
    "            # Calculate target distance (midpoint of range)\n",
    "            target_dist = (seq_min_dist + seq_max_dist) / 2\n",
    "            \n",
    "            # Get direction vector\n",
    "            direction = next_pos - current_pos\n",
    "            direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "            \n",
    "            # Apply partial adjustment based on constraint strength\n",
    "            adjustment = (target_dist - current_dist) * constraint_strength\n",
    "            \n",
    "            # Only adjust the next position to preserve the overall fold\n",
    "            refined_coords[i+1] = current_pos + direction * (current_dist + adjustment)\n",
    "    \n",
    "    # 2. Steric clash prevention (more conservative)\n",
    "    min_allowed_distance = 3.8  # Minimum distance between non-consecutive C1' atoms\n",
    "    \n",
    "    # Calculate all pairwise distances\n",
    "    dist_matrix = distance_matrix(refined_coords, refined_coords)\n",
    "    \n",
    "    # Find severe clashes (atoms too close)\n",
    "    severe_clashes = np.where((dist_matrix < min_allowed_distance) & (dist_matrix > 0))\n",
    "    \n",
    "    # Fix severe clashes\n",
    "    for idx in range(len(severe_clashes[0])):\n",
    "        i, j = severe_clashes[0][idx], severe_clashes[1][idx]\n",
    "        \n",
    "        # Skip consecutive nucleotides and previously processed pairs\n",
    "        if abs(i - j) <= 1 or i >= j:\n",
    "            continue\n",
    "            \n",
    "        # Get current positions and distance\n",
    "        pos_i = refined_coords[i]\n",
    "        pos_j = refined_coords[j]\n",
    "        current_dist = dist_matrix[i, j]\n",
    "        \n",
    "        # Calculate necessary adjustment but scale by constraint strength\n",
    "        direction = pos_j - pos_i\n",
    "        direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "        \n",
    "        # Calculate partial adjustment\n",
    "        adjustment = (min_allowed_distance - current_dist) * constraint_strength\n",
    "        \n",
    "        # Move points apart\n",
    "        refined_coords[i] = pos_i - direction * (adjustment / 2)\n",
    "        refined_coords[j] = pos_j + direction * (adjustment / 2)\n",
    "    \n",
    "    # 3. Very light base-pair constraining (if confidence is low)\n",
    "    if constraint_strength > 0.3:  # Only apply if template confidence is low\n",
    "        # Simple Watson-Crick base pairs\n",
    "        pairs = {'A': 'U', 'U': 'A', 'G': 'C', 'C': 'G'}\n",
    "        \n",
    "        # Scan for potential base pairs\n",
    "        for i in range(n_residues):\n",
    "            base_i = sequence[i]\n",
    "            complement = pairs.get(base_i)\n",
    "            \n",
    "            if not complement:\n",
    "                continue\n",
    "                \n",
    "            # Look for complementary bases within a reasonable range\n",
    "            for j in range(i + 3, min(i + 20, n_residues)):\n",
    "                if sequence[j] == complement:\n",
    "                    # Calculate current distance\n",
    "                    current_dist = np.linalg.norm(refined_coords[i] - refined_coords[j])\n",
    "                    \n",
    "                    # Only consider if distance suggests potential pairing\n",
    "                    if 8.0 < current_dist < 14.0:\n",
    "                        # Target 10.5Å as generic base-pair C1'-C1' distance\n",
    "                        target_dist = 10.5\n",
    "                        \n",
    "                        # Calculate very gentle adjustment (scaled by constraint_strength)\n",
    "                        adjustment = (target_dist - current_dist) * (constraint_strength * 0.3)\n",
    "                        \n",
    "                        # Get direction vector\n",
    "                        direction = refined_coords[j] - refined_coords[i]\n",
    "                        direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "                        \n",
    "                        # Apply very gentle adjustment to both positions\n",
    "                        refined_coords[i] = refined_coords[i] - direction * (adjustment / 2)\n",
    "                        refined_coords[j] = refined_coords[j] + direction * (adjustment / 2)\n",
    "                        \n",
    "                        # Only consider one potential pair per base (closest match)\n",
    "                        break\n",
    "    \n",
    "    return refined_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24351702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:37:03.578766Z",
     "iopub.status.busy": "2026-01-09T18:37:03.578374Z",
     "iopub.status.idle": "2026-01-09T18:37:03.595000Z",
     "shell.execute_reply": "2026-01-09T18:37:03.593939Z"
    },
    "papermill": {
     "duration": 0.302756,
     "end_time": "2026-01-09T18:37:03.596718",
     "exception": false,
     "start_time": "2026-01-09T18:37:03.293962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adapt_template_to_query(query_seq, template_seq, template_coords, alignment=None):\n",
    "    if alignment is None:\n",
    "        from Bio.Seq import Seq\n",
    "        from Bio import pairwise2\n",
    "        \n",
    "        query_seq_obj = Seq(query_seq)\n",
    "        template_seq_obj = Seq(template_seq)\n",
    "        alignments = pairwise2.align.globalms(query_seq_obj, template_seq_obj, 2.9, -1, -10, -0.5, one_alignment_only=True)\n",
    "        \n",
    "        if not alignments:\n",
    "            return generate_improved_rna_structure(query_seq)\n",
    "            \n",
    "        alignment = alignments[0]\n",
    "    \n",
    "    aligned_query = alignment.seqA\n",
    "    aligned_template = alignment.seqB\n",
    "    \n",
    "    query_coords = np.zeros((len(query_seq), 3))\n",
    "    query_coords.fill(np.nan)\n",
    "    \n",
    "    # Map template coordinates to query\n",
    "    query_idx = 0\n",
    "    template_idx = 0\n",
    "    \n",
    "    for i in range(len(aligned_query)):\n",
    "        query_char = aligned_query[i]\n",
    "        template_char = aligned_template[i]\n",
    "        \n",
    "        if query_char != '-' and template_char != '-':\n",
    "            if template_idx < len(template_coords):\n",
    "                query_coords[query_idx] = template_coords[template_idx]\n",
    "            template_idx += 1\n",
    "            query_idx += 1\n",
    "        elif query_char != '-' and template_char == '-':\n",
    "            query_idx += 1\n",
    "        elif query_char == '-' and template_char != '-':\n",
    "            template_idx += 1\n",
    "    \n",
    "    # IMPROVED GAP FILLING - maintains RNA backbone geometry\n",
    "    backbone_distance = 5.9  # Typical C1'-C1' distance\n",
    "    \n",
    "    # Fill gaps by maintaining realistic backbone connectivity\n",
    "    for i in range(len(query_coords)):\n",
    "        if np.isnan(query_coords[i, 0]):\n",
    "            # Find nearest valid neighbors\n",
    "            prev_valid = next_valid = None\n",
    "            \n",
    "            for j in range(i-1, -1, -1):\n",
    "                if not np.isnan(query_coords[j, 0]):\n",
    "                    prev_valid = j\n",
    "                    break\n",
    "                    \n",
    "            for j in range(i+1, len(query_coords)):\n",
    "                if not np.isnan(query_coords[j, 0]):\n",
    "                    next_valid = j\n",
    "                    break\n",
    "            \n",
    "            if prev_valid is not None and next_valid is not None:\n",
    "                # Interpolate along realistic RNA backbone path\n",
    "                gap_size = next_valid - prev_valid\n",
    "                total_distance = np.linalg.norm(query_coords[next_valid] - query_coords[prev_valid])\n",
    "                expected_distance = gap_size * backbone_distance\n",
    "                \n",
    "                # If gap is compressed, extend it realistically\n",
    "                if total_distance < expected_distance * 0.7:\n",
    "                    direction = query_coords[next_valid] - query_coords[prev_valid]\n",
    "                    direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "                    \n",
    "                    # Place intermediate points along extended path\n",
    "                    for k, idx in enumerate(range(prev_valid + 1, next_valid)):\n",
    "                        progress = (k + 1) / gap_size\n",
    "                        base_pos = query_coords[prev_valid] + direction * expected_distance * progress\n",
    "                        \n",
    "                        # Add slight curvature for realism\n",
    "                        perpendicular = np.cross(direction, [0, 0, 1])\n",
    "                        if np.linalg.norm(perpendicular) < 1e-6:\n",
    "                            perpendicular = np.cross(direction, [1, 0, 0])\n",
    "                        perpendicular = perpendicular / (np.linalg.norm(perpendicular) + 1e-10)\n",
    "                        \n",
    "                        curve_amplitude = 2.0 * np.sin(progress * np.pi)\n",
    "                        query_coords[idx] = base_pos + perpendicular * curve_amplitude\n",
    "                else:\n",
    "                    # Linear interpolation for normal gaps\n",
    "                    for k, idx in enumerate(range(prev_valid + 1, next_valid)):\n",
    "                        weight = (k + 1) / gap_size\n",
    "                        query_coords[idx] = (1 - weight) * query_coords[prev_valid] + weight * query_coords[next_valid]\n",
    "            \n",
    "            elif prev_valid is not None:\n",
    "                # Extend from previous position\n",
    "                if prev_valid > 0 and not np.isnan(query_coords[prev_valid-1, 0]):\n",
    "                    direction = query_coords[prev_valid] - query_coords[prev_valid-1]\n",
    "                    direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "                else:\n",
    "                    direction = np.array([1.0, 0.0, 0.0])\n",
    "                \n",
    "                steps_needed = i - prev_valid\n",
    "                for step in range(1, steps_needed + 1):\n",
    "                    pos_idx = prev_valid + step\n",
    "                    if pos_idx < len(query_coords):\n",
    "                        query_coords[pos_idx] = query_coords[prev_valid] + direction * backbone_distance * step\n",
    "            \n",
    "            elif next_valid is not None:\n",
    "                # Work backwards from next position\n",
    "                direction = np.array([-1.0, 0.0, 0.0])  # Default backward direction\n",
    "                steps_needed = next_valid - i\n",
    "                for step in range(steps_needed, 0, -1):\n",
    "                    pos_idx = next_valid - step\n",
    "                    if pos_idx >= 0:\n",
    "                        query_coords[pos_idx] = query_coords[next_valid] - direction * backbone_distance * step\n",
    "    \n",
    "    # Final cleanup\n",
    "    query_coords = np.nan_to_num(query_coords)\n",
    "    return query_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1727f1a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:37:04.251196Z",
     "iopub.status.busy": "2026-01-09T18:37:04.250794Z",
     "iopub.status.idle": "2026-01-09T18:37:04.264172Z",
     "shell.execute_reply": "2026-01-09T18:37:04.263090Z"
    },
    "papermill": {
     "duration": 0.333072,
     "end_time": "2026-01-09T18:37:04.265930",
     "exception": false,
     "start_time": "2026-01-09T18:37:03.932858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_improved_rna_structure(sequence):\n",
    "    \"\"\"\n",
    "    Generate a more realistic RNA structure fallback based on sequence patterns\n",
    "    and basic RNA structure principles.\n",
    "    \n",
    "    Args:\n",
    "        sequence: RNA sequence string\n",
    "        \n",
    "    Returns:\n",
    "        Array of 3D coordinates\n",
    "    \"\"\"\n",
    "    n_residues = len(sequence)\n",
    "    coordinates = np.zeros((n_residues, 3))\n",
    "    \n",
    "    # Analyze sequence to predict structural elements\n",
    "    # Look for complementary regions that could form base pairs\n",
    "    potential_stems = identify_potential_stems(sequence)\n",
    "    \n",
    "    # Default parameters\n",
    "    radius_helix = 10.0\n",
    "    radius_loop = 15.0\n",
    "    rise_per_residue_helix = 2.5\n",
    "    rise_per_residue_loop = 1.5\n",
    "    angle_per_residue_helix = 0.6\n",
    "    angle_per_residue_loop = 0.3\n",
    "    \n",
    "    # Assign structural classifications\n",
    "    structure_types = assign_structure_types(sequence, potential_stems)\n",
    "    \n",
    "    # Generate coordinates based on predicted structure\n",
    "    current_pos = np.array([0.0, 0.0, 0.0])\n",
    "    current_direction = np.array([0.0, 0.0, 1.0])\n",
    "    current_angle = 0.0\n",
    "    \n",
    "    for i in range(n_residues):\n",
    "        if structure_types[i] == 'stem':\n",
    "            # Part of a helical stem\n",
    "            current_angle += angle_per_residue_helix\n",
    "            coordinates[i] = [\n",
    "                radius_helix * np.cos(current_angle), \n",
    "                radius_helix * np.sin(current_angle), \n",
    "                current_pos[2] + rise_per_residue_helix\n",
    "            ]\n",
    "            current_pos = coordinates[i]\n",
    "        elif structure_types[i] == 'loop':\n",
    "            # Part of a loop\n",
    "            current_angle += angle_per_residue_loop\n",
    "            z_shift = rise_per_residue_loop * np.sin(current_angle * 0.5)\n",
    "            coordinates[i] = [\n",
    "                radius_loop * np.cos(current_angle), \n",
    "                radius_loop * np.sin(current_angle), \n",
    "                current_pos[2] + z_shift\n",
    "            ]\n",
    "            current_pos = coordinates[i]\n",
    "        else:\n",
    "            # Single-stranded region\n",
    "            # Add some randomness to make it look more realistic\n",
    "            jitter = np.random.normal(0, 1, 3) * 2.0\n",
    "            coordinates[i] = current_pos + jitter\n",
    "            current_pos = coordinates[i]\n",
    "            \n",
    "    return coordinates\n",
    "\n",
    "def identify_potential_stems(sequence):\n",
    "    \"\"\"\n",
    "    Identify potential stem regions by looking for self-complementary segments.\n",
    "    \n",
    "    Args:\n",
    "        sequence: RNA sequence string\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples (start1, end1, start2, end2) representing potentially paired regions\n",
    "    \"\"\"\n",
    "    complementary_bases = {'A': 'U', 'U': 'A', 'G': 'C', 'C': 'G'}\n",
    "    min_stem_length = 3\n",
    "    potential_stems = []\n",
    "    \n",
    "    # Simple stem identification\n",
    "    for i in range(len(sequence) - min_stem_length):\n",
    "        for j in range(i + min_stem_length + 3, len(sequence) - min_stem_length + 1):\n",
    "            # Check if regions could form a stem\n",
    "            potential_stem_len = min(min_stem_length, len(sequence) - j)\n",
    "            is_stem = True\n",
    "            \n",
    "            for k in range(potential_stem_len):\n",
    "                if sequence[i+k] not in complementary_bases or \\\n",
    "                   complementary_bases[sequence[i+k]] != sequence[j+potential_stem_len-k-1]:\n",
    "                    is_stem = False\n",
    "                    break\n",
    "            \n",
    "            if is_stem:\n",
    "                potential_stems.append((i, i+potential_stem_len-1, j, j+potential_stem_len-1))\n",
    "    \n",
    "    return potential_stems\n",
    "\n",
    "def assign_structure_types(sequence, potential_stems):\n",
    "    \"\"\"\n",
    "    Assign each nucleotide to a structural element type.\n",
    "    \n",
    "    Args:\n",
    "        sequence: RNA sequence string\n",
    "        potential_stems: List of tuples representing stem regions\n",
    "        \n",
    "    Returns:\n",
    "        List of structure types ('stem', 'loop', 'single')\n",
    "    \"\"\"\n",
    "    structure_types = ['single'] * len(sequence)\n",
    "    \n",
    "    # Mark stem regions\n",
    "    for stem in potential_stems:\n",
    "        start1, end1, start2, end2 = stem\n",
    "        for i in range(end1 - start1 + 1):\n",
    "            structure_types[start1 + i] = 'stem'\n",
    "            structure_types[end2 - i] = 'stem'\n",
    "    \n",
    "    # Mark loop regions (regions between paired regions)\n",
    "    for i in range(len(potential_stems) - 1):\n",
    "        _, end1, start2, _ = potential_stems[i]\n",
    "        next_start1, _, _, _ = potential_stems[i+1]\n",
    "        \n",
    "        if next_start1 > end1 + 1 and start2 > next_start1:\n",
    "            for j in range(end1 + 1, next_start1):\n",
    "                structure_types[j] = 'loop'\n",
    "    \n",
    "    return structure_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48fdc375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:37:04.824308Z",
     "iopub.status.busy": "2026-01-09T18:37:04.823925Z",
     "iopub.status.idle": "2026-01-09T18:37:04.836335Z",
     "shell.execute_reply": "2026-01-09T18:37:04.835223Z"
    },
    "papermill": {
     "duration": 0.293332,
     "end_time": "2026-01-09T18:37:04.838172",
     "exception": false,
     "start_time": "2026-01-09T18:37:04.544840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create a more realistic RNA structure when no good templates are found\n",
    "def generate_rna_structure(sequence, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "    \n",
    "    n_residues = len(sequence)\n",
    "    coordinates = np.zeros((n_residues, 3))\n",
    "    \n",
    "    # Initialize the first few residues in a helix\n",
    "    for i in range(min(3, n_residues)):\n",
    "        angle = i * 0.6\n",
    "        coordinates[i] = [10.0 * np.cos(angle), 10.0 * np.sin(angle), i * 2.5]\n",
    "    \n",
    "    # Add more complex folding patterns\n",
    "    current_direction = np.array([0.0, 0.0, 1.0])  # Start moving along z-axis\n",
    "    \n",
    "    # Define base-pairing tendencies (G-C and A-U pairs)\n",
    "    for i in range(3, n_residues):\n",
    "        # Check for potential base-pairing in the sequence\n",
    "        has_pair = False\n",
    "        pair_idx = -1\n",
    "        \n",
    "        # Simple detection of complementary bases (G-C, A-U)\n",
    "        complementary = {'G': 'C', 'C': 'G', 'A': 'U', 'U': 'A'}\n",
    "        current_base = sequence[i]\n",
    "        \n",
    "        # Look for potential base-pairing within a window before the current position\n",
    "        window_size = min(i, 15)  # Look back up to 15 bases\n",
    "        for j in range(i-window_size, i):\n",
    "            if j >= 0 and sequence[j] == complementary.get(current_base, 'X'):\n",
    "                # Found a potential pair\n",
    "                has_pair = True\n",
    "                pair_idx = j\n",
    "                break\n",
    "        \n",
    "        if has_pair and i - pair_idx <= 10 and random.random() < 0.7:\n",
    "            # Try to create a base-pair by positioning this nucleotide near its pair\n",
    "            pair_pos = coordinates[pair_idx]\n",
    "            \n",
    "            # Create a position that's roughly opposite to the pair\n",
    "            random_offset = np.random.normal(0, 1, 3) * 2.0\n",
    "            base_pair_distance = 10.0 + random.uniform(-1.0, 1.0)\n",
    "            \n",
    "            # Calculate a vector from base-pair toward center of structure\n",
    "            center = np.mean(coordinates[:i], axis=0)\n",
    "            direction = center - pair_pos\n",
    "            direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "            \n",
    "            # Position new nucleotide in the general direction of the \"center\"\n",
    "            coordinates[i] = pair_pos + direction * base_pair_distance + random_offset\n",
    "            \n",
    "            # Update direction for next nucleotide\n",
    "            current_direction = np.random.normal(0, 0.3, 3)\n",
    "            current_direction = current_direction / (np.linalg.norm(current_direction) + 1e-10)\n",
    "            \n",
    "        else:\n",
    "            # No base-pairing detected, continue with the current fold direction\n",
    "            # Randomly rotate current direction to simulate RNA flexibility\n",
    "            if random.random() < 0.3:\n",
    "                # More significant direction change\n",
    "                angle = random.uniform(0.2, 0.6)\n",
    "                axis = np.random.normal(0, 1, 3)\n",
    "                axis = axis / (np.linalg.norm(axis) + 1e-10)\n",
    "                rotation = R.from_rotvec(angle * axis)\n",
    "                current_direction = rotation.apply(current_direction)\n",
    "            else:\n",
    "                # Small random changes in direction\n",
    "                current_direction += np.random.normal(0, 0.15, 3)\n",
    "                current_direction = current_direction / (np.linalg.norm(current_direction) + 1e-10)\n",
    "            \n",
    "            # Distance between consecutive nucleotides (3.5-4.5Å is typical)\n",
    "            step_size = random.uniform(3.5, 4.5)\n",
    "            \n",
    "            # Update position\n",
    "            coordinates[i] = coordinates[i-1] + step_size * current_direction\n",
    "    \n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a06a9d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:37:05.405604Z",
     "iopub.status.busy": "2026-01-09T18:37:05.405249Z",
     "iopub.status.idle": "2026-01-09T18:37:05.413931Z",
     "shell.execute_reply": "2026-01-09T18:37:05.412348Z"
    },
    "papermill": {
     "duration": 0.295115,
     "end_time": "2026-01-09T18:37:05.415851",
     "exception": false,
     "start_time": "2026-01-09T18:37:05.120736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_rna_structures(sequence, target_id, train_seqs_df, train_coords_dict, n_predictions=5):\n",
    "    predictions = []\n",
    "    \n",
    "    # Find similar sequences in the training data\n",
    "    similar_seqs = find_similar_sequences(sequence, train_seqs_df, train_coords_dict, top_n=n_predictions)\n",
    "    \n",
    "    # If we found any similar sequences, use them as templates\n",
    "    if similar_seqs:\n",
    "        for i, (template_id, template_seq, similarity_score, template_coords) in enumerate(similar_seqs):\n",
    "            # Adapt template coordinates to the query sequence\n",
    "            adapted_coords = adapt_template_to_query(sequence, template_seq, template_coords)\n",
    "            \n",
    "            if adapted_coords is not None:\n",
    "                # Apply adaptive constraints based on template similarity\n",
    "                # For high similarity templates, apply very gentle constraints\n",
    "                refined_coords = adaptive_rna_constraints(adapted_coords, sequence, confidence=similarity_score)\n",
    "                \n",
    "                # Add some randomness (less for better templates)\n",
    "                random_scale = max(0.05, 0.8 - similarity_score)  # Reduced randomness\n",
    "                randomized_coords = refined_coords.copy()\n",
    "                randomized_coords += np.random.normal(0, random_scale, randomized_coords.shape)\n",
    "                \n",
    "                predictions.append(randomized_coords)\n",
    "                \n",
    "                if len(predictions) >= n_predictions:\n",
    "                    break\n",
    "    \n",
    "    # If we don't have enough predictions from templates, generate de novo structures\n",
    "    while len(predictions) < n_predictions:\n",
    "        seed_value = hash(target_id) % 10000 + len(predictions) * 1000\n",
    "        de_novo_coords = generate_rna_structure(sequence, seed=seed_value)\n",
    "        \n",
    "        # Apply stronger constraints to de novo structures (lower confidence)\n",
    "        refined_de_novo = adaptive_rna_constraints(de_novo_coords, sequence, confidence=0.2)\n",
    "        \n",
    "        predictions.append(refined_de_novo)\n",
    "    \n",
    "    return predictions[:n_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c32dcaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T18:37:06.028784Z",
     "iopub.status.busy": "2026-01-09T18:37:06.028295Z",
     "iopub.status.idle": "2026-01-09T23:15:28.193739Z",
     "shell.execute_reply": "2026-01-09T23:15:28.192530Z"
    },
    "papermill": {
     "duration": 16702.775074,
     "end_time": "2026-01-09T23:15:28.472990",
     "exception": false,
     "start_time": "2026-01-09T18:37:05.697916",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target 1/28: 8ZNQ (30 nt), elapsed: 0.0s, est. remaining: 0.0s\n",
      "Processing target 6/28: 9E9Q (101 nt), elapsed: 12761.1s, est. remaining: 46790.8s\n",
      "Processing target 11/28: 9G4R (47 nt), elapsed: 12962.5s, est. remaining: 20033.0s\n",
      "Processing target 16/28: 9I9W (28 nt), elapsed: 13167.2s, est. remaining: 9875.4s\n",
      "Processing target 21/28: 9WHV (80 nt), elapsed: 13274.6s, est. remaining: 4424.9s\n",
      "Processing target 26/28: 9EBP (81 nt), elapsed: 13508.9s, est. remaining: 1039.1s\n",
      "Generated predictions for 28 RNA sequences\n",
      "Total runtime: 16702.2 seconds\n"
     ]
    }
   ],
   "source": [
    "# List to store all prediction records\n",
    "all_predictions = []\n",
    "\n",
    "# Set up time tracking\n",
    "start_time = time.time()\n",
    "total_targets = len(test_seqs)\n",
    "\n",
    "# For each sequence in the test set\n",
    "for idx, row in test_seqs.iterrows():\n",
    "    target_id = row['target_id']\n",
    "    sequence = row['sequence']\n",
    "    \n",
    "    # Progress tracking\n",
    "    if idx % 5 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        targets_processed = idx + 1\n",
    "        if targets_processed > 0:\n",
    "            avg_time_per_target = elapsed / targets_processed\n",
    "            est_time_remaining = avg_time_per_target * (total_targets - targets_processed)\n",
    "            print(f\"Processing target {targets_processed}/{total_targets}: {target_id} ({len(sequence)} nt), \"\n",
    "                  f\"elapsed: {elapsed:.1f}s, est. remaining: {est_time_remaining:.1f}s\")\n",
    "    \n",
    "    # Generate 5 different structure predictions\n",
    "    predictions = predict_rna_structures(sequence, target_id, train_seqs_extended, train_coords_dict, n_predictions=5)\n",
    "    \n",
    "    # For each residue in the sequence\n",
    "    for j in range(len(sequence)):\n",
    "        pred_row = {\n",
    "            'ID': f\"{target_id}_{j+1}\",\n",
    "            'resname': sequence[j],\n",
    "            'resid': j + 1\n",
    "        }\n",
    "        \n",
    "        # Add coordinates from all 5 predictions\n",
    "        for i in range(5):\n",
    "            pred_row[f'x_{i+1}'] = predictions[i][j][0]\n",
    "            pred_row[f'y_{i+1}'] = predictions[i][j][1]\n",
    "            pred_row[f'z_{i+1}'] = predictions[i][j][2]\n",
    "        \n",
    "        all_predictions.append(pred_row)\n",
    "\n",
    "# Create DataFrame with predictions\n",
    "submission_df = pd.DataFrame(all_predictions)\n",
    "\n",
    "# Ensure the submission file has the correct format\n",
    "column_order = ['ID', 'resname', 'resid']\n",
    "for i in range(1, 6):\n",
    "    for coord in ['x', 'y', 'z']:\n",
    "        column_order.append(f'{coord}_{i}')\n",
    "submission_df = submission_df[column_order]\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(f\"Generated predictions for {len(test_seqs)} RNA sequences\")\n",
    "print(f\"Total runtime: {time.time() - start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd582490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T23:15:29.038389Z",
     "iopub.status.busy": "2026-01-09T23:15:29.037997Z",
     "iopub.status.idle": "2026-01-09T23:15:29.061482Z",
     "shell.execute_reply": "2026-01-09T23:15:29.060431Z"
    },
    "papermill": {
     "duration": 0.308298,
     "end_time": "2026-01-09T23:15:29.063208",
     "exception": false,
     "start_time": "2026-01-09T23:15:28.754910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>resname</th>\n",
       "      <th>resid</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>z_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>z_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>y_3</th>\n",
       "      <th>z_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>y_4</th>\n",
       "      <th>z_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>y_5</th>\n",
       "      <th>z_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8ZNQ_1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>35.853228</td>\n",
       "      <td>95.261037</td>\n",
       "      <td>62.394657</td>\n",
       "      <td>23.024874</td>\n",
       "      <td>19.139786</td>\n",
       "      <td>41.649047</td>\n",
       "      <td>197.884904</td>\n",
       "      <td>120.256265</td>\n",
       "      <td>158.482942</td>\n",
       "      <td>-3.098839</td>\n",
       "      <td>-20.145793</td>\n",
       "      <td>3.797393</td>\n",
       "      <td>190.226501</td>\n",
       "      <td>126.255426</td>\n",
       "      <td>225.922881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8ZNQ_2</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>29.757382</td>\n",
       "      <td>95.118496</td>\n",
       "      <td>62.386624</td>\n",
       "      <td>19.737772</td>\n",
       "      <td>19.454974</td>\n",
       "      <td>37.456936</td>\n",
       "      <td>192.052225</td>\n",
       "      <td>119.817063</td>\n",
       "      <td>158.778604</td>\n",
       "      <td>-1.551140</td>\n",
       "      <td>-17.865362</td>\n",
       "      <td>9.150216</td>\n",
       "      <td>184.402980</td>\n",
       "      <td>126.618741</td>\n",
       "      <td>225.704096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8ZNQ_3</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>24.261750</td>\n",
       "      <td>95.186504</td>\n",
       "      <td>62.916168</td>\n",
       "      <td>17.567870</td>\n",
       "      <td>23.672571</td>\n",
       "      <td>34.667171</td>\n",
       "      <td>194.381658</td>\n",
       "      <td>124.049484</td>\n",
       "      <td>156.158124</td>\n",
       "      <td>1.948366</td>\n",
       "      <td>-14.903012</td>\n",
       "      <td>11.661395</td>\n",
       "      <td>178.433950</td>\n",
       "      <td>126.312357</td>\n",
       "      <td>225.162743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8ZNQ_4</td>\n",
       "      <td>G</td>\n",
       "      <td>4</td>\n",
       "      <td>18.244248</td>\n",
       "      <td>95.138361</td>\n",
       "      <td>62.492362</td>\n",
       "      <td>20.064861</td>\n",
       "      <td>32.577292</td>\n",
       "      <td>33.704380</td>\n",
       "      <td>196.686669</td>\n",
       "      <td>126.375146</td>\n",
       "      <td>151.856519</td>\n",
       "      <td>5.756412</td>\n",
       "      <td>-10.433468</td>\n",
       "      <td>12.052913</td>\n",
       "      <td>172.404635</td>\n",
       "      <td>126.496839</td>\n",
       "      <td>225.920147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8ZNQ_5</td>\n",
       "      <td>U</td>\n",
       "      <td>5</td>\n",
       "      <td>12.416085</td>\n",
       "      <td>95.257703</td>\n",
       "      <td>62.769748</td>\n",
       "      <td>24.808092</td>\n",
       "      <td>35.719246</td>\n",
       "      <td>33.954790</td>\n",
       "      <td>196.581674</td>\n",
       "      <td>128.657010</td>\n",
       "      <td>146.959312</td>\n",
       "      <td>7.364590</td>\n",
       "      <td>-6.499753</td>\n",
       "      <td>8.646469</td>\n",
       "      <td>167.037361</td>\n",
       "      <td>126.519636</td>\n",
       "      <td>225.772730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9757</th>\n",
       "      <td>9ZCC_1456</td>\n",
       "      <td>C</td>\n",
       "      <td>1456</td>\n",
       "      <td>268.868061</td>\n",
       "      <td>321.375172</td>\n",
       "      <td>327.454360</td>\n",
       "      <td>202.828932</td>\n",
       "      <td>252.224056</td>\n",
       "      <td>280.706453</td>\n",
       "      <td>170.063077</td>\n",
       "      <td>270.940731</td>\n",
       "      <td>211.620175</td>\n",
       "      <td>134.501384</td>\n",
       "      <td>200.558758</td>\n",
       "      <td>195.484593</td>\n",
       "      <td>392.683218</td>\n",
       "      <td>347.196496</td>\n",
       "      <td>334.487567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9758</th>\n",
       "      <td>9ZCC_1457</td>\n",
       "      <td>C</td>\n",
       "      <td>1457</td>\n",
       "      <td>271.820210</td>\n",
       "      <td>316.737674</td>\n",
       "      <td>330.309371</td>\n",
       "      <td>203.668549</td>\n",
       "      <td>255.903268</td>\n",
       "      <td>276.476889</td>\n",
       "      <td>164.769397</td>\n",
       "      <td>272.743010</td>\n",
       "      <td>207.928152</td>\n",
       "      <td>135.523424</td>\n",
       "      <td>203.052724</td>\n",
       "      <td>200.595778</td>\n",
       "      <td>394.633866</td>\n",
       "      <td>341.962072</td>\n",
       "      <td>339.528652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9759</th>\n",
       "      <td>9ZCC_1458</td>\n",
       "      <td>U</td>\n",
       "      <td>1458</td>\n",
       "      <td>257.656430</td>\n",
       "      <td>345.283953</td>\n",
       "      <td>316.577209</td>\n",
       "      <td>205.729108</td>\n",
       "      <td>260.229961</td>\n",
       "      <td>273.748580</td>\n",
       "      <td>159.293100</td>\n",
       "      <td>274.352595</td>\n",
       "      <td>206.890874</td>\n",
       "      <td>138.591985</td>\n",
       "      <td>202.148681</td>\n",
       "      <td>204.964104</td>\n",
       "      <td>393.222914</td>\n",
       "      <td>336.315031</td>\n",
       "      <td>341.644099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9760</th>\n",
       "      <td>9ZCC_1459</td>\n",
       "      <td>U</td>\n",
       "      <td>1459</td>\n",
       "      <td>252.290799</td>\n",
       "      <td>355.578117</td>\n",
       "      <td>314.521149</td>\n",
       "      <td>210.936312</td>\n",
       "      <td>261.663166</td>\n",
       "      <td>272.066652</td>\n",
       "      <td>154.033846</td>\n",
       "      <td>275.871063</td>\n",
       "      <td>204.477851</td>\n",
       "      <td>139.326855</td>\n",
       "      <td>197.649997</td>\n",
       "      <td>207.896754</td>\n",
       "      <td>397.504319</td>\n",
       "      <td>333.209147</td>\n",
       "      <td>345.576420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9761</th>\n",
       "      <td>9ZCC_1460</td>\n",
       "      <td>A</td>\n",
       "      <td>1460</td>\n",
       "      <td>251.429916</td>\n",
       "      <td>356.813827</td>\n",
       "      <td>319.306356</td>\n",
       "      <td>216.298919</td>\n",
       "      <td>264.153098</td>\n",
       "      <td>270.400201</td>\n",
       "      <td>148.926848</td>\n",
       "      <td>278.755599</td>\n",
       "      <td>202.339728</td>\n",
       "      <td>140.560986</td>\n",
       "      <td>192.752953</td>\n",
       "      <td>209.291746</td>\n",
       "      <td>401.006225</td>\n",
       "      <td>327.893683</td>\n",
       "      <td>348.304936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9762 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID resname  resid         x_1         y_1         z_1  \\\n",
       "0        8ZNQ_1       A      1   35.853228   95.261037   62.394657   \n",
       "1        8ZNQ_2       C      2   29.757382   95.118496   62.386624   \n",
       "2        8ZNQ_3       C      3   24.261750   95.186504   62.916168   \n",
       "3        8ZNQ_4       G      4   18.244248   95.138361   62.492362   \n",
       "4        8ZNQ_5       U      5   12.416085   95.257703   62.769748   \n",
       "...         ...     ...    ...         ...         ...         ...   \n",
       "9757  9ZCC_1456       C   1456  268.868061  321.375172  327.454360   \n",
       "9758  9ZCC_1457       C   1457  271.820210  316.737674  330.309371   \n",
       "9759  9ZCC_1458       U   1458  257.656430  345.283953  316.577209   \n",
       "9760  9ZCC_1459       U   1459  252.290799  355.578117  314.521149   \n",
       "9761  9ZCC_1460       A   1460  251.429916  356.813827  319.306356   \n",
       "\n",
       "             x_2         y_2         z_2         x_3         y_3         z_3  \\\n",
       "0      23.024874   19.139786   41.649047  197.884904  120.256265  158.482942   \n",
       "1      19.737772   19.454974   37.456936  192.052225  119.817063  158.778604   \n",
       "2      17.567870   23.672571   34.667171  194.381658  124.049484  156.158124   \n",
       "3      20.064861   32.577292   33.704380  196.686669  126.375146  151.856519   \n",
       "4      24.808092   35.719246   33.954790  196.581674  128.657010  146.959312   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "9757  202.828932  252.224056  280.706453  170.063077  270.940731  211.620175   \n",
       "9758  203.668549  255.903268  276.476889  164.769397  272.743010  207.928152   \n",
       "9759  205.729108  260.229961  273.748580  159.293100  274.352595  206.890874   \n",
       "9760  210.936312  261.663166  272.066652  154.033846  275.871063  204.477851   \n",
       "9761  216.298919  264.153098  270.400201  148.926848  278.755599  202.339728   \n",
       "\n",
       "             x_4         y_4         z_4         x_5         y_5         z_5  \n",
       "0      -3.098839  -20.145793    3.797393  190.226501  126.255426  225.922881  \n",
       "1      -1.551140  -17.865362    9.150216  184.402980  126.618741  225.704096  \n",
       "2       1.948366  -14.903012   11.661395  178.433950  126.312357  225.162743  \n",
       "3       5.756412  -10.433468   12.052913  172.404635  126.496839  225.920147  \n",
       "4       7.364590   -6.499753    8.646469  167.037361  126.519636  225.772730  \n",
       "...          ...         ...         ...         ...         ...         ...  \n",
       "9757  134.501384  200.558758  195.484593  392.683218  347.196496  334.487567  \n",
       "9758  135.523424  203.052724  200.595778  394.633866  341.962072  339.528652  \n",
       "9759  138.591985  202.148681  204.964104  393.222914  336.315031  341.644099  \n",
       "9760  139.326855  197.649997  207.896754  397.504319  333.209147  345.576420  \n",
       "9761  140.560986  192.752953  209.291746  401.006225  327.893683  348.304936  \n",
       "\n",
       "[9762 rows x 18 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b3144",
   "metadata": {
    "papermill": {
     "duration": 0.275512,
     "end_time": "2026-01-09T23:15:29.613659",
     "exception": false,
     "start_time": "2026-01-09T23:15:29.338147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15231210,
     "isSourceIdPinned": false,
     "sourceId": 118765,
     "sourceType": "competition"
    },
    {
     "datasetId": 7306643,
     "sourceId": 11644010,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7526656,
     "sourceId": 11969392,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18286.097676,
   "end_time": "2026-01-09T23:15:32.734563",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-09T18:10:46.636887",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
