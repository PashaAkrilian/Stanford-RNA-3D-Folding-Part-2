{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f64325",
   "metadata": {
    "_cell_guid": "0ec49051-b8ac-4f27-b152-0c2fe59d4b5c",
    "_uuid": "7e2d63ef-4429-4722-95ee-6681af62a6d5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-09T19:45:23.776403Z",
     "iopub.status.busy": "2026-01-09T19:45:23.776086Z",
     "iopub.status.idle": "2026-01-09T19:45:37.725625Z",
     "shell.execute_reply": "2026-01-09T19:45:37.724615Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 13.957721,
     "end_time": "2026-01-09T19:45:37.727369",
     "exception": false,
     "start_time": "2026-01-09T19:45:23.769648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data files...\n",
      "Loaded 5716 training sequences, 28 validation sequences, and 28 test sequences\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial import distance_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nLoading data files...\")\n",
    "train_seqs = pd.read_csv('/kaggle/input/stanford-rna-3d-folding-2/train_sequences.csv')\n",
    "valid_seqs = pd.read_csv('/kaggle/input/stanford-rna-3d-folding-2/validation_sequences.csv')\n",
    "test_seqs = pd.read_csv('/kaggle/input/stanford-rna-3d-folding-2/test_sequences.csv')\n",
    "train_labels = pd.read_csv('/kaggle/input/stanford-rna-3d-folding-2/train_labels.csv')\n",
    "valid_labels = pd.read_csv('/kaggle/input/stanford-rna-3d-folding-2/validation_labels.csv')\n",
    "\n",
    "print(f\"Loaded {len(train_seqs)} training sequences, {len(valid_seqs)} validation sequences, and {len(test_seqs)} test sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b6aefd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:45:37.739111Z",
     "iopub.status.busy": "2026-01-09T19:45:37.738795Z",
     "iopub.status.idle": "2026-01-09T19:45:37.744257Z",
     "shell.execute_reply": "2026-01-09T19:45:37.743317Z"
    },
    "papermill": {
     "duration": 0.012969,
     "end_time": "2026-01-09T19:45:37.745783",
     "exception": false,
     "start_time": "2026-01-09T19:45:37.732814",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # 1. Check if v2 is larger than original\n",
    "# print(\"Dataset Size Comparison:\")\n",
    "# print(f\"train_seqs: {len(train_seqs)} rows → train_seqs_v2: {len(train_seqs_v2)} rows\")\n",
    "# print(f\"train_labels: {len(train_labels)} rows → train_labels_v2: {len(train_labels_v2)} rows\")\n",
    "# print()\n",
    "\n",
    "# # 2. Verify all train_seqs records exist in train_seqs_v2\n",
    "# print(\"Checking if original sequences exist in v2...\")\n",
    "# original_target_ids = set(train_seqs['target_id'])\n",
    "# extended_target_ids = set(train_seqs_v2['target_id'])\n",
    "# all_targets_included = original_target_ids.issubset(extended_target_ids)\n",
    "\n",
    "# print(f\"Original train_seqs has {len(original_target_ids)} unique target_ids\")\n",
    "# print(f\"All original target_ids found in train_seqs_v2: {all_targets_included}\")\n",
    "\n",
    "# if not all_targets_included:\n",
    "#     missing_ids = original_target_ids - extended_target_ids\n",
    "#     print(f\"Missing {len(missing_ids)} target_ids\")\n",
    "#     if len(missing_ids) <= 5:\n",
    "#         print(f\"Missing IDs: {list(missing_ids)}\")\n",
    "#     else:\n",
    "#         print(f\"First 5 missing IDs: {list(missing_ids)[:5]}\")\n",
    "# print()\n",
    "\n",
    "# # 3. Check for consistency in a sample of target_ids that exist in both datasets\n",
    "# if all_targets_included:\n",
    "#     print(\"Checking consistency of data between original and v2 sequences...\")\n",
    "#     # Sample a few target_ids that exist in both datasets\n",
    "#     sample_size = min(5, len(original_target_ids))\n",
    "#     sample_ids = np.random.choice(list(original_target_ids), sample_size, replace=False)\n",
    "    \n",
    "#     for target_id in sample_ids:\n",
    "#         original_row = train_seqs[train_seqs['target_id'] == target_id].iloc[0]\n",
    "#         extended_row = train_seqs_v2[train_seqs_v2['target_id'] == target_id].iloc[0]\n",
    "        \n",
    "#         # Compare important columns\n",
    "#         sequence_match = original_row['sequence'] == extended_row['sequence']\n",
    "#         print(f\"Target ID {target_id}: Sequences match: {sequence_match}\")\n",
    "# print()\n",
    "\n",
    "# # 4. Check if all train_labels IDs exist in train_labels_v2\n",
    "# print(\"Checking labels dataset...\")\n",
    "# # Since labels dataset is large, we'll check a sample of IDs\n",
    "# sample_size = min(1000, len(train_labels))\n",
    "# sample_indices = np.random.choice(len(train_labels), sample_size, replace=False)\n",
    "# sample_rows = train_labels.iloc[sample_indices]\n",
    "\n",
    "# # Create a composite key for comparison (ID + resid)\n",
    "# sample_rows['composite_key'] = sample_rows['ID'] + '_' + sample_rows['resid'].astype(str)\n",
    "# train_labels_v2['composite_key'] = train_labels_v2['ID'] + '_' + train_labels_v2['resid'].astype(str)\n",
    "\n",
    "# sample_keys = set(sample_rows['composite_key'])\n",
    "# extended_keys = set(train_labels_v2['composite_key'])\n",
    "\n",
    "# keys_found = sample_keys.issubset(extended_keys)\n",
    "# if keys_found:\n",
    "#     found_percentage = 100\n",
    "# else:\n",
    "#     intersection = sample_keys.intersection(extended_keys)\n",
    "#     found_percentage = (len(intersection) / len(sample_keys)) * 100\n",
    "\n",
    "# print(f\"Sampled {len(sample_keys)} keys from train_labels\")\n",
    "# print(f\"All sampled keys found in train_labels_v2: {keys_found} ({found_percentage:.2f}%)\")\n",
    "\n",
    "# if not keys_found:\n",
    "#     missing_keys = sample_keys - extended_keys\n",
    "#     print(f\"Missing {len(missing_keys)} keys out of {len(sample_keys)} sampled\")\n",
    "#     if len(missing_keys) <= 5:\n",
    "#         print(f\"Missing keys: {list(missing_keys)}\")\n",
    "#     else:\n",
    "#         print(f\"First 5 missing keys: {list(missing_keys)[:5]}\")\n",
    "# print()\n",
    "\n",
    "# # 5. Data type consistency check\n",
    "# print(\"Data type consistency check:\")\n",
    "# print(\"train_seqs vs train_seqs_v2:\")\n",
    "# for col in train_seqs.columns:\n",
    "#     print(f\"  Column '{col}': {train_seqs[col].dtype} → {train_seqs_v2[col].dtype} - Match: {train_seqs[col].dtype == train_seqs_v2[col].dtype}\")\n",
    "\n",
    "# print(\"\\ntrain_labels vs train_labels_v2:\")\n",
    "# for col in train_labels.columns:\n",
    "#     if col != 'composite_key':  # Skip the key we created\n",
    "#         print(f\"  Column '{col}': {train_labels[col].dtype} → {train_labels_v2[col].dtype} - Match: {train_labels[col].dtype == train_labels_v2[col].dtype}\")\n",
    "# print()\n",
    "\n",
    "# # 6. Check for missing values pattern\n",
    "# print(\"Missing values comparison:\")\n",
    "# print(\"train_seqs vs train_seqs_v2:\")\n",
    "# for col in train_seqs.columns:\n",
    "#     original_missing = train_seqs[col].isnull().sum() / len(train_seqs) * 100\n",
    "#     extended_missing = train_seqs_v2[col].isnull().sum() / len(train_seqs_v2) * 100\n",
    "#     print(f\"  Column '{col}': {original_missing:.2f}% → {extended_missing:.2f}%\")\n",
    "\n",
    "# print(\"\\ntrain_labels vs train_labels_v2:\")\n",
    "# for col in train_labels.columns:\n",
    "#     if col != 'composite_key':  # Skip the key we created\n",
    "#         original_missing = train_labels[col].isnull().sum() / len(train_labels) * 100\n",
    "#         extended_missing = train_labels_v2[col].isnull().sum() / len(train_labels_v2) * 100\n",
    "#         print(f\"  Column '{col}': {original_missing:.2f}% → {extended_missing:.2f}%\")\n",
    "# print()\n",
    "\n",
    "# # Clean up the temporary column we added\n",
    "# if 'composite_key' in train_labels_v2.columns:\n",
    "#     train_labels_v2.drop('composite_key', axis=1, inplace=True)\n",
    "\n",
    "# # Final assessment\n",
    "# print(\"FINAL ASSESSMENT:\")\n",
    "# print(\"-\" * 50)\n",
    "# seqs_extended_properly = all_targets_included and len(train_seqs_v2) > len(train_seqs)\n",
    "# labels_extended_properly = found_percentage > 99 and len(train_labels_v2) > len(train_labels)\n",
    "\n",
    "# if seqs_extended_properly and labels_extended_properly:\n",
    "#     print(\"✓ PASS: Both train_seqs_v2 and train_labels_v2 appear to be proper extensions of the original datasets.\")\n",
    "#     print(\"✓ It should be safe to swap them.\")\n",
    "# else:\n",
    "#     print(\"✗ ISSUES DETECTED:\")\n",
    "#     if not seqs_extended_properly:\n",
    "#         print(\"  - train_seqs_v2 may not fully contain train_seqs data\")\n",
    "#     if not labels_extended_properly:\n",
    "#         print(\"  - train_labels_v2 may not fully contain train_labels data\")\n",
    "#     print(\"✗ Recommend investigating the issues above before swapping datasets.\")\n",
    "# print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dba3c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:45:37.756915Z",
     "iopub.status.busy": "2026-01-09T19:45:37.756550Z",
     "iopub.status.idle": "2026-01-09T19:47:03.260180Z",
     "shell.execute_reply": "2026-01-09T19:47:03.259017Z"
    },
    "papermill": {
     "duration": 85.514598,
     "end_time": "2026-01-09T19:47:03.265238",
     "exception": false,
     "start_time": "2026-01-09T19:45:37.750640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXTENDING SEQUENCE DATASETS\n",
      "==================================================\n",
      "Extending train_seqs...\n",
      "  Original size: 5716 rows\n",
      "  v2 size: 28 rows\n",
      "  Keys only in original: 5716\n",
      "  Keys only in v2: 28\n",
      "  Common keys: 0\n",
      "  New records added: 28\n",
      "  Extended dataset size: 5744 rows\n",
      "  Verification - All original keys in extended dataset: True\n",
      "  Column 'ligand_ids': Missing values - Original: 1868, Extended: 1882\n",
      "  Column 'ligand_SMILES': Missing values - Original: 1855, Extended: 1869\n",
      "\n",
      "==================================================\n",
      "EXTENDING LABELS DATASETS\n",
      "==================================================\n",
      "Extending train_labels...\n",
      "  Original size: 7794971 rows\n",
      "  v2 size: 9762 rows\n",
      "  Keys only in original: 7794971\n",
      "  Keys only in v2: 9762\n",
      "  Common keys: 0\n",
      "  New records added: 9762\n",
      "  Extended dataset size: 7804733 rows\n",
      "  Verification - All original keys in extended dataset: True\n",
      "  Column 'x_1': Missing values - Original: 486412, Extended: 486412\n",
      "  Column 'y_1': Missing values - Original: 486412, Extended: 486412\n",
      "  Column 'z_1': Missing values - Original: 486412, Extended: 486412\n",
      "  Column 'x_2': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_2': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_2': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_3': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_3': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_3': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_4': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_4': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_4': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_5': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_5': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_5': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_6': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_6': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_6': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_7': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_7': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_7': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_8': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_8': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_8': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_9': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_9': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_9': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_10': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_10': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_10': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_11': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_11': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_11': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_12': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_12': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_12': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_13': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_13': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_13': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_14': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_14': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_14': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_15': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_15': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_15': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_16': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_16': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_16': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_17': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_17': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_17': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_18': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_18': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_18': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_19': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_19': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_19': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_20': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_20': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_20': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_21': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_21': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_21': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_22': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_22': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_22': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_23': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_23': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_23': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_24': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_24': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_24': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_25': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_25': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_25': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_26': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_26': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_26': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_27': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_27': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_27': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_28': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_28': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_28': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_29': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_29': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_29': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_30': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_30': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_30': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_31': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_31': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_31': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_32': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_32': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_32': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_33': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_33': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_33': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_34': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_34': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_34': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_35': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_35': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_35': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_36': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_36': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_36': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_37': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_37': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_37': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_38': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_38': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_38': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_39': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_39': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_39': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'x_40': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'y_40': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'z_40': Missing values - Original: (not in original), Extended: 7794971\n",
      "  Column 'Usage': Missing values - Original: (not in original), Extended: 7794971\n",
      "\n",
      "==================================================\n",
      "VERIFYING RELATIONSHIPS\n",
      "==================================================\n",
      "Total unique sequence IDs: 5744\n",
      "Sequence IDs with corresponding labels: 0 (0.00%)\n",
      "Sequence IDs without corresponding labels: 5744 (100.00%)\n",
      "Sample of sequence IDs without labels (up to 5):\n",
      "['6ZXD', '6UGI', '4DV0', '7JZX', '2NC0']\n",
      "\n",
      "==================================================\n",
      "SUMMARY OF EXTENDED DATASETS\n",
      "==================================================\n",
      "Original train_seqs: 5716 rows\n",
      "Original train_labels: 7794971 rows\n",
      "Extended train_seqs: 5744 rows (+28)\n",
      "Extended train_labels: 7804733 rows (+9762)\n",
      "\n",
      "==================================================\n",
      "DONE! Extended datasets created.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Ensure v1 exists ---\n",
    "try:\n",
    "    train_seqs\n",
    "except NameError:\n",
    "    train_seqs = pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding-2/train_sequences.csv\")\n",
    "\n",
    "try:\n",
    "    train_labels\n",
    "except NameError:\n",
    "    train_labels = pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding-2/train_labels.csv\")\n",
    "\n",
    "# --- Define \"v2\" as validation (since no separate v2 dataset is provided) ---\n",
    "try:\n",
    "    train_seqs_v2\n",
    "except NameError:\n",
    "    train_seqs_v2 = pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding-2/validation_sequences.csv\")\n",
    "\n",
    "try:\n",
    "    train_labels_v2\n",
    "except NameError:\n",
    "    train_labels_v2 = pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding-2/validation_labels.csv\")\n",
    "\n",
    "\n",
    "# Function to extend the original dataset with new records from v2\n",
    "def extend_dataset(original_df, v2_df, key_columns, dataset_name):\n",
    "    print(f\"Extending {dataset_name}...\")\n",
    "    print(f\"  Original size: {len(original_df)} rows\")\n",
    "    print(f\"  v2 size: {len(v2_df)} rows\")\n",
    "    \n",
    "    # Create a composite key for identification if multiple key columns\n",
    "    if isinstance(key_columns, list) and len(key_columns) > 1:\n",
    "        original_df['temp_key'] = original_df[key_columns].astype(str).agg('_'.join, axis=1)\n",
    "        v2_df['temp_key'] = v2_df[key_columns].astype(str).agg('_'.join, axis=1)\n",
    "        key_for_identification = 'temp_key'\n",
    "    else:\n",
    "        key_for_identification = key_columns[0] if isinstance(key_columns, list) else key_columns\n",
    "    \n",
    "    # Identify unique records in each dataset\n",
    "    original_keys = set(original_df[key_for_identification])\n",
    "    v2_keys = set(v2_df[key_for_identification])\n",
    "    \n",
    "    # Calculate stats\n",
    "    keys_only_in_original = original_keys - v2_keys\n",
    "    keys_only_in_v2 = v2_keys - original_keys \n",
    "    common_keys = original_keys.intersection(v2_keys)\n",
    "    \n",
    "    print(f\"  Keys only in original: {len(keys_only_in_original)}\")\n",
    "    print(f\"  Keys only in v2: {len(keys_only_in_v2)}\")\n",
    "    print(f\"  Common keys: {len(common_keys)}\")\n",
    "    \n",
    "    # Create a mask to filter v2 records that don't exist in original\n",
    "    new_records_mask = ~v2_df[key_for_identification].isin(original_keys)\n",
    "    new_records = v2_df[new_records_mask].copy()\n",
    "    \n",
    "    # Drop temporary key if it was created\n",
    "    if key_for_identification == 'temp_key':\n",
    "        new_records.drop('temp_key', axis=1, inplace=True)\n",
    "        original_df.drop('temp_key', axis=1, inplace=True)\n",
    "    \n",
    "    # Combine original with new records from v2\n",
    "    extended_df = pd.concat([original_df, new_records], ignore_index=True, sort=False)\n",
    "    \n",
    "    # Report final sizes\n",
    "    print(f\"  New records added: {len(new_records)}\")\n",
    "    print(f\"  Extended dataset size: {len(extended_df)} rows\")\n",
    "    print(f\"  Verification - All original keys in extended dataset: {set(original_df[key_columns[0] if isinstance(key_columns, list) else key_columns]).issubset(set(extended_df[key_columns[0] if isinstance(key_columns, list) else key_columns]))}\")\n",
    "    \n",
    "    # Check for missing values in key columns (SAFE for columns not present in original_df)\n",
    "    for col in extended_df.columns:\n",
    "        if col in original_df.columns:\n",
    "            original_missing = original_df[col].isnull().sum()\n",
    "        else:\n",
    "            original_missing = \"(not in original)\"\n",
    "        \n",
    "        extended_missing = extended_df[col].isnull().sum()\n",
    "        \n",
    "        if (original_missing != 0 and original_missing != \"(not in original)\") or (extended_missing > 0) or (original_missing == \"(not in original)\"):\n",
    "            print(f\"  Column '{col}': Missing values - Original: {original_missing}, Extended: {extended_missing}\")\n",
    "    \n",
    "    # Clean up (in case temp_key still exists in v2_df for some reason)\n",
    "    if 'temp_key' in v2_df.columns:\n",
    "        v2_df.drop('temp_key', axis=1, inplace=True)\n",
    "        \n",
    "    return extended_df\n",
    "\n",
    "\n",
    "# 1. Extend train_seqs with train_seqs_v2\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXTENDING SEQUENCE DATASETS\")\n",
    "print(\"=\"*50)\n",
    "train_seqs_extended = extend_dataset(\n",
    "    train_seqs, \n",
    "    train_seqs_v2,\n",
    "    ['target_id'],  # Using target_id as the unique identifier\n",
    "    \"train_seqs\"\n",
    ")\n",
    "\n",
    "# 2. Extend train_labels with train_labels_v2\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXTENDING LABELS DATASETS\")\n",
    "print(\"=\"*50)\n",
    "train_labels_extended = extend_dataset(\n",
    "    train_labels,\n",
    "    train_labels_v2,\n",
    "    ['ID', 'resid'],  # Using composite key\n",
    "    \"train_labels\"\n",
    ")\n",
    "\n",
    "# Verify relationships between extended datasets\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VERIFYING RELATIONSHIPS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "seq_ids = set(train_seqs_extended['target_id'].unique())\n",
    "label_ids = set(train_labels_extended['ID'].unique())\n",
    "\n",
    "seq_ids_with_labels = seq_ids.intersection(label_ids)\n",
    "seq_ids_without_labels = seq_ids - label_ids\n",
    "\n",
    "print(f\"Total unique sequence IDs: {len(seq_ids)}\")\n",
    "print(f\"Sequence IDs with corresponding labels: {len(seq_ids_with_labels)} ({len(seq_ids_with_labels)/len(seq_ids)*100:.2f}%)\")\n",
    "print(f\"Sequence IDs without corresponding labels: {len(seq_ids_without_labels)} ({len(seq_ids_without_labels)/len(seq_ids)*100:.2f}%)\")\n",
    "\n",
    "if len(seq_ids_without_labels) > 0:\n",
    "    print(\"Sample of sequence IDs without labels (up to 5):\")\n",
    "    print(list(seq_ids_without_labels)[:5])\n",
    "\n",
    "# Print summary of extended datasets\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY OF EXTENDED DATASETS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original train_seqs: {len(train_seqs)} rows\")\n",
    "print(f\"Original train_labels: {len(train_labels)} rows\")\n",
    "print(f\"Extended train_seqs: {len(train_seqs_extended)} rows (+{len(train_seqs_extended)-len(train_seqs)})\")\n",
    "print(f\"Extended train_labels: {len(train_labels_extended)} rows (+{len(train_labels_extended)-len(train_labels)})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DONE! Extended datasets created.\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ec6afc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:47:03.279221Z",
     "iopub.status.busy": "2026-01-09T19:47:03.278855Z",
     "iopub.status.idle": "2026-01-09T19:47:03.298781Z",
     "shell.execute_reply": "2026-01-09T19:47:03.297552Z"
    },
    "papermill": {
     "duration": 0.028608,
     "end_time": "2026-01-09T19:47:03.300272",
     "exception": false,
     "start_time": "2026-01-09T19:47:03.271664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5744 entries, 0 to 5743\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   target_id        5744 non-null   object\n",
      " 1   sequence         5744 non-null   object\n",
      " 2   temporal_cutoff  5744 non-null   object\n",
      " 3   description      5744 non-null   object\n",
      " 4   stoichiometry    5744 non-null   object\n",
      " 5   all_sequences    5744 non-null   object\n",
      " 6   ligand_ids       3862 non-null   object\n",
      " 7   ligand_SMILES    3875 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 359.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_seqs_extended.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1df708f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:47:03.314340Z",
     "iopub.status.busy": "2026-01-09T19:47:03.314015Z",
     "iopub.status.idle": "2026-01-09T19:47:03.323507Z",
     "shell.execute_reply": "2026-01-09T19:47:03.322471Z"
    },
    "papermill": {
     "duration": 0.018324,
     "end_time": "2026-01-09T19:47:03.325143",
     "exception": false,
     "start_time": "2026-01-09T19:47:03.306819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7804733 entries, 0 to 7804732\n",
      "Columns: 126 entries, ID to Usage\n",
      "dtypes: float64(120), int64(2), object(4)\n",
      "memory usage: 7.3+ GB\n"
     ]
    }
   ],
   "source": [
    "train_labels_extended.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bccfb95b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:47:03.339153Z",
     "iopub.status.busy": "2026-01-09T19:47:03.338840Z",
     "iopub.status.idle": "2026-01-09T19:47:03.342353Z",
     "shell.execute_reply": "2026-01-09T19:47:03.341482Z"
    },
    "papermill": {
     "duration": 0.012387,
     "end_time": "2026-01-09T19:47:03.343991",
     "exception": false,
     "start_time": "2026-01-09T19:47:03.331604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get the first 500 sequences\n",
    "# train_seqs_small = train_seqs_extended.iloc[:500].copy()\n",
    "\n",
    "# # Extract base IDs from train_labels_extended once\n",
    "# base_ids = train_labels_extended['ID'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# # Filter labels where the base ID is in our sequence IDs\n",
    "# train_labels_small = train_labels_extended[base_ids.isin(train_seqs_small['target_id'])].copy()\n",
    "\n",
    "# # Verify\n",
    "# print(f\"Number of sequences in train_seqs_small: {len(train_seqs_small)}\")\n",
    "# print(f\"Total rows in train_labels_small: {len(train_labels_small)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68973e1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:47:03.358301Z",
     "iopub.status.busy": "2026-01-09T19:47:03.357935Z",
     "iopub.status.idle": "2026-01-09T19:55:45.662421Z",
     "shell.execute_reply": "2026-01-09T19:55:45.661201Z"
    },
    "papermill": {
     "duration": 522.313594,
     "end_time": "2026-01-09T19:55:45.664096",
     "exception": false,
     "start_time": "2026-01-09T19:47:03.350502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing structures: 100%|██████████| 5744/5744 [07:53<00:00, 12.13it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_labels(labels_df):\n",
    "    coords_dict = {}\n",
    "    \n",
    "    # Group by target ID and wrap with tqdm for progress tracking\n",
    "    id_groups = labels_df.groupby(lambda x: labels_df['ID'][x].rsplit('_', 1)[0])\n",
    "    for id_prefix, group in tqdm(id_groups, desc=\"Processing structures\"):\n",
    "        # Extract just the coordinates columns for the first structure (x_1, y_1, z_1)\n",
    "        coords = []\n",
    "        for _, row in group.sort_values('resid').iterrows():\n",
    "            coords.append([row['x_1'], row['y_1'], row['z_1']])\n",
    "        \n",
    "        coords_dict[id_prefix] = np.array(coords)\n",
    "    \n",
    "    return coords_dict\n",
    "\n",
    "train_coords_dict = process_labels(train_labels_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6e322",
   "metadata": {
    "papermill": {
     "duration": 0.079064,
     "end_time": "2026-01-09T19:55:45.877555",
     "exception": false,
     "start_time": "2026-01-09T19:55:45.798491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dff587d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:55:46.038266Z",
     "iopub.status.busy": "2026-01-09T19:55:46.037884Z",
     "iopub.status.idle": "2026-01-09T19:55:46.638565Z",
     "shell.execute_reply": "2026-01-09T19:55:46.637444Z"
    },
    "papermill": {
     "duration": 0.682544,
     "end_time": "2026-01-09T19:55:46.640532",
     "exception": false,
     "start_time": "2026-01-09T19:55:45.957988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio import pairwise2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_sequences(query_seq, train_seqs_df, train_coords_dict, top_n=5):\n",
    "    \"\"\"\n",
    "    Find similar RNA sequences using enhanced scoring and clustering for diversity.\n",
    "    \n",
    "    Improvements:\n",
    "    - Multi-tier length filtering\n",
    "    - Enhanced alignment scoring with multiple algorithms\n",
    "    - RNA-specific structural features\n",
    "    - Adaptive clustering\n",
    "    \"\"\"\n",
    "    similar_seqs = []\n",
    "    query_seq_obj = Seq(query_seq)\n",
    "    query_features = _extract_enhanced_rna_features(query_seq)\n",
    "    \n",
    "    # Step 1: Enhanced candidate selection with multi-tier filtering\n",
    "    for _, row in train_seqs_df.iterrows():\n",
    "        target_id = row['target_id']\n",
    "        train_seq = row['sequence']\n",
    "        \n",
    "        # Skip if coordinates not available\n",
    "        if target_id not in train_coords_dict:\n",
    "            continue\n",
    "        \n",
    "        # Multi-tier length filtering (more permissive for very short/long sequences)\n",
    "        len_ratio = abs(len(train_seq) - len(query_seq)) / max(len(train_seq), len(query_seq))\n",
    "        if len(query_seq) < 50 or len(train_seq) < 50:  # Short sequences - more permissive\n",
    "            if len_ratio > 0.6:\n",
    "                continue\n",
    "        elif len(query_seq) > 1000 or len(train_seq) > 1000:  # Long sequences - stricter\n",
    "            if len_ratio > 0.2:\n",
    "                continue\n",
    "        else:  # Medium sequences - original threshold\n",
    "            if len_ratio > 0.4:\n",
    "                continue\n",
    "        \n",
    "        # Calculate composite similarity score\n",
    "        composite_score = _calculate_composite_similarity(query_seq, train_seq, query_features)\n",
    "        \n",
    "        if composite_score > 0:  # Only keep sequences with positive similarity\n",
    "            similar_seqs.append((target_id, train_seq, composite_score, train_coords_dict[target_id]))\n",
    "    \n",
    "    # Sort by composite score and take top candidates\n",
    "    similar_seqs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Adaptive candidate selection based on score distribution\n",
    "    candidate_count = min(50, len(similar_seqs))  # Increased initial pool\n",
    "    if len(similar_seqs) > 10:\n",
    "        # Filter out sequences with very low scores (bottom 20%)\n",
    "        score_threshold = np.percentile([x[2] for x in similar_seqs], 80)\n",
    "        filtered_candidates = [x for x in similar_seqs if x[2] >= score_threshold]\n",
    "        candidate_count = min(candidate_count, len(filtered_candidates))\n",
    "        top_candidates = filtered_candidates[:candidate_count]\n",
    "    else:\n",
    "        top_candidates = similar_seqs[:candidate_count]\n",
    "    \n",
    "    # If we have fewer sequences than requested clusters, return all\n",
    "    if len(top_candidates) <= top_n:\n",
    "        return top_candidates[:top_n]\n",
    "    \n",
    "    # Step 2: Enhanced feature matrix for better clustering\n",
    "    feature_matrix = []\n",
    "    for _, seq, _, _ in top_candidates:\n",
    "        features = _extract_enhanced_rna_features(seq)\n",
    "        feature_matrix.append(features)\n",
    "    \n",
    "    feature_matrix = np.array(feature_matrix)\n",
    "    \n",
    "    # Step 3: Adaptive clustering\n",
    "    n_clusters = min(top_n, len(top_candidates))\n",
    "    \n",
    "    # Use different clustering approach based on dataset size\n",
    "    if len(top_candidates) >= 15:\n",
    "        # K-means for larger datasets\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "    else:\n",
    "        # Simple diversity-based selection for smaller datasets\n",
    "        cluster_labels = _diversity_based_clustering(feature_matrix, n_clusters)\n",
    "    \n",
    "    # Step 4: Select best representative from each cluster\n",
    "    final_results = []\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_sequences = [top_candidates[i] for i in range(len(top_candidates)) \n",
    "                           if cluster_labels[i] == cluster_id]\n",
    "        \n",
    "        if cluster_sequences:\n",
    "            # Sort by composite score and take the best one\n",
    "            cluster_sequences.sort(key=lambda x: x[2], reverse=True)\n",
    "            final_results.append(cluster_sequences[0])\n",
    "    \n",
    "    # Sort final results by similarity score\n",
    "    final_results.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return final_results[:top_n]\n",
    "\n",
    "def _calculate_composite_similarity(query_seq, train_seq, query_features):\n",
    "    \"\"\"\n",
    "    Calculate composite similarity using multiple alignment methods and features.\n",
    "    \"\"\"\n",
    "    query_seq_obj = Seq(query_seq)\n",
    "    \n",
    "    # 1. Global alignment (original method)\n",
    "    global_alignments = pairwise2.align.globalms(query_seq_obj, train_seq, 2.9, -1, -10, -0.5, one_alignment_only=True)\n",
    "    global_score = 0\n",
    "    if global_alignments:\n",
    "        alignment = global_alignments[0]\n",
    "        global_score = alignment.score / (2 * min(len(query_seq), len(train_seq)))\n",
    "    \n",
    "    # 2. Local alignment for finding similar regions\n",
    "    local_alignments = pairwise2.align.localms(query_seq_obj, train_seq, 2.9, -1, -10, -0.5, one_alignment_only=True)\n",
    "    local_score = 0\n",
    "    if local_alignments:\n",
    "        alignment = local_alignments[0]\n",
    "        local_score = alignment.score / (2 * min(len(query_seq), len(train_seq)))\n",
    "    \n",
    "    # 3. Feature-based similarity\n",
    "    train_features = _extract_enhanced_rna_features(train_seq)\n",
    "    feature_similarity = cosine_similarity([query_features], [train_features])[0][0]\n",
    "    \n",
    "    # 4. K-mer similarity for sequence motifs\n",
    "    kmer_similarity = _calculate_kmer_similarity(query_seq, train_seq, k=3)\n",
    "    \n",
    "    # Weighted composite score\n",
    "    composite_score = (\n",
    "        0.4 * global_score + \n",
    "        0.3 * local_score + \n",
    "        0.2 * feature_similarity + \n",
    "        0.1 * kmer_similarity\n",
    "    )\n",
    "    \n",
    "    return composite_score\n",
    "\n",
    "def _calculate_kmer_similarity(seq1, seq2, k=3):\n",
    "    \"\"\"Calculate k-mer based similarity between sequences.\"\"\"\n",
    "    def get_kmers(seq, k):\n",
    "        return set(seq[i:i+k] for i in range(len(seq) - k + 1))\n",
    "    \n",
    "    kmers1 = get_kmers(seq1.upper(), k)\n",
    "    kmers2 = get_kmers(seq2.upper(), k)\n",
    "    \n",
    "    if not kmers1 or not kmers2:\n",
    "        return 0\n",
    "    \n",
    "    intersection = len(kmers1.intersection(kmers2))\n",
    "    union = len(kmers1.union(kmers2))\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def _diversity_based_clustering(feature_matrix, n_clusters):\n",
    "    \"\"\"Simple diversity-based clustering for small datasets.\"\"\"\n",
    "    n_samples = len(feature_matrix)\n",
    "    cluster_labels = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    if n_samples <= n_clusters:\n",
    "        return np.arange(n_samples)\n",
    "    \n",
    "    # Select diverse representatives\n",
    "    selected_indices = [0]  # Start with first sequence\n",
    "    \n",
    "    for cluster_id in range(1, n_clusters):\n",
    "        max_min_distance = -1\n",
    "        best_idx = -1\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            if i in selected_indices:\n",
    "                continue\n",
    "            \n",
    "            # Find minimum distance to already selected sequences\n",
    "            min_distance = min(\n",
    "                np.linalg.norm(feature_matrix[i] - feature_matrix[j]) \n",
    "                for j in selected_indices\n",
    "            )\n",
    "            \n",
    "            if min_distance > max_min_distance:\n",
    "                max_min_distance = min_distance\n",
    "                best_idx = i\n",
    "        \n",
    "        if best_idx != -1:\n",
    "            selected_indices.append(best_idx)\n",
    "    \n",
    "    # Assign remaining sequences to closest cluster centers\n",
    "    for i in range(n_samples):\n",
    "        if i not in selected_indices:\n",
    "            distances = [\n",
    "                np.linalg.norm(feature_matrix[i] - feature_matrix[j]) \n",
    "                for j in selected_indices\n",
    "            ]\n",
    "            cluster_labels[i] = np.argmin(distances)\n",
    "        else:\n",
    "            cluster_labels[i] = selected_indices.index(i)\n",
    "    \n",
    "    return cluster_labels\n",
    "\n",
    "def _extract_enhanced_rna_features(sequence):\n",
    "    \"\"\"\n",
    "    Extract comprehensive RNA-specific features for better clustering and similarity.\n",
    "    \"\"\"\n",
    "    seq = sequence.upper()\n",
    "    features = []\n",
    "    \n",
    "    # 1. Basic nucleotide frequencies\n",
    "    nucleotides = ['A', 'U', 'G', 'C']\n",
    "    for nuc in nucleotides:\n",
    "        freq = seq.count(nuc) / len(seq) if len(seq) > 0 else 0\n",
    "        features.append(freq)\n",
    "    \n",
    "    # 2. Dinucleotide frequencies (reduced set - most important for RNA)\n",
    "    important_dinucs = ['AU', 'UA', 'GC', 'CG', 'GU', 'UG', 'AA', 'UU', 'GG', 'CC']\n",
    "    for dinuc in important_dinucs:\n",
    "        count = 0\n",
    "        for i in range(len(seq) - 1):\n",
    "            if seq[i:i+2] == dinuc:\n",
    "                count += 1\n",
    "        freq = count / (len(seq) - 1) if len(seq) > 1 else 0\n",
    "        features.append(freq)\n",
    "    \n",
    "    # 3. RNA secondary structure indicators\n",
    "    gc_content = (seq.count('G') + seq.count('C')) / len(seq) if len(seq) > 0 else 0\n",
    "    au_content = (seq.count('A') + seq.count('U')) / len(seq) if len(seq) > 0 else 0\n",
    "    purine_content = (seq.count('A') + seq.count('G')) / len(seq) if len(seq) > 0 else 0\n",
    "    pyrimidine_content = (seq.count('U') + seq.count('C')) / len(seq) if len(seq) > 0 else 0\n",
    "    \n",
    "    features.extend([gc_content, au_content, purine_content, pyrimidine_content])\n",
    "    \n",
    "    # 4. Sequence complexity measures\n",
    "    length_normalized = min(len(seq) / 1000.0, 1.0)  # Capped normalization\n",
    "    \n",
    "    # Simple entropy calculation\n",
    "    entropy = 0\n",
    "    for nuc in nucleotides:\n",
    "        freq = seq.count(nuc) / len(seq) if len(seq) > 0 else 0\n",
    "        if freq > 0:\n",
    "            entropy -= freq * np.log2(freq)\n",
    "    entropy_normalized = entropy / 2.0  # Max entropy for 4 nucleotides is 2\n",
    "    \n",
    "    features.extend([length_normalized, entropy_normalized])\n",
    "    \n",
    "    # 5. Repetitive pattern detection\n",
    "    repeat_content = _calculate_repeat_content(seq)\n",
    "    features.append(repeat_content)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def _calculate_repeat_content(sequence):\n",
    "    \"\"\"Calculate the proportion of repetitive content in the sequence.\"\"\"\n",
    "    if len(sequence) < 6:\n",
    "        return 0\n",
    "    \n",
    "    repeat_count = 0\n",
    "    window_size = 3\n",
    "    \n",
    "    for i in range(len(sequence) - window_size + 1):\n",
    "        motif = sequence[i:i + window_size]\n",
    "        # Look for the same motif in the rest of the sequence\n",
    "        for j in range(i + window_size, len(sequence) - window_size + 1):\n",
    "            if sequence[j:j + window_size] == motif:\n",
    "                repeat_count += 1\n",
    "                break\n",
    "    \n",
    "    return repeat_count / (len(sequence) - window_size + 1) if len(sequence) > window_size else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece1b5f8",
   "metadata": {
    "_cell_guid": "aaf3f747-0607-4470-a425-dbe714562373",
    "_uuid": "0d532205-a881-4262-86ec-36588b58b8f8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-09T19:55:46.803111Z",
     "iopub.status.busy": "2026-01-09T19:55:46.802783Z",
     "iopub.status.idle": "2026-01-09T19:55:46.814482Z",
     "shell.execute_reply": "2026-01-09T19:55:46.813662Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.093847,
     "end_time": "2026-01-09T19:55:46.816071",
     "exception": false,
     "start_time": "2026-01-09T19:55:46.722224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adaptive_rna_constraints(coordinates, sequence, confidence=1.0):\n",
    "    # Make a copy of coordinates to refine\n",
    "    refined_coords = coordinates.copy()\n",
    "    n_residues = len(sequence)\n",
    "    \n",
    "    # Calculate constraint strength (inverse of confidence)\n",
    "    # High confidence templates receive gentler constraints\n",
    "    constraint_strength = 0.8 * (1.0 - min(confidence, 0.8))\n",
    "    \n",
    "    # 1. Sequential distance constraints (consecutive nucleotides)\n",
    "    # More flexible distance range (statistical distribution from PDB)\n",
    "    seq_min_dist = 5.5  # Minimum sequential distance\n",
    "    seq_max_dist = 6.5  # Maximum sequential distance\n",
    "    \n",
    "    for i in range(n_residues - 1):\n",
    "        current_pos = refined_coords[i]\n",
    "        next_pos = refined_coords[i+1]\n",
    "        \n",
    "        # Calculate current distance\n",
    "        current_dist = np.linalg.norm(next_pos - current_pos)\n",
    "        \n",
    "        # Only adjust if significantly outside expected range\n",
    "        if current_dist < seq_min_dist or current_dist > seq_max_dist:\n",
    "            # Calculate target distance (midpoint of range)\n",
    "            target_dist = (seq_min_dist + seq_max_dist) / 2\n",
    "            \n",
    "            # Get direction vector\n",
    "            direction = next_pos - current_pos\n",
    "            direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "            \n",
    "            # Apply partial adjustment based on constraint strength\n",
    "            adjustment = (target_dist - current_dist) * constraint_strength\n",
    "            \n",
    "            # Only adjust the next position to preserve the overall fold\n",
    "            refined_coords[i+1] = current_pos + direction * (current_dist + adjustment)\n",
    "    \n",
    "    # 2. Steric clash prevention (more conservative)\n",
    "    min_allowed_distance = 3.8  # Minimum distance between non-consecutive C1' atoms\n",
    "    \n",
    "    # Calculate all pairwise distances\n",
    "    dist_matrix = distance_matrix(refined_coords, refined_coords)\n",
    "    \n",
    "    # Find severe clashes (atoms too close)\n",
    "    severe_clashes = np.where((dist_matrix < min_allowed_distance) & (dist_matrix > 0))\n",
    "    \n",
    "    # Fix severe clashes\n",
    "    for idx in range(len(severe_clashes[0])):\n",
    "        i, j = severe_clashes[0][idx], severe_clashes[1][idx]\n",
    "        \n",
    "        # Skip consecutive nucleotides and previously processed pairs\n",
    "        if abs(i - j) <= 1 or i >= j:\n",
    "            continue\n",
    "            \n",
    "        # Get current positions and distance\n",
    "        pos_i = refined_coords[i]\n",
    "        pos_j = refined_coords[j]\n",
    "        current_dist = dist_matrix[i, j]\n",
    "        \n",
    "        # Calculate necessary adjustment but scale by constraint strength\n",
    "        direction = pos_j - pos_i\n",
    "        direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "        \n",
    "        # Calculate partial adjustment\n",
    "        adjustment = (min_allowed_distance - current_dist) * constraint_strength\n",
    "        \n",
    "        # Move points apart\n",
    "        refined_coords[i] = pos_i - direction * (adjustment / 2)\n",
    "        refined_coords[j] = pos_j + direction * (adjustment / 2)\n",
    "    \n",
    "    # 3. Very light base-pair constraining (if confidence is low)\n",
    "    if constraint_strength > 0.3:  # Only apply if template confidence is low\n",
    "        # Simple Watson-Crick base pairs\n",
    "        pairs = {'A': 'U', 'U': 'A', 'G': 'C', 'C': 'G'}\n",
    "        \n",
    "        # Scan for potential base pairs\n",
    "        for i in range(n_residues):\n",
    "            base_i = sequence[i]\n",
    "            complement = pairs.get(base_i)\n",
    "            \n",
    "            if not complement:\n",
    "                continue\n",
    "                \n",
    "            # Look for complementary bases within a reasonable range\n",
    "            for j in range(i + 3, min(i + 20, n_residues)):\n",
    "                if sequence[j] == complement:\n",
    "                    # Calculate current distance\n",
    "                    current_dist = np.linalg.norm(refined_coords[i] - refined_coords[j])\n",
    "                    \n",
    "                    # Only consider if distance suggests potential pairing\n",
    "                    if 8.0 < current_dist < 14.0:\n",
    "                        # Target 10.5Å as generic base-pair C1'-C1' distance\n",
    "                        target_dist = 10.5\n",
    "                        \n",
    "                        # Calculate very gentle adjustment (scaled by constraint_strength)\n",
    "                        adjustment = (target_dist - current_dist) * (constraint_strength * 0.3)\n",
    "                        \n",
    "                        # Get direction vector\n",
    "                        direction = refined_coords[j] - refined_coords[i]\n",
    "                        direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "                        \n",
    "                        # Apply very gentle adjustment to both positions\n",
    "                        refined_coords[i] = refined_coords[i] - direction * (adjustment / 2)\n",
    "                        refined_coords[j] = refined_coords[j] + direction * (adjustment / 2)\n",
    "                        \n",
    "                        # Only consider one potential pair per base (closest match)\n",
    "                        break\n",
    "    \n",
    "    return refined_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b51a8a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:55:46.977193Z",
     "iopub.status.busy": "2026-01-09T19:55:46.976870Z",
     "iopub.status.idle": "2026-01-09T19:55:46.991868Z",
     "shell.execute_reply": "2026-01-09T19:55:46.991056Z"
    },
    "papermill": {
     "duration": 0.097881,
     "end_time": "2026-01-09T19:55:46.993558",
     "exception": false,
     "start_time": "2026-01-09T19:55:46.895677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adapt_template_to_query(query_seq, template_seq, template_coords, alignment=None):\n",
    "    if alignment is None:\n",
    "        from Bio.Seq import Seq\n",
    "        from Bio import pairwise2\n",
    "        \n",
    "        query_seq_obj = Seq(query_seq)\n",
    "        template_seq_obj = Seq(template_seq)\n",
    "        alignments = pairwise2.align.globalms(query_seq_obj, template_seq_obj, 2.9, -1, -10, -0.5, one_alignment_only=True)\n",
    "        \n",
    "        if not alignments:\n",
    "            return generate_improved_rna_structure(query_seq)\n",
    "            \n",
    "        alignment = alignments[0]\n",
    "    \n",
    "    aligned_query = alignment.seqA\n",
    "    aligned_template = alignment.seqB\n",
    "    \n",
    "    query_coords = np.zeros((len(query_seq), 3))\n",
    "    query_coords.fill(np.nan)\n",
    "    \n",
    "    # Map template coordinates to query\n",
    "    query_idx = 0\n",
    "    template_idx = 0\n",
    "    \n",
    "    for i in range(len(aligned_query)):\n",
    "        query_char = aligned_query[i]\n",
    "        template_char = aligned_template[i]\n",
    "        \n",
    "        if query_char != '-' and template_char != '-':\n",
    "            if template_idx < len(template_coords):\n",
    "                query_coords[query_idx] = template_coords[template_idx]\n",
    "            template_idx += 1\n",
    "            query_idx += 1\n",
    "        elif query_char != '-' and template_char == '-':\n",
    "            query_idx += 1\n",
    "        elif query_char == '-' and template_char != '-':\n",
    "            template_idx += 1\n",
    "    \n",
    "    # IMPROVED GAP FILLING - maintains RNA backbone geometry\n",
    "    backbone_distance = 5.9  # Typical C1'-C1' distance\n",
    "    \n",
    "    # Fill gaps by maintaining realistic backbone connectivity\n",
    "    for i in range(len(query_coords)):\n",
    "        if np.isnan(query_coords[i, 0]):\n",
    "            # Find nearest valid neighbors\n",
    "            prev_valid = next_valid = None\n",
    "            \n",
    "            for j in range(i-1, -1, -1):\n",
    "                if not np.isnan(query_coords[j, 0]):\n",
    "                    prev_valid = j\n",
    "                    break\n",
    "                    \n",
    "            for j in range(i+1, len(query_coords)):\n",
    "                if not np.isnan(query_coords[j, 0]):\n",
    "                    next_valid = j\n",
    "                    break\n",
    "            \n",
    "            if prev_valid is not None and next_valid is not None:\n",
    "                # Interpolate along realistic RNA backbone path\n",
    "                gap_size = next_valid - prev_valid\n",
    "                total_distance = np.linalg.norm(query_coords[next_valid] - query_coords[prev_valid])\n",
    "                expected_distance = gap_size * backbone_distance\n",
    "                \n",
    "                # If gap is compressed, extend it realistically\n",
    "                if total_distance < expected_distance * 0.7:\n",
    "                    direction = query_coords[next_valid] - query_coords[prev_valid]\n",
    "                    direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "                    \n",
    "                    # Place intermediate points along extended path\n",
    "                    for k, idx in enumerate(range(prev_valid + 1, next_valid)):\n",
    "                        progress = (k + 1) / gap_size\n",
    "                        base_pos = query_coords[prev_valid] + direction * expected_distance * progress\n",
    "                        \n",
    "                        # Add slight curvature for realism\n",
    "                        perpendicular = np.cross(direction, [0, 0, 1])\n",
    "                        if np.linalg.norm(perpendicular) < 1e-6:\n",
    "                            perpendicular = np.cross(direction, [1, 0, 0])\n",
    "                        perpendicular = perpendicular / (np.linalg.norm(perpendicular) + 1e-10)\n",
    "                        \n",
    "                        curve_amplitude = 2.0 * np.sin(progress * np.pi)\n",
    "                        query_coords[idx] = base_pos + perpendicular * curve_amplitude\n",
    "                else:\n",
    "                    # Linear interpolation for normal gaps\n",
    "                    for k, idx in enumerate(range(prev_valid + 1, next_valid)):\n",
    "                        weight = (k + 1) / gap_size\n",
    "                        query_coords[idx] = (1 - weight) * query_coords[prev_valid] + weight * query_coords[next_valid]\n",
    "            \n",
    "            elif prev_valid is not None:\n",
    "                # Extend from previous position\n",
    "                if prev_valid > 0 and not np.isnan(query_coords[prev_valid-1, 0]):\n",
    "                    direction = query_coords[prev_valid] - query_coords[prev_valid-1]\n",
    "                    direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "                else:\n",
    "                    direction = np.array([1.0, 0.0, 0.0])\n",
    "                \n",
    "                steps_needed = i - prev_valid\n",
    "                for step in range(1, steps_needed + 1):\n",
    "                    pos_idx = prev_valid + step\n",
    "                    if pos_idx < len(query_coords):\n",
    "                        query_coords[pos_idx] = query_coords[prev_valid] + direction * backbone_distance * step\n",
    "            \n",
    "            elif next_valid is not None:\n",
    "                # Work backwards from next position\n",
    "                direction = np.array([-1.0, 0.0, 0.0])  # Default backward direction\n",
    "                steps_needed = next_valid - i\n",
    "                for step in range(steps_needed, 0, -1):\n",
    "                    pos_idx = next_valid - step\n",
    "                    if pos_idx >= 0:\n",
    "                        query_coords[pos_idx] = query_coords[next_valid] - direction * backbone_distance * step\n",
    "    \n",
    "    # Final cleanup\n",
    "    query_coords = np.nan_to_num(query_coords)\n",
    "    return query_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbc74016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:55:47.154451Z",
     "iopub.status.busy": "2026-01-09T19:55:47.154086Z",
     "iopub.status.idle": "2026-01-09T19:55:47.166632Z",
     "shell.execute_reply": "2026-01-09T19:55:47.165732Z"
    },
    "papermill": {
     "duration": 0.094861,
     "end_time": "2026-01-09T19:55:47.168160",
     "exception": false,
     "start_time": "2026-01-09T19:55:47.073299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_improved_rna_structure(sequence):\n",
    "    \"\"\"\n",
    "    Generate a more realistic RNA structure fallback based on sequence patterns\n",
    "    and basic RNA structure principles.\n",
    "    \n",
    "    Args:\n",
    "        sequence: RNA sequence string\n",
    "        \n",
    "    Returns:\n",
    "        Array of 3D coordinates\n",
    "    \"\"\"\n",
    "    n_residues = len(sequence)\n",
    "    coordinates = np.zeros((n_residues, 3))\n",
    "    \n",
    "    # Analyze sequence to predict structural elements\n",
    "    # Look for complementary regions that could form base pairs\n",
    "    potential_stems = identify_potential_stems(sequence)\n",
    "    \n",
    "    # Default parameters\n",
    "    radius_helix = 10.0\n",
    "    radius_loop = 15.0\n",
    "    rise_per_residue_helix = 2.5\n",
    "    rise_per_residue_loop = 1.5\n",
    "    angle_per_residue_helix = 0.6\n",
    "    angle_per_residue_loop = 0.3\n",
    "    \n",
    "    # Assign structural classifications\n",
    "    structure_types = assign_structure_types(sequence, potential_stems)\n",
    "    \n",
    "    # Generate coordinates based on predicted structure\n",
    "    current_pos = np.array([0.0, 0.0, 0.0])\n",
    "    current_direction = np.array([0.0, 0.0, 1.0])\n",
    "    current_angle = 0.0\n",
    "    \n",
    "    for i in range(n_residues):\n",
    "        if structure_types[i] == 'stem':\n",
    "            # Part of a helical stem\n",
    "            current_angle += angle_per_residue_helix\n",
    "            coordinates[i] = [\n",
    "                radius_helix * np.cos(current_angle), \n",
    "                radius_helix * np.sin(current_angle), \n",
    "                current_pos[2] + rise_per_residue_helix\n",
    "            ]\n",
    "            current_pos = coordinates[i]\n",
    "        elif structure_types[i] == 'loop':\n",
    "            # Part of a loop\n",
    "            current_angle += angle_per_residue_loop\n",
    "            z_shift = rise_per_residue_loop * np.sin(current_angle * 0.5)\n",
    "            coordinates[i] = [\n",
    "                radius_loop * np.cos(current_angle), \n",
    "                radius_loop * np.sin(current_angle), \n",
    "                current_pos[2] + z_shift\n",
    "            ]\n",
    "            current_pos = coordinates[i]\n",
    "        else:\n",
    "            # Single-stranded region\n",
    "            # Add some randomness to make it look more realistic\n",
    "            jitter = np.random.normal(0, 1, 3) * 2.0\n",
    "            coordinates[i] = current_pos + jitter\n",
    "            current_pos = coordinates[i]\n",
    "            \n",
    "    return coordinates\n",
    "\n",
    "def identify_potential_stems(sequence):\n",
    "    \"\"\"\n",
    "    Identify potential stem regions by looking for self-complementary segments.\n",
    "    \n",
    "    Args:\n",
    "        sequence: RNA sequence string\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples (start1, end1, start2, end2) representing potentially paired regions\n",
    "    \"\"\"\n",
    "    complementary_bases = {'A': 'U', 'U': 'A', 'G': 'C', 'C': 'G'}\n",
    "    min_stem_length = 3\n",
    "    potential_stems = []\n",
    "    \n",
    "    # Simple stem identification\n",
    "    for i in range(len(sequence) - min_stem_length):\n",
    "        for j in range(i + min_stem_length + 3, len(sequence) - min_stem_length + 1):\n",
    "            # Check if regions could form a stem\n",
    "            potential_stem_len = min(min_stem_length, len(sequence) - j)\n",
    "            is_stem = True\n",
    "            \n",
    "            for k in range(potential_stem_len):\n",
    "                if sequence[i+k] not in complementary_bases or \\\n",
    "                   complementary_bases[sequence[i+k]] != sequence[j+potential_stem_len-k-1]:\n",
    "                    is_stem = False\n",
    "                    break\n",
    "            \n",
    "            if is_stem:\n",
    "                potential_stems.append((i, i+potential_stem_len-1, j, j+potential_stem_len-1))\n",
    "    \n",
    "    return potential_stems\n",
    "\n",
    "def assign_structure_types(sequence, potential_stems):\n",
    "    \"\"\"\n",
    "    Assign each nucleotide to a structural element type.\n",
    "    \n",
    "    Args:\n",
    "        sequence: RNA sequence string\n",
    "        potential_stems: List of tuples representing stem regions\n",
    "        \n",
    "    Returns:\n",
    "        List of structure types ('stem', 'loop', 'single')\n",
    "    \"\"\"\n",
    "    structure_types = ['single'] * len(sequence)\n",
    "    \n",
    "    # Mark stem regions\n",
    "    for stem in potential_stems:\n",
    "        start1, end1, start2, end2 = stem\n",
    "        for i in range(end1 - start1 + 1):\n",
    "            structure_types[start1 + i] = 'stem'\n",
    "            structure_types[end2 - i] = 'stem'\n",
    "    \n",
    "    # Mark loop regions (regions between paired regions)\n",
    "    for i in range(len(potential_stems) - 1):\n",
    "        _, end1, start2, _ = potential_stems[i]\n",
    "        next_start1, _, _, _ = potential_stems[i+1]\n",
    "        \n",
    "        if next_start1 > end1 + 1 and start2 > next_start1:\n",
    "            for j in range(end1 + 1, next_start1):\n",
    "                structure_types[j] = 'loop'\n",
    "    \n",
    "    return structure_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c702f890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:55:47.328754Z",
     "iopub.status.busy": "2026-01-09T19:55:47.328306Z",
     "iopub.status.idle": "2026-01-09T19:55:47.339380Z",
     "shell.execute_reply": "2026-01-09T19:55:47.338341Z"
    },
    "papermill": {
     "duration": 0.093476,
     "end_time": "2026-01-09T19:55:47.341042",
     "exception": false,
     "start_time": "2026-01-09T19:55:47.247566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create a more realistic RNA structure when no good templates are found\n",
    "def generate_rna_structure(sequence, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "    \n",
    "    n_residues = len(sequence)\n",
    "    coordinates = np.zeros((n_residues, 3))\n",
    "    \n",
    "    # Initialize the first few residues in a helix\n",
    "    for i in range(min(3, n_residues)):\n",
    "        angle = i * 0.6\n",
    "        coordinates[i] = [10.0 * np.cos(angle), 10.0 * np.sin(angle), i * 2.5]\n",
    "    \n",
    "    # Add more complex folding patterns\n",
    "    current_direction = np.array([0.0, 0.0, 1.0])  # Start moving along z-axis\n",
    "    \n",
    "    # Define base-pairing tendencies (G-C and A-U pairs)\n",
    "    for i in range(3, n_residues):\n",
    "        # Check for potential base-pairing in the sequence\n",
    "        has_pair = False\n",
    "        pair_idx = -1\n",
    "        \n",
    "        # Simple detection of complementary bases (G-C, A-U)\n",
    "        complementary = {'G': 'C', 'C': 'G', 'A': 'U', 'U': 'A'}\n",
    "        current_base = sequence[i]\n",
    "        \n",
    "        # Look for potential base-pairing within a window before the current position\n",
    "        window_size = min(i, 15)  # Look back up to 15 bases\n",
    "        for j in range(i-window_size, i):\n",
    "            if j >= 0 and sequence[j] == complementary.get(current_base, 'X'):\n",
    "                # Found a potential pair\n",
    "                has_pair = True\n",
    "                pair_idx = j\n",
    "                break\n",
    "        \n",
    "        if has_pair and i - pair_idx <= 10 and random.random() < 0.7:\n",
    "            # Try to create a base-pair by positioning this nucleotide near its pair\n",
    "            pair_pos = coordinates[pair_idx]\n",
    "            \n",
    "            # Create a position that's roughly opposite to the pair\n",
    "            random_offset = np.random.normal(0, 1, 3) * 2.0\n",
    "            base_pair_distance = 10.0 + random.uniform(-1.0, 1.0)\n",
    "            \n",
    "            # Calculate a vector from base-pair toward center of structure\n",
    "            center = np.mean(coordinates[:i], axis=0)\n",
    "            direction = center - pair_pos\n",
    "            direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "            \n",
    "            # Position new nucleotide in the general direction of the \"center\"\n",
    "            coordinates[i] = pair_pos + direction * base_pair_distance + random_offset\n",
    "            \n",
    "            # Update direction for next nucleotide\n",
    "            current_direction = np.random.normal(0, 0.3, 3)\n",
    "            current_direction = current_direction / (np.linalg.norm(current_direction) + 1e-10)\n",
    "            \n",
    "        else:\n",
    "            # No base-pairing detected, continue with the current fold direction\n",
    "            # Randomly rotate current direction to simulate RNA flexibility\n",
    "            if random.random() < 0.3:\n",
    "                # More significant direction change\n",
    "                angle = random.uniform(0.2, 0.6)\n",
    "                axis = np.random.normal(0, 1, 3)\n",
    "                axis = axis / (np.linalg.norm(axis) + 1e-10)\n",
    "                rotation = R.from_rotvec(angle * axis)\n",
    "                current_direction = rotation.apply(current_direction)\n",
    "            else:\n",
    "                # Small random changes in direction\n",
    "                current_direction += np.random.normal(0, 0.15, 3)\n",
    "                current_direction = current_direction / (np.linalg.norm(current_direction) + 1e-10)\n",
    "            \n",
    "            # Distance between consecutive nucleotides (3.5-4.5Å is typical)\n",
    "            step_size = random.uniform(3.5, 4.5)\n",
    "            \n",
    "            # Update position\n",
    "            coordinates[i] = coordinates[i-1] + step_size * current_direction\n",
    "    \n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3d0ddd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:55:47.504421Z",
     "iopub.status.busy": "2026-01-09T19:55:47.504099Z",
     "iopub.status.idle": "2026-01-09T19:55:47.511219Z",
     "shell.execute_reply": "2026-01-09T19:55:47.510226Z"
    },
    "papermill": {
     "duration": 0.089725,
     "end_time": "2026-01-09T19:55:47.512916",
     "exception": false,
     "start_time": "2026-01-09T19:55:47.423191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_rna_structures(sequence, target_id, train_seqs_df, train_coords_dict, n_predictions=5):\n",
    "    predictions = []\n",
    "    \n",
    "    # Find similar sequences in the training data\n",
    "    similar_seqs = find_similar_sequences(sequence, train_seqs_df, train_coords_dict, top_n=n_predictions)\n",
    "    \n",
    "    # If we found any similar sequences, use them as templates\n",
    "    if similar_seqs:\n",
    "        for i, (template_id, template_seq, similarity_score, template_coords) in enumerate(similar_seqs):\n",
    "            # Adapt template coordinates to the query sequence\n",
    "            adapted_coords = adapt_template_to_query(sequence, template_seq, template_coords)\n",
    "            \n",
    "            if adapted_coords is not None:\n",
    "                # Apply adaptive constraints based on template similarity\n",
    "                # For high similarity templates, apply very gentle constraints\n",
    "                refined_coords = adaptive_rna_constraints(adapted_coords, sequence, confidence=similarity_score)\n",
    "                \n",
    "                # Add some randomness (less for better templates)\n",
    "                random_scale = max(0.05, 0.8 - similarity_score)  # Reduced randomness\n",
    "                randomized_coords = refined_coords.copy()\n",
    "                randomized_coords += np.random.normal(0, random_scale, randomized_coords.shape)\n",
    "                \n",
    "                predictions.append(randomized_coords)\n",
    "                \n",
    "                if len(predictions) >= n_predictions:\n",
    "                    break\n",
    "    \n",
    "    # If we don't have enough predictions from templates, generate de novo structures\n",
    "    while len(predictions) < n_predictions:\n",
    "        seed_value = hash(target_id) % 10000 + len(predictions) * 1000\n",
    "        de_novo_coords = generate_rna_structure(sequence, seed=seed_value)\n",
    "        \n",
    "        # Apply stronger constraints to de novo structures (lower confidence)\n",
    "        refined_de_novo = adaptive_rna_constraints(de_novo_coords, sequence, confidence=0.2)\n",
    "        \n",
    "        predictions.append(refined_de_novo)\n",
    "    \n",
    "    return predictions[:n_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61ec6743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:55:47.677284Z",
     "iopub.status.busy": "2026-01-09T19:55:47.676940Z",
     "iopub.status.idle": "2026-01-09T23:28:11.482565Z",
     "shell.execute_reply": "2026-01-09T23:28:11.481465Z"
    },
    "papermill": {
     "duration": 12744.070374,
     "end_time": "2026-01-09T23:28:11.664507",
     "exception": false,
     "start_time": "2026-01-09T19:55:47.594133",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target 1/28: 8ZNQ (30 nt), elapsed: 0.0s, est. remaining: 0.0s\n",
      "Processing target 6/28: 9E9Q (101 nt), elapsed: 12118.1s, est. remaining: 44433.2s\n",
      "Processing target 11/28: 9G4R (47 nt), elapsed: 12162.2s, est. remaining: 18796.1s\n",
      "Processing target 16/28: 9I9W (28 nt), elapsed: 12236.0s, est. remaining: 9177.0s\n",
      "Processing target 21/28: 9WHV (80 nt), elapsed: 12259.0s, est. remaining: 4086.3s\n",
      "Processing target 26/28: 9EBP (81 nt), elapsed: 12327.0s, est. remaining: 948.2s\n",
      "Generated predictions for 28 RNA sequences\n",
      "Total runtime: 12743.8 seconds\n"
     ]
    }
   ],
   "source": [
    "# List to store all prediction records\n",
    "all_predictions = []\n",
    "\n",
    "# Set up time tracking\n",
    "start_time = time.time()\n",
    "total_targets = len(test_seqs)\n",
    "\n",
    "# For each sequence in the test set\n",
    "for idx, row in test_seqs.iterrows():\n",
    "    target_id = row['target_id']\n",
    "    sequence = row['sequence']\n",
    "    \n",
    "    # Progress tracking\n",
    "    if idx % 5 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        targets_processed = idx + 1\n",
    "        if targets_processed > 0:\n",
    "            avg_time_per_target = elapsed / targets_processed\n",
    "            est_time_remaining = avg_time_per_target * (total_targets - targets_processed)\n",
    "            print(f\"Processing target {targets_processed}/{total_targets}: {target_id} ({len(sequence)} nt), \"\n",
    "                  f\"elapsed: {elapsed:.1f}s, est. remaining: {est_time_remaining:.1f}s\")\n",
    "    \n",
    "    # Generate 5 different structure predictions\n",
    "    predictions = predict_rna_structures(sequence, target_id, train_seqs_extended, train_coords_dict, n_predictions=5)\n",
    "    \n",
    "    # For each residue in the sequence\n",
    "    for j in range(len(sequence)):\n",
    "        pred_row = {\n",
    "            'ID': f\"{target_id}_{j+1}\",\n",
    "            'resname': sequence[j],\n",
    "            'resid': j + 1\n",
    "        }\n",
    "        \n",
    "        # Add coordinates from all 5 predictions\n",
    "        for i in range(5):\n",
    "            pred_row[f'x_{i+1}'] = predictions[i][j][0]\n",
    "            pred_row[f'y_{i+1}'] = predictions[i][j][1]\n",
    "            pred_row[f'z_{i+1}'] = predictions[i][j][2]\n",
    "        \n",
    "        all_predictions.append(pred_row)\n",
    "\n",
    "# Create DataFrame with predictions\n",
    "submission_df = pd.DataFrame(all_predictions)\n",
    "\n",
    "# Ensure the submission file has the correct format\n",
    "column_order = ['ID', 'resname', 'resid']\n",
    "for i in range(1, 6):\n",
    "    for coord in ['x', 'y', 'z']:\n",
    "        column_order.append(f'{coord}_{i}')\n",
    "submission_df = submission_df[column_order]\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(f\"Generated predictions for {len(test_seqs)} RNA sequences\")\n",
    "print(f\"Total runtime: {time.time() - start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2be2e0e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T23:28:11.827049Z",
     "iopub.status.busy": "2026-01-09T23:28:11.826649Z",
     "iopub.status.idle": "2026-01-09T23:28:11.860997Z",
     "shell.execute_reply": "2026-01-09T23:28:11.859609Z"
    },
    "papermill": {
     "duration": 0.11747,
     "end_time": "2026-01-09T23:28:11.863021",
     "exception": false,
     "start_time": "2026-01-09T23:28:11.745551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>resname</th>\n",
       "      <th>resid</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>z_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>z_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>y_3</th>\n",
       "      <th>z_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>y_4</th>\n",
       "      <th>z_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>y_5</th>\n",
       "      <th>z_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8ZNQ_1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.053576</td>\n",
       "      <td>-15.047171</td>\n",
       "      <td>20.836220</td>\n",
       "      <td>31.965609</td>\n",
       "      <td>42.835702</td>\n",
       "      <td>29.639326</td>\n",
       "      <td>126.784322</td>\n",
       "      <td>139.995939</td>\n",
       "      <td>178.139892</td>\n",
       "      <td>151.626188</td>\n",
       "      <td>185.230519</td>\n",
       "      <td>167.981654</td>\n",
       "      <td>-7.358097</td>\n",
       "      <td>6.459116</td>\n",
       "      <td>18.920617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8ZNQ_2</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.040020</td>\n",
       "      <td>-15.020788</td>\n",
       "      <td>15.242151</td>\n",
       "      <td>35.770951</td>\n",
       "      <td>44.339019</td>\n",
       "      <td>25.258695</td>\n",
       "      <td>129.673918</td>\n",
       "      <td>139.835987</td>\n",
       "      <td>173.179698</td>\n",
       "      <td>145.855262</td>\n",
       "      <td>184.984842</td>\n",
       "      <td>167.296240</td>\n",
       "      <td>-6.091913</td>\n",
       "      <td>9.581328</td>\n",
       "      <td>15.028237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8ZNQ_3</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.358215</td>\n",
       "      <td>-13.424466</td>\n",
       "      <td>10.359759</td>\n",
       "      <td>39.860860</td>\n",
       "      <td>42.332300</td>\n",
       "      <td>22.249928</td>\n",
       "      <td>132.366435</td>\n",
       "      <td>136.791894</td>\n",
       "      <td>169.262945</td>\n",
       "      <td>139.975538</td>\n",
       "      <td>185.240515</td>\n",
       "      <td>167.389514</td>\n",
       "      <td>-4.049528</td>\n",
       "      <td>14.396373</td>\n",
       "      <td>12.546336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8ZNQ_4</td>\n",
       "      <td>G</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.544701</td>\n",
       "      <td>-11.050287</td>\n",
       "      <td>6.473632</td>\n",
       "      <td>43.639751</td>\n",
       "      <td>38.560124</td>\n",
       "      <td>21.316990</td>\n",
       "      <td>128.732735</td>\n",
       "      <td>128.951437</td>\n",
       "      <td>166.310116</td>\n",
       "      <td>142.743202</td>\n",
       "      <td>186.715887</td>\n",
       "      <td>172.617340</td>\n",
       "      <td>-0.944601</td>\n",
       "      <td>19.763611</td>\n",
       "      <td>12.775691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8ZNQ_5</td>\n",
       "      <td>U</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.352305</td>\n",
       "      <td>-6.164257</td>\n",
       "      <td>4.449658</td>\n",
       "      <td>45.253300</td>\n",
       "      <td>33.225154</td>\n",
       "      <td>20.580884</td>\n",
       "      <td>123.512125</td>\n",
       "      <td>124.103347</td>\n",
       "      <td>163.726847</td>\n",
       "      <td>143.715583</td>\n",
       "      <td>190.816930</td>\n",
       "      <td>176.389813</td>\n",
       "      <td>3.019878</td>\n",
       "      <td>22.170579</td>\n",
       "      <td>14.485997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9757</th>\n",
       "      <td>9ZCC_1456</td>\n",
       "      <td>C</td>\n",
       "      <td>1456</td>\n",
       "      <td>287.370332</td>\n",
       "      <td>357.238468</td>\n",
       "      <td>349.594585</td>\n",
       "      <td>203.073773</td>\n",
       "      <td>251.903131</td>\n",
       "      <td>280.434847</td>\n",
       "      <td>201.127759</td>\n",
       "      <td>271.585340</td>\n",
       "      <td>271.948767</td>\n",
       "      <td>1176.487584</td>\n",
       "      <td>3740.628435</td>\n",
       "      <td>6723.756211</td>\n",
       "      <td>170.259186</td>\n",
       "      <td>231.123948</td>\n",
       "      <td>206.786397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9758</th>\n",
       "      <td>9ZCC_1457</td>\n",
       "      <td>C</td>\n",
       "      <td>1457</td>\n",
       "      <td>287.822278</td>\n",
       "      <td>362.259911</td>\n",
       "      <td>346.974643</td>\n",
       "      <td>203.246681</td>\n",
       "      <td>256.079363</td>\n",
       "      <td>276.738861</td>\n",
       "      <td>206.481448</td>\n",
       "      <td>273.348025</td>\n",
       "      <td>273.211704</td>\n",
       "      <td>1177.687096</td>\n",
       "      <td>3742.976071</td>\n",
       "      <td>6728.792738</td>\n",
       "      <td>173.904188</td>\n",
       "      <td>233.135077</td>\n",
       "      <td>211.826055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9759</th>\n",
       "      <td>9ZCC_1458</td>\n",
       "      <td>U</td>\n",
       "      <td>1458</td>\n",
       "      <td>290.039959</td>\n",
       "      <td>366.350706</td>\n",
       "      <td>343.530662</td>\n",
       "      <td>205.330849</td>\n",
       "      <td>260.665068</td>\n",
       "      <td>273.606855</td>\n",
       "      <td>191.696883</td>\n",
       "      <td>266.252745</td>\n",
       "      <td>279.505270</td>\n",
       "      <td>1178.226505</td>\n",
       "      <td>3746.393006</td>\n",
       "      <td>6733.947157</td>\n",
       "      <td>177.790489</td>\n",
       "      <td>233.517207</td>\n",
       "      <td>214.679119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9760</th>\n",
       "      <td>9ZCC_1459</td>\n",
       "      <td>U</td>\n",
       "      <td>1459</td>\n",
       "      <td>293.826408</td>\n",
       "      <td>368.434585</td>\n",
       "      <td>339.880664</td>\n",
       "      <td>211.011545</td>\n",
       "      <td>262.217106</td>\n",
       "      <td>271.973449</td>\n",
       "      <td>187.195010</td>\n",
       "      <td>264.639427</td>\n",
       "      <td>286.020464</td>\n",
       "      <td>1179.208480</td>\n",
       "      <td>3748.888531</td>\n",
       "      <td>6739.251584</td>\n",
       "      <td>182.383690</td>\n",
       "      <td>234.936778</td>\n",
       "      <td>215.871511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9761</th>\n",
       "      <td>9ZCC_1460</td>\n",
       "      <td>A</td>\n",
       "      <td>1460</td>\n",
       "      <td>297.967132</td>\n",
       "      <td>366.940565</td>\n",
       "      <td>336.072751</td>\n",
       "      <td>216.822030</td>\n",
       "      <td>264.137027</td>\n",
       "      <td>270.489138</td>\n",
       "      <td>185.206746</td>\n",
       "      <td>263.747020</td>\n",
       "      <td>293.016684</td>\n",
       "      <td>1180.123018</td>\n",
       "      <td>3751.581004</td>\n",
       "      <td>6744.322167</td>\n",
       "      <td>174.838435</td>\n",
       "      <td>203.761175</td>\n",
       "      <td>213.096458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9762 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID resname  resid         x_1         y_1         z_1  \\\n",
       "0        8ZNQ_1       A      1   -2.053576  -15.047171   20.836220   \n",
       "1        8ZNQ_2       C      2   -2.040020  -15.020788   15.242151   \n",
       "2        8ZNQ_3       C      3   -3.358215  -13.424466   10.359759   \n",
       "3        8ZNQ_4       G      4   -5.544701  -11.050287    6.473632   \n",
       "4        8ZNQ_5       U      5   -6.352305   -6.164257    4.449658   \n",
       "...         ...     ...    ...         ...         ...         ...   \n",
       "9757  9ZCC_1456       C   1456  287.370332  357.238468  349.594585   \n",
       "9758  9ZCC_1457       C   1457  287.822278  362.259911  346.974643   \n",
       "9759  9ZCC_1458       U   1458  290.039959  366.350706  343.530662   \n",
       "9760  9ZCC_1459       U   1459  293.826408  368.434585  339.880664   \n",
       "9761  9ZCC_1460       A   1460  297.967132  366.940565  336.072751   \n",
       "\n",
       "             x_2         y_2         z_2         x_3         y_3         z_3  \\\n",
       "0      31.965609   42.835702   29.639326  126.784322  139.995939  178.139892   \n",
       "1      35.770951   44.339019   25.258695  129.673918  139.835987  173.179698   \n",
       "2      39.860860   42.332300   22.249928  132.366435  136.791894  169.262945   \n",
       "3      43.639751   38.560124   21.316990  128.732735  128.951437  166.310116   \n",
       "4      45.253300   33.225154   20.580884  123.512125  124.103347  163.726847   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "9757  203.073773  251.903131  280.434847  201.127759  271.585340  271.948767   \n",
       "9758  203.246681  256.079363  276.738861  206.481448  273.348025  273.211704   \n",
       "9759  205.330849  260.665068  273.606855  191.696883  266.252745  279.505270   \n",
       "9760  211.011545  262.217106  271.973449  187.195010  264.639427  286.020464   \n",
       "9761  216.822030  264.137027  270.489138  185.206746  263.747020  293.016684   \n",
       "\n",
       "              x_4          y_4          z_4         x_5         y_5  \\\n",
       "0      151.626188   185.230519   167.981654   -7.358097    6.459116   \n",
       "1      145.855262   184.984842   167.296240   -6.091913    9.581328   \n",
       "2      139.975538   185.240515   167.389514   -4.049528   14.396373   \n",
       "3      142.743202   186.715887   172.617340   -0.944601   19.763611   \n",
       "4      143.715583   190.816930   176.389813    3.019878   22.170579   \n",
       "...           ...          ...          ...         ...         ...   \n",
       "9757  1176.487584  3740.628435  6723.756211  170.259186  231.123948   \n",
       "9758  1177.687096  3742.976071  6728.792738  173.904188  233.135077   \n",
       "9759  1178.226505  3746.393006  6733.947157  177.790489  233.517207   \n",
       "9760  1179.208480  3748.888531  6739.251584  182.383690  234.936778   \n",
       "9761  1180.123018  3751.581004  6744.322167  174.838435  203.761175   \n",
       "\n",
       "             z_5  \n",
       "0      18.920617  \n",
       "1      15.028237  \n",
       "2      12.546336  \n",
       "3      12.775691  \n",
       "4      14.485997  \n",
       "...          ...  \n",
       "9757  206.786397  \n",
       "9758  211.826055  \n",
       "9759  214.679119  \n",
       "9760  215.871511  \n",
       "9761  213.096458  \n",
       "\n",
       "[9762 rows x 18 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0db2a3",
   "metadata": {
    "papermill": {
     "duration": 0.083677,
     "end_time": "2026-01-09T23:28:12.030685",
     "exception": false,
     "start_time": "2026-01-09T23:28:11.947008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15231210,
     "sourceId": 118765,
     "sourceType": "competition"
    },
    {
     "datasetId": 7306643,
     "sourceId": 11644010,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7526656,
     "sourceId": 11969392,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13374.105133,
   "end_time": "2026-01-09T23:28:14.748728",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-09T19:45:20.643595",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
